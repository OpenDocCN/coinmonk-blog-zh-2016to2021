<html>
<head>
<title>Review: OverFeat — Winner of ILSVRC 2013 Localization Task (Object Detection)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾:over feat——ils vrc 2013本地化任务(目标检测)冠军</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754?source=collection_archive---------0-----------------------#2018-08-29">https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754?source=collection_archive---------0-----------------------#2018-08-29</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><p id="e04a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">在ILSVRC 2013中有3项任务—分类、定位和检测。</p><p id="c125" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"> OverFeat [1]由一个CNN完成全部3个任务，在ILSVRC ( </strong> <a class="ae jo" href="http://www.image-net.org/challenges/LSVRC/" rel="noopener ugc nofollow" target="_blank"> <strong class="is hu"> ImageNet大规模视觉识别竞赛</strong> </a> <strong class="is hu"> ) 2013年[2]中获得定位任务，在当时的分类任务中获得rank 4，在赛后工作中获得当时的检测任务rank 1。</strong></p><p id="1ef2" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">对了，这是LeNet [9]的发明者Lecun教授小组的作品，经典的深度学习研究作品。而且是我写这个故事的时候<strong class="is hu"> 2014 ICLR </strong>论文<strong class="is hu">2000多篇引用</strong>。(<a class="jp jq gr" href="https://medium.com/u/aff72a0c1243?source=post_page-----a6f8b9044754--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl jr js hb jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="hm hn ho hp hq"><p id="44e9" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">分类</strong>:对图像内的物体进行分类。<br/> <strong class="is hu">定位</strong>:对物体进行分类，通过图像内的包围盒对物体进行定位。<br/> <strong class="is hu">检测</strong>:类似于定位，我们也需要对物体进行分类，通过包围盒对物体进行定位，但是可以包含小的物体，评价指标也不同于定位。它还需要预测没有对象时的背景类。</p><figure class="jz ka kb kc fq kd fe ff paragraph-image"><div class="fe ff jy"><img src="../Images/db3e8b1933a3c03052c3730dd4f43c0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*SeqsDmYPJznksGgcVjsS0A.png"/></div><figcaption class="kg kh fg fe ff ki kj bd b be z ek"><strong class="bd kk">Localization (Top) and Detection (Bottom)</strong></figcaption></figure></div><div class="ab cl jr js hb jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="hm hn ho hp hq"><h1 id="7bc5" class="kl km ht bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li dt translated">将涵盖哪些内容:</h1><ol class=""><li id="7d98" class="lj lk ht is b it ll ix lm jb ln jf lo jj lp jn lq lr ls lt dt translated">由AlexNet修改的CNN模型(快速准确模型)</li><li id="64f0" class="lj lk ht is b it lu ix lv jb lw jf lx jj ly jn lq lr ls lt dt translated">精细步幅最大汇集</li><li id="61e7" class="lj lk ht is b it lu ix lv jb lw jf lx jj ly jn lq lr ls lt dt translated">多尺度分类</li><li id="058f" class="lj lk ht is b it lu ix lv jb lw jf lx jj ly jn lq lr ls lt dt translated">分类结果</li><li id="b88e" class="lj lk ht is b it lu ix lv jb lw jf lx jj ly jn lq lr ls lt dt translated">用于定位/检测的回归网络</li></ol></div><div class="ab cl jr js hb jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="hm hn ho hp hq"><h1 id="db5f" class="kl km ht bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li dt translated">1.由AlexNet修改的CNN模型(快速准确模型)</h1><figure class="jz ka kb kc fq kd fe ff paragraph-image"><div class="ab fr cl lz"><img src="../Images/a2f7f2f9d4ec874f35631fc2886758e8.png" data-original-src="https://miro.medium.com/v2/format:webp/1*UDH-Shn5KbVTeuxt5mzHaA.png"/></div><figcaption class="kg kh fg fe ff ki kj bd b be z ek"><strong class="bd kk">The Original AlexNet (Single-GPU Version)</strong></figcaption></figure><p id="d2a9" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">以上是AlexNet [3]单GPU版本。(如有兴趣请访问我的评论[4]。)作者将其修改为<strong class="is hu">快速</strong>和<strong class="is hu">精确</strong>型号如下:</p><figure class="jz ka kb kc fq kd fe ff paragraph-image"><div class="fe ff ma"><img src="../Images/8f54f3abb1cbd456c781833ec486ea5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*PJVEGAjCXsQumtPganrcrg.png"/></div><figcaption class="kg kh fg fe ff ki kj bd b be z ek"><strong class="bd kk">Fast Model</strong></figcaption></figure><figure class="jz ka kb kc fq kd fe ff paragraph-image"><div class="fe ff mb"><img src="../Images/4e5178a721098888eb5fac6261613545.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*WwmNnJSpL9sxRLmjglL6cg.png"/></div><figcaption class="kg kh fg fe ff ki kj bd b be z ek"><strong class="bd kk">Accurate Model</strong></figcaption></figure><p id="0938" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">简单地说，这两个模型在AlexNet上有一些修改，但在整体架构上没有大的变化。</p><p id="1b62" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">例如，没有局部对比度标准化(LRN)。汇集层是不重叠的。由于使用较小的步幅2，第1层和第2层的要素地图更大。ZFNet [5]已经证明了较小的步幅有助于通过可视化CNN层来提高精度。(如有兴趣，请访问我的评论[6]。)</p></div><div class="ab cl jr js hb jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="hm hn ho hp hq"><h1 id="4d6a" class="kl km ht bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li dt translated">2.精细步幅最大汇集</h1><p id="5eac" class="pw-post-body-paragraph iq ir ht is b it ll iv iw ix lm iz ja jb mc jd je jf md jh ji jj me jl jm jn hm dt translated">在修改后的网络的第5层中添加了一个精细的最大跨距池。</p><p id="ee36" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">(b)在第5层，从{0，1，2}起，以不同的像素偏移<strong class="is hu">δx和δy，多次进行<strong class="is hu"> 3×3最大汇集。</strong></strong></p><p id="776e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">(c) <strong class="is hu">进行3×3次最大合并</strong>，共合并9张特征图。</p><p id="5cae" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">(d)每个融合后的特征图<strong class="is hu">经过FC层6、7、8 </strong>得到<strong class="is hu">输出概率向量</strong>。</p><p id="9528" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">(e)将所有矢量重新整形为<strong class="is hu">三维输出图</strong>。</p><figure class="jz ka kb kc fq kd fe ff paragraph-image"><div class="fe ff mf"><img src="../Images/a680dfff2d70a8b40062f4c7e1cff2cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*xq6F8iLyzc_ugKZv-vILog.png"/></div><figcaption class="kg kh fg fe ff ki kj bd b be z ek"><strong class="bd kk">Fine Stride Max Pooling</strong></figcaption></figure><p id="c358" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">如果对三维输出图进行平均，我们就可以得到预测。</p></div><div class="ab cl jr js hb jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="hm hn ho hp hq"><h1 id="6762" class="kl km ht bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li dt translated">3.多尺度分类</h1><p id="7460" class="pw-post-body-paragraph iq ir ht is b it ll iv iw ix lm iz ja jb mc jd je jf md jh ji jj me jl jm jn hm dt translated">与AlexNet中使用10视图预测不同，OverFeat输入整个图像以进行预测，比例为6:</p><figure class="jz ka kb kc fq kd fe ff paragraph-image"><div class="fe ff mg"><img src="../Images/6b57d81a436b17a654dda40cebeea066.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*QVk0adByNAOcYik1WxFAow.png"/></div><figcaption class="kg kh fg fe ff ki kj bd b be z ek"><strong class="bd kk">6 Different Scales</strong></figcaption></figure><p id="9ca5" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">使用6种不同大小的输入图像，产生不同空间分辨率的第5层未冷却特征图，从而提高精度。</p><p id="c45c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">测试时，FC层变为1 <strong class="is hu"> × </strong> 1 conv层。整个图像进入网络，得到一个类似VGGNet的类地图[7]。(如果有兴趣，请访问我的VGGNet评论[8]。)</p><figure class="jz ka kb kc fq kd fe ff paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="fe ff mh"><img src="../Images/7e869afee51ca0cedfea54139b6f0330.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1dBD766Enmg5wFko1hn05Q.png"/></div></div><figcaption class="kg kh fg fe ff ki kj bd b be z ek"><strong class="bd kk">Test Time</strong></figcaption></figure></div><div class="ab cl jr js hb jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="hm hn ho hp hq"><h1 id="19a2" class="kl km ht bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li dt translated">4.分类结果</h1><figure class="jz ka kb kc fq kd fe ff paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="fe ff mm"><img src="../Images/0fe54cc380ab0326545d21e5b6315b28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V910kY_5kL5JgIspbvUTug.png"/></div></div><figcaption class="kg kh fg fe ff ki kj bd b be z ek"><strong class="bd kk">Ablation Study of OverFeat</strong></figcaption></figure><p id="4e9d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">消融研究如上所述进行。</p><p id="4189" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"> AlexNet </strong>获得<strong class="is hu"> 18.2% </strong>前5位错误率。<strong class="is hu">快速模型</strong>根据上述修改获得较好的<strong class="is hu"> 17.12% </strong>错误率。(粗步幅表示使用传统的最大池。)</p><p id="f08a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"> 7快模式+ 4秤+精步最大汇集</strong>后，错误率大大降低到<strong class="is hu"> 13.86% </strong>。这7个模型实际上是在VGGNet、ZFNet、AlexNet、LeNet中已经普遍使用的增强技术或集成方法[3，5，7，9]。(如果感兴趣，请访问我对这些模型的评论。[4,6,8,10])</p><p id="e554" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"> 7个精确模型+ 4个比例尺+精细跨步最大汇集后，</strong>错误率大大降低到了<strong class="is hu"> 13.24% </strong>。</p><figure class="jz ka kb kc fq kd fe ff paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="fe ff mn"><img src="../Images/e175d56f95ba41e9a73b16d3cc1b4bc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MMOrrNub6b2tDpZTtZZSbg.png"/></div></div><figcaption class="kg kh fg fe ff ki kj bd b be z ek"><strong class="bd kk">Comparison with State-of-the-art Approaches</strong></figcaption></figure><p id="a4fc" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">在ILSVRC 2013中，<strong class="is hu"> ZFNet </strong>获得最佳结果，错误率<strong class="is hu"> 11.2%。<br/> <strong class="is hu">过食</strong>获得<strong class="is hu"> 13.6%的错误率</strong>，与安德鲁·霍华德在<strong class="is hu">排名第4位</strong>时相同，但为赛后结果。OverFeat的结果比ILSVRC 2012的冠军AlexNet好得多。</strong></p></div><div class="ab cl jr js hb jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="hm hn ho hp hq"><h1 id="f821" class="kl km ht bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li dt translated">5.用于定位/检测的回归网络</h1><p id="3457" class="pw-post-body-paragraph iq ir ht is b it ll iv iw ix lm iz ja jb mc jd je jf md jh ji jj me jl jm jn hm dt translated">有一个<strong class="is hu">回归网络</strong>连接在CNN的第五层。使用2个FC层(大小为4096和1024)对边界框边缘的坐标进行回归预测。示例如下:</p><figure class="jz ka kb kc fq kd fe ff paragraph-image"><div class="fe ff mo"><img src="../Images/a587c4501468d9c6c218fb035cfd52bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*0Tam0siZM4WjOBc7WfdqRw.png"/></div><figcaption class="kg kh fg fe ff ki kj bd b be z ek"><strong class="bd kk">Application of the regression network to layer 5 features, at scale 2, for example.</strong></figcaption></figure><p id="fd4d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">(a)该比例尺的回归器为6 <strong class="is hu"> × </strong> 7像素，每班256个通道。<br/> (b)回归网的第1层连接到第5层地图中的5 <strong class="is hu"> × </strong> 5空间邻域，以及所有256个通道。<br/>(三)第二回归层有1024个单元，完全连通。<br/> (d)回归网络的输出为4向量。</p><figure class="jz ka kb kc fq kd fe ff paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="fe ff mp"><img src="../Images/78b67c87d54dd22181696ef54f0216ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C6LVB0KR6gEX8KNYPHMINw.png"/></div></div><figcaption class="kg kh fg fe ff ki kj bd b be z ek"><strong class="bd kk">Ablation Study for Localization in ILSVRC 2012</strong></figcaption></figure><figure class="jz ka kb kc fq kd fe ff paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="fe ff mq"><img src="../Images/24d4753291317b418d14211c765f03f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HwBLnDicTNdhL4lxwwOaDg.png"/></div></div><figcaption class="kg kh fg fe ff ki kj bd b be z ek"><strong class="bd kk">Comparison with State-of-the-art Approaches for Localization Task</strong></figcaption></figure><p id="a77a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">作者尝试了多种组合，每类回归(PCR)效果不好，错误率为44.1%。<strong class="is hu">单类回归(SCR)和4个量表在验证集<br/>上实现了<br/> - </strong> 30.0%的错误率，在测试集、<br/>-<strong class="is hu">上实现了<strong class="is hu"> 29.9%的错误率，在ILSVRC 2013 </strong>上完成了本地化任务。</strong></p><figure class="jz ka kb kc fq kd fe ff paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="fe ff mr"><img src="../Images/07ef39e8c73bccc7e0a4f648691d7f6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U8fRhFjfSxzJgWAoD_9Cow.png"/></div></div><figcaption class="kg kh fg fe ff ki kj bd b be z ek"><strong class="bd kk">Comparison with State-of-the-art Approaches for Detection Task</strong></figcaption></figure><p id="1457" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">对于检测，主要的区别是评估度量以及在没有对象时预测背景类别的必要性。对于检测任务，OverFeat也获得了<strong class="is hu"> 24.3%的mAP(均值预测)，在赛后时刻优于其他方法。</strong></p><figure class="jz ka kb kc fq kd fe ff paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="fe ff ms"><img src="../Images/611dfd470642e49c63f4f70837e0a557.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UEE7YJl6HrafirUg4kg4EA.png"/></div></div><figcaption class="kg kh fg fe ff ki kj bd b be z ek"><strong class="bd kk">Some Bounding boxes Prediction Examples</strong></figcaption></figure><p id="5427" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">这里显示了一些边界框预测的例子。正如我们所见，有许多重叠的包围盒浪费了计算。</p><p id="feb0" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">然而，本文在图像分类和目标检测等领域启发了许多新的深度学习方法。</p><p id="b638" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">稍后，我将介绍其他最新的对象检测方法。请继续关注。</p></div><div class="ab cl jr js hb jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="hm hn ho hp hq"><h1 id="7a80" class="kl km ht bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li dt translated">参考</h1><ol class=""><li id="d2b4" class="lj lk ht is b it ll ix lm jb ln jf lo jj lp jn lq lr ls lt dt translated">【2014 ICLR】【过吃】<br/> <a class="ae jo" href="https://arxiv.org/pdf/1312.6229" rel="noopener ugc nofollow" target="_blank">过吃:使用卷积网络的综合识别、定位和检测</a></li><li id="9022" class="lj lk ht is b it lu ix lv jb lw jf lx jj ly jn lq lr ls lt dt translated">ILSVRC 2013年结果<br/><a class="ae jo" href="http://www.image-net.org/challenges/LSVRC/2013/results.php" rel="noopener ugc nofollow" target="_blank">http://www.image-net.org/challenges/LSVRC/2013/results.php</a></li><li id="0d86" class="lj lk ht is b it lu ix lv jb lw jf lx jj ly jn lq lr ls lt dt translated">【2012 NIPS】【Alex net】<br/><a class="ae jo" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank">使用深度卷积神经网络的ImageNet分类</a></li><li id="7f05" class="lj lk ht is b it lu ix lv jb lw jf lx jj ly jn lq lr ls lt dt translated"><a class="ae jo" rel="noopener" href="/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160">2012年国际影像分类奖得主AlexNet、CaffeNet综述</a></li><li id="5966" class="lj lk ht is b it lu ix lv jb lw jf lx jj ly jn lq lr ls lt dt translated">【2014 ECCV】【ZFNet】<br/><a class="ae jo" href="https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf" rel="noopener ugc nofollow" target="_blank">可视化和理解卷积网络</a></li><li id="8ebc" class="lj lk ht is b it lu ix lv jb lw jf lx jj ly jn lq lr ls lt dt translated"><a class="ae jo" rel="noopener" href="/coinmonks/paper-review-of-zfnet-the-winner-of-ilsvlc-2013-image-classification-d1a5a0c45103">2013年ILSVRC(图像分类)获奖者ZFNet点评</a></li><li id="c690" class="lj lk ht is b it lu ix lv jb lw jf lx jj ly jn lq lr ls lt dt translated">【2015 ICLR】【VGGNet】<br/><a class="ae jo" href="https://arxiv.org/pdf/1409.1556" rel="noopener ugc nofollow" target="_blank">用于大规模图像识别的极深度卷积网络</a></li><li id="293e" class="lj lk ht is b it lu ix lv jb lw jf lx jj ly jn lq lr ls lt dt translated"><a class="ae jo" rel="noopener" href="/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11">VG gnet回顾——ils VLC 2014(影像分类)亚军</a></li><li id="7446" class="lj lk ht is b it lu ix lv jb lw jf lx jj ly jn lq lr ls lt dt translated">[1998年Proc。IEEE] [LeNet-1，LeNet-4，LeNet-5，Boosted LeNet-4] <br/> <a class="ae jo" href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener ugc nofollow" target="_blank">基于梯度的学习应用于文档识别</a></li><li id="90fa" class="lj lk ht is b it lu ix lv jb lw jf lx jj ly jn lq lr ls lt dt translated"><a class="ae jo" rel="noopener" href="/@sh.tsang/paper-brief-review-of-lenet-1-lenet-4-lenet-5-boosted-lenet-4-image-classification-1f5f809dbf17">审查LeNet-1、LeNet-4、LeNet-5、Boosted LeNet-4(图像分类)</a></li></ol><blockquote class="mt"><p id="9d13" class="mu mv ht bd mw mx my mz na nb nc jn ek translated"><a class="ae jo" href="https://coincodecap.com/?utm_source=coinmonks" rel="noopener ugc nofollow" target="_blank">直接在您的收件箱中获得最佳软件交易</a></p></blockquote><figure class="ne nf ng nh ni kd fe ff paragraph-image"><a href="https://coincodecap.com/?utm_source=coinmonks"><div class="fe ff nd"><img src="../Images/7c0b3dfdcbfea594cc0ae7d4f9bf6fcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/0*OJ-qb5G6i863msBB.png"/></div></a></figure></div></div>    
</body>
</html>