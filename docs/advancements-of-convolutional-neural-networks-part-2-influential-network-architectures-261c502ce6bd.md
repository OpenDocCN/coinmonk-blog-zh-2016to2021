# 深度学习的进展 2:有影响力的卷积神经网络架构

> 原文：<https://medium.com/coinmonks/advancements-of-convolutional-neural-networks-part-2-influential-network-architectures-261c502ce6bd?source=collection_archive---------5----------------------->

![](img/ca885c278eea1b2085b1cd927fac91ab.png)

Visualization from [http://terencebroad.com/convnetvis/vis.html](http://terencebroad.com/convnetvis/vis.html)

今天，深度学习已经成为人工智能中一种强大而流行的算法。卷积神经网络(CNN)是主要的深度学习架构之一，成功用于许多不同的应用和不断发展的研究领域。从上一篇[文章第一部分](/@dashankanadeeshandesilva/advancements-of-convolutional-neural-networks-part-1-introduction-and-main-operations-5d12f35b28d4)开始，我们简要讨论了的主要操作及其最新进展。本文简要讨论了最有影响力的 CNN 架构。

CNN 的学习主要基于从输入直接从局部特征开始到逐步构建高层特征的过程。Hubel 和 Wiesel 在 1959 年首次提出了**分级学习**的概念。他们发现了初级视觉皮层学习过程的概念(所谓的猫实验)，这后来成为 CNN 的灵感。然后在 1980 年，[福岛*等人*](http://www.scholarpedia.org/article/Neocognitron) 构建了第一个学习型人工神经网络，叫做 Neocognitron。

## [**【LeNet 5】**](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)**(1994)**

![](img/92751ba1aaa8210cc6fca62173b52fd3.png)

LeNet-5 architecture as published in the [original paper](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf).

美国有线电视新闻网于 1994 年推出的首批节目之一，由深度学习领域的先驱科学家之一 Yann LeCun 开发。LeNet 5 能够使用与由可学习参数组成的滤波器核的卷积来提取对图像(输入)的特征洞察。由于这些滤波器仅在单个卷积(也称为感受野)期间观察局部面片，因此与传统的神经网络模型相比，该网络需要更少数量的可训练参数。然而，整个输入区域上的卷积允许网络的层将分布的空间特征映射到整个输入区域上，这被称为特征映射。
传统神经网络与传统人工神经网络的主要区别之一在于神经元之间的连接没有完全连接。相反，每个卷积只考虑局部面片。这有助于节省大量计算成本。LeNet 5 使用了 3 种类型的图层序列:卷积、非线性和汇集，这将成为以后 CNN 图层布局的标准顺序。该架构包括 7 层、3 个卷积层和 2 个完全连接的层(或者我们可以说是传统的神经网络连接)。对于非线性，使用了双曲正切挤压函数。总之，LeNet 5 为 CNNs 奠定了基础。

**其中一个主要缺点**是，直到最近，像图形处理器这样的时代还没有太多的计算能力可用，并且中央处理器不够快。这一事实使得人们直到 2012 年才开始关注这一发现(然而，在 2010 年有了一些小的进步，比如 Dan Ciresan Net)。从 1998 年到 2012 年，计算机科学领域有了许多进步和发展。尤其是互联网使用率的快速增长和消费电子产品使用率的快速增长，产生了越来越多的数据，如图像和其他形式的数据。此外，通用计算中 GPU 的引入以及 CPU 越来越强大使得深度学习问题的解决变得可行且高效。

## [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) (2012)

![](img/42dfaa813da6f78990f7914860ea0790.png)

AlexNet architecture as published in the [original paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)

可以认为是第一个现代成功的 CNN 架构的网络模型，由 Alex Krizhevsky 在 2012 年开发。AlexNet 比 LeNet 更有深度，能够以较大优势赢得 2012 年的 ImageNet 挑战赛。这是第一次 CNN 模特参加 ImageNet 挑战赛，所取得的成功使 AlexNet 闻名遐迩。经典的 AlexNet 架构包括 5 个卷积层和 2 个全连接层。有几个方面使得这个网络如此强大。Krizhevsky 等人使用了比 tanh 和 sigmoids 性能更好的整流线性单元。为了引入泛化或防止网络过度拟合，使用了一种称为丢弃的技术，该技术基本上是在训练阶段丢弃相邻层之间的神经元连接。一个重要的事实是，网络是使用英伟达 GTX 580 图形处理器训练的。这使得训练过程比使用 CPU 快 10 倍，因为与 CPU 相比，GPU 中的核心数量要多得多。由于 AlexNet 比 LeNet 更深，因此在此之后，使用网络模型更深层次的概念开始发挥作用。所以一些研究人员开始建立更深层次的网络。

## VGG (2014 年)

![](img/7a86fd49e3e70cbcb063559c975caf6a.png)

VGG networks configuration table from the [original paper](https://arxiv.org/pdf/1409.1556.pdf)

随着更深层次的 CNN 的概念，Simonyan *等人*引入了 VGG 网络。VGG 网络与 AlexNet 有两个主要的不同之处:网络的深度和卷积滤波器的大小。文中介绍了几种网络深度配置。其中，VGG 16 和 VGG 19 是最著名的架构，分别有 13 和 16 个卷积层，每个网络有 3 个全连接层。对于 ImgeNet，AlexNet 中使用的卷积滤波器的大小或感受野的大小是 11×11。但是在 VGG 网络中，作者使用了 3×3 感受野，而不是之前表现最好的条目所使用的更大的各个感受野，包括步长为 4 的 11×11(Krizhevsky 等人，2012)和步长为 2 的 7×7(泽勒&弗格斯，2013；Sermanet 等人，2014 年)。这个想法是通过一个 3×3 的序列来模拟更大的感受野。如论文中所解释的，两个 3×3 卷积层覆盖 5×5 的有效感受野，三个 3×3 卷积层具有 7×7 的有效感受野。为什么他们使用这样的方法，而不是坚持旧的更大的感受野？主要有两个原因。第一个是，这使得网络能够用三个非线性整流而不是单个非线性整流来并入正向流。额外的非线性引入使得决策函数更具鉴别性。第二，这种方法大大减少了参数的数量。如果我们从数字的角度来理解这一点。具有 n 个输入和输出通道的三层 3×3 卷积堆叠具有 3(n×3×3×n) = 27n，但是对于具有相同通道的单个 7×7 卷积层将需要(n×7×7×n) =7 n = 49n 这大约是直接影响学习时间和能力的 81%以上的参数。但是这里出现的缺点是许多层，因此开发了大量的特征(地图)。这使得部署 VGG 网络在内存和时间两方面的计算成本都很高。

## [谷歌网/盗梦空间 V1](https://arxiv.org/pdf/1409.4842) (2014 年)

![](img/c56e34e88b2b2983c0925b3f19b06760.png)

[GoogLeNet/ Inception V1 network](https://arxiv.org/abs/1409.4842)

也代号为 Inception(第 1 版)，他在 2014 年的 ImageNet 挑战赛中建立了一个新的艺术分类。GoogLeNet 是 22 层深度的 CNN，但整体架构视图与典型的 CNN 非常不同。有趣的事实是，尽管网络的深度和分散的架构，GoogLeNet 中的参数数量比 AlexNet 少 12 倍。这导致计算负担的巨大减少，同时，它比当时的艺术表现状态(ImageNet)更准确。作者通过提出 Inception 模块使这一切成为可能。

![](img/6041962120d459bedc8940e8d7114a8c.png)

[Inception module](https://arxiv.org/abs/1409.4842)

论文中的上图展示了初始模块的基本结构，他们也称之为幼稚版本。看起来，Inception 模块是两层之间的一堆并行连接。这些连接操作 1×1、3×3 和 5×5 卷积。1×1 卷积能够检测局部斑块中的相关性，而像 3×3 和 5×5 这样的较大卷积检测相对较大的空间洞察力。但是出现的问题是，这种大量的滤波器使得整个过程在计算上很昂贵。通过引入所谓的瓶颈层来解决。

![](img/6c40566fa1e5f890ecf64c347721fc48.png)

[Bottleneck layer of inception module](https://arxiv.org/abs/1409.4842)

瓶颈层的基本思想是在 3×3 和 5×5 卷积之前应用 1×1 卷积，如上图中的初始模块所示。正如论文中所解释的，“一般来说，初始网络是由上述类型的模块相互堆叠而成的网络，偶尔会有跨度为 2 的最大池层，以将网格的分辨率减半”。瓶颈层所做的是，首先使用数量减少的输入特征执行 1×1 卷积，而不是使用计算量大的 3×3 或 5×5 将所有输入特征卷积到输出。然后对 3×3 和 5×5 的初始模块分支执行卷积。最后是输出特征的串联。如果我们用一个例子来理解这一点(如论文中所解释的)，让我们以一个有 192 个输入通道的初始模块(第一个模块)为例。该模块有 128 个 3×3 滤波器和 32 个 5×5 滤波器。每个滤波器的计算量将为 192×3×3×128=221，184 和 192× 5×5×32=153，600，这是一个很大的数字，随着网络的深入，可能会越来越大。因此，如前所述，初始模块在应用更大的卷积之前应用 1×1 卷积。在这篇论文中，他们使用了 16，1×1 卷积，因此将发生 192×1×1×16 次计算。此外，使用 3×3 的总计算次数将是 192×1×1×16+16×3×3×128=21，504，而使用 5×5 的总计算次数将是 192×1×1×16+16×5×32 = 15，872。这些评估表明，与普通的 CNN 连接相比，Inception 模块能够将操作数量减少近 10 倍。

除了卷积层中的所有这些修改之外，GoogLeNet 还将完全连接的层替换为来自最后一个卷积层的简单全局平均池。这也有助于显著减少参数的数量。另一个重要的架构修改是，作者将辅助分类器连接到中间层，以成功地将梯度传播回每一层，从而调整可学习的参数并防止消失梯度问题。

# **盗梦空间 2015 年初)**

2015 年初，塞格迪等人带来了《盗梦空间》的第二版《盗梦 V2》。根据新颖性，在初始阶段引入了[批量规范化](https://arxiv.org/pdf/1502.03167)。一般来说，批量标准化为学习到的特征引入了数据白化过程。这是通过计算所有特征图的平均值和标准偏差来完成的，并且通过使它们的平均值为零并且具有相同的范围来标准化响应。这有助于训练过程，不必担心学习下一层输入的数据偏移。

# [**《盗梦空间》V3**](https://arxiv.org/pdf/1512.00567.pdf)**(2015 年末)**

下一个版本在同年年底发布了新版本的 Inception，Inception V3。他们做了几处修改。用多个 3×3 分解 5×5 和 7×7 滤波器。这个过程也被称为卷积的因式分解。

![](img/87dcd75aeaff294064eb9ad082def9d5.png)

Factorization of convolutions from [Inception V3](https://arxiv.org/pdf/1512.00567.pdf)

第二种方法是通过 1×n 卷积后接 n×1 卷积对 n×n 卷积进行因式分解。理论上，这种方法可以随着 *n* 的增长而降低计算成本，但是在实践中，因式分解在中等网格尺寸上效果很好。因此，作者使用了 1×7 和随后的 7×1 卷积。

![](img/cd2c3e7bb3329c40e0ea005a479d4488.png)

Factorization of **nxn** convolutions from [Inception V3](https://arxiv.org/pdf/1512.00567.pdf)

在执行初始计算时，初始模块还可以通过提供池来减少数据的大小。这基本上等同于使用简单的池层并行执行跨距卷积。

第一个初始网络引入了辅助分类器，以确保梯度流过整个网络，并通过解决梯度消失问题来改善训练阶段的收敛性。来自第三版(论文)的作者声称，辅助分支的优势出现在接近训练结束时，并且认为辅助分类器更可能充当正则化器。

# [ResNet](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) (2015)

从理论上讲，网络越深，学习复杂特征表示的能力就越强，因此模型精度也就越高。但在实践中，这些网络往往会过度拟合，并遭受梯度消失的问题。当训练这种更深的网络时，另一个问题是退化问题。在训练阶段，较深的网络有大量的参数要优化。增加更多的层或增加网络的深度将导致更高的训练误差的产生。这就是所谓的退化问题。作者通过引入深度剩余框架解决了这个问题。

![](img/adac017f209754a960969b1b1cb70a3a.png)

[ResNet](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) residual module

假设我们在网络中增加了一层。训练误差要不要超出先前的误差(在添加层之前)可以有两个选项。新添加的层不执行学习或者仅执行身份映射。通过阐述这一思想，作者引入了剩余框架。这个框架包括提供退化问题解决方案的剩余模块。如果我们考虑一个模块，它在模块的输入和输出之间创建了一个直接路径(如上图所示)。这些快捷连接执行身份映射，且不包含任何参数或计算复杂性。由于这种身份映射在添加额外层之前提供了网络的原始条件，所以额外层只需学习残差。这被称为残差映射，并且它不会不期望地影响训练误差。如果我们看上面的图像， *F(x)* 是剩余映射，而 *x* 提供了单位映射。残差模块最终将映射连接为元素相加，并通过非线性处理来生成输出。此外，该论文声称残差块可以包含两层或三层，因为残差( *F* )函数可以灵活地这样做。因此，所提出的残差网络和平面网络(没有残差连接)具有相同数量的参数、深度、宽度和计算成本，除了可忽略的逐元素添加。作者评估并展示了具有 152 层深度的 ResNet 架构已经实现了比 VGG 网络和 GoogLeNet 更好的分类精度和计算效率。

# [移动互联网](https://arxiv.org/pdf/1704.04861.pdf) (2017)

![](img/b647eb69e8bc57a9a49d6991c5c48552.png)

[MobileNet](https://arxiv.org/pdf/1704.04861.pdf) architecture

对于硬件能力有限的实时应用，如移动应用或嵌入式系统，CNN 固有地消耗资源且麻烦。因此，2017 年由 Howard *等人*推出 MobileNet。来解决这个问题。核心思想是通过用深度方向上可分离的卷积代替标准卷积，使网络更有效。标准卷积的基本思想是组合输入区域(二维)和整个输入深度上的局部面片的输出。深度可分卷积是因子分解卷积的一种形式。它基本上将标准卷积过程分为两个不同的步骤。首先是深度方向卷积，其次是点方向卷积。深度方向卷积在单个深度切片内的正方形区域上执行卷积。逐点卷积使用 1×1 卷积在整个深度上合并从上一步获得的信息。MobileNet 架构是使用所讨论的深度方向可分离卷积和用于第一层的单个全卷积来构建的。

在过去的十年中，进行了大量的研究，并引入了许多 CNN 架构变体。但是核心原则和框架保持相似。为了彻底理解所讨论的架构背后的直觉和算法，强烈建议浏览相关的公开论文(这里也提到了)。当我们考察一段时间内的总体发展时，可以清楚地看到这个领域的进展有多快。一般来说，CNN 分类的分类误差(来自 ILSVRC)每年都变得更小，2016 年已经超过了 5%的人为误差。本文仅讨论了影响当前艺术水平的最有影响力的 CNN 架构。但是，文献中还有许多其他架构发展和算法可供讨论。