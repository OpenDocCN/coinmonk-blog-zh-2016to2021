<html>
<head>
<title>Advancements of Deep Learning 2: Influential convolutional neural network architectures</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习的进展2:有影响力的卷积神经网络架构</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/advancements-of-convolutional-neural-networks-part-2-influential-network-architectures-261c502ce6bd?source=collection_archive---------5-----------------------#2018-07-04">https://medium.com/coinmonks/advancements-of-convolutional-neural-networks-part-2-influential-network-architectures-261c502ce6bd?source=collection_archive---------5-----------------------#2018-07-04</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><figure class="hs ht fm fo hu hv fe ff paragraph-image"><div role="button" tabindex="0" class="hw hx di hy bf hz"><div class="fe ff hr"><img src="../Images/ca885c278eea1b2085b1cd927fac91ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h_eEe9kdcq7RXw7fLZk1_g.png"/></div></div><figcaption class="ic id fg fe ff ie if bd b be z ek">Visualization from <a class="ae ig" href="http://terencebroad.com/convnetvis/vis.html" rel="noopener ugc nofollow" target="_blank">http://terencebroad.com/convnetvis/vis.html</a></figcaption></figure><div class=""/><p id="027a" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">今天，深度学习已经成为人工智能中一种强大而流行的算法。卷积神经网络(CNN)是主要的深度学习架构之一，成功用于许多不同的应用和不断发展的研究领域。从上一篇<a class="ae ig" rel="noopener" href="/@dashankanadeeshandesilva/advancements-of-convolutional-neural-networks-part-1-introduction-and-main-operations-5d12f35b28d4">文章第一部分</a>开始，我们简要讨论了的主要操作及其最新进展。本文简要讨论了最有影响力的CNN架构。</p><p id="e439" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">CNN的学习主要基于从输入直接从局部特征开始到逐步构建高层特征的过程。Hubel和Wiesel在1959年首次提出了<strong class="ji ik">分级学习</strong>的概念。他们发现了初级视觉皮层学习过程的概念(所谓的猫实验)，这后来成为CNN的灵感。然后在1980年，<a class="ae ig" href="http://www.scholarpedia.org/article/Neocognitron" rel="noopener ugc nofollow" target="_blank">福岛<em class="ke">等人</em> </a>构建了第一个学习型人工神经网络，叫做Neocognitron。</p><h2 id="4998" class="kf kg ij bd kh ki kj kk kl km kn ko kp jr kq kr ks jv kt ku kv jz kw kx ky kz dt translated"><a class="ae ig" href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="ak">【LeNet 5】</strong></a><strong class="ak">(1994)</strong></h2><figure class="lb lc ld le fq hv fe ff paragraph-image"><div role="button" tabindex="0" class="hw hx di hy bf hz"><div class="fe ff la"><img src="../Images/92751ba1aaa8210cc6fca62173b52fd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hhrR6fi_LcaGn5r0OTs5HQ.jpeg"/></div></div><figcaption class="ic id fg fe ff ie if bd b be z ek">LeNet-5 architecture as published in the <a class="ae ig" href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener ugc nofollow" target="_blank">original paper</a>.</figcaption></figure><p id="1c4c" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">美国有线电视新闻网于1994年推出的首批节目之一，由深度学习领域的先驱科学家之一Yann LeCun开发。LeNet 5能够使用与由可学习参数组成的滤波器核的卷积来提取对图像(输入)的特征洞察。由于这些滤波器仅在单个卷积(也称为感受野)期间观察局部面片，因此与传统的神经网络模型相比，该网络需要更少数量的可训练参数。然而，整个输入区域上的卷积允许网络的层将分布的空间特征映射到整个输入区域上，这被称为特征映射。<br/>传统神经网络与传统人工神经网络的主要区别之一在于神经元之间的连接没有完全连接。相反，每个卷积只考虑局部面片。这有助于节省大量计算成本。LeNet 5使用了3种类型的图层序列:卷积、非线性和汇集，这将成为以后CNN图层布局的标准顺序。该架构包括7层、3个卷积层和2个完全连接的层(或者我们可以说是传统的神经网络连接)。对于非线性，使用了双曲正切挤压函数。总之，LeNet 5为CNNs奠定了基础。</p><p id="bb3f" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated"><strong class="ji ik">其中一个主要缺点</strong>是，直到最近，像图形处理器这样的时代还没有太多的计算能力可用，并且中央处理器不够快。这一事实使得人们直到2012年才开始关注这一发现(然而，在2010年有了一些小的进步，比如Dan Ciresan Net)。从1998年到2012年，计算机科学领域有了许多进步和发展。尤其是互联网使用率的快速增长和消费电子产品使用率的快速增长，产生了越来越多的数据，如图像和其他形式的数据。此外，通用计算中GPU的引入以及CPU越来越强大使得深度学习问题的解决变得可行且高效。</p><h2 id="86b3" class="kf kg ij bd kh ki kj kk kl km kn ko kp jr kq kr ks jv kt ku kv jz kw kx ky kz dt translated"><a class="ae ig" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank"> AlexNet </a> (2012)</h2><figure class="lb lc ld le fq hv fe ff paragraph-image"><div role="button" tabindex="0" class="hw hx di hy bf hz"><div class="fe ff lf"><img src="../Images/42dfaa813da6f78990f7914860ea0790.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*arJJYgK-_7VcuKKX1TCKMA.png"/></div></div><figcaption class="ic id fg fe ff ie if bd b be z ek">AlexNet architecture as published in the <a class="ae ig" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank">original paper</a></figcaption></figure><p id="a888" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">可以认为是第一个现代成功的CNN架构的网络模型，由Alex Krizhevsky在2012年开发。AlexNet比LeNet更有深度，能够以较大优势赢得2012年的ImageNet挑战赛。这是第一次CNN模特参加ImageNet挑战赛，所取得的成功使AlexNet闻名遐迩。经典的AlexNet架构包括5个卷积层和2个全连接层。有几个方面使得这个网络如此强大。Krizhevsky等人使用了比tanh和sigmoids性能更好的整流线性单元。为了引入泛化或防止网络过度拟合，使用了一种称为丢弃的技术，该技术基本上是在训练阶段丢弃相邻层之间的神经元连接。一个重要的事实是，网络是使用英伟达GTX 580图形处理器训练的。这使得训练过程比使用CPU快10倍，因为与CPU相比，GPU中的核心数量要多得多。由于AlexNet比LeNet更深，因此在此之后，使用网络模型更深层次的概念开始发挥作用。所以一些研究人员开始建立更深层次的网络。</p><h2 id="45ea" class="kf kg ij bd kh ki kj kk kl km kn ko kp jr kq kr ks jv kt ku kv jz kw kx ky kz dt translated">VGG  (2014年)</h2><figure class="lb lc ld le fq hv fe ff paragraph-image"><div class="fe ff lg"><img src="../Images/7a86fd49e3e70cbcb063559c975caf6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*NIN7l2u-nTpj7ofsf9LRCw.png"/></div><figcaption class="ic id fg fe ff ie if bd b be z ek">VGG networks configuration table from the <a class="ae ig" href="https://arxiv.org/pdf/1409.1556.pdf" rel="noopener ugc nofollow" target="_blank">original paper</a></figcaption></figure><p id="aef6" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">随着更深层次的CNN的概念，Simonyan <em class="ke">等人</em>引入了VGG网络。VGG网络与AlexNet有两个主要的不同之处:网络的深度和卷积滤波器的大小。文中介绍了几种网络深度配置。其中，VGG 16和VGG 19是最著名的架构，分别有13和16个卷积层，每个网络有3个全连接层。对于ImgeNet，AlexNet中使用的卷积滤波器的大小或感受野的大小是11×11。但是在VGG网络中，作者使用了3×3感受野，而不是之前表现最好的条目所使用的更大的各个感受野，包括步长为4的11×11(Krizhevsky等人，2012)和步长为2的7×7(泽勒&amp;弗格斯，2013；Sermanet等人，2014年)。这个想法是通过一个3×3的序列来模拟更大的感受野。如论文中所解释的，两个3×3卷积层覆盖5×5的有效感受野，三个3×3卷积层具有7×7的有效感受野。为什么他们使用这样的方法，而不是坚持旧的更大的感受野？主要有两个原因。第一个是，这使得网络能够用三个非线性整流而不是单个非线性整流来并入正向流。额外的非线性引入使得决策函数更具鉴别性。第二，这种方法大大减少了参数的数量。如果我们从数字的角度来理解这一点。具有n个输入和输出通道的三层3×3卷积堆叠具有3(n×3×3×n) = 27n，但是对于具有相同通道的单个7×7卷积层将需要(n×7×7×n) =7 n = 49n这大约是直接影响学习时间和能力的81%以上的参数。但是这里出现的缺点是许多层，因此开发了大量的特征(地图)。这使得部署VGG网络在内存和时间两方面的计算成本都很高。</p><h2 id="9e0e" class="kf kg ij bd kh ki kj kk kl km kn ko kp jr kq kr ks jv kt ku kv jz kw kx ky kz dt translated"><a class="ae ig" href="https://arxiv.org/pdf/1409.4842" rel="noopener ugc nofollow" target="_blank">谷歌网/盗梦空间V1 </a> (2014年)</h2><figure class="lb lc ld le fq hv fe ff paragraph-image"><div class="fe ff lh"><img src="../Images/c56e34e88b2b2983c0925b3f19b06760.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*1Dluwy-x0eMr5igLl9RiXQ.png"/></div><figcaption class="ic id fg fe ff ie if bd b be z ek"><a class="ae ig" href="https://arxiv.org/abs/1409.4842" rel="noopener ugc nofollow" target="_blank">GoogLeNet/ Inception V1 network</a></figcaption></figure><p id="0cbc" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">也代号为Inception(第1版)，他在2014年的ImageNet挑战赛中建立了一个新的艺术分类。GoogLeNet是22层深度的CNN，但整体架构视图与典型的CNN非常不同。有趣的事实是，尽管网络的深度和分散的架构，GoogLeNet中的参数数量比AlexNet少12倍。这导致计算负担的巨大减少，同时，它比当时的艺术表现状态(ImageNet)更准确。作者通过提出Inception模块使这一切成为可能。</p><figure class="lb lc ld le fq hv fe ff paragraph-image"><div role="button" tabindex="0" class="hw hx di hy bf hz"><div class="fe ff li"><img src="../Images/6041962120d459bedc8940e8d7114a8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XA99YokK8_PPuT2-jTFFCQ.jpeg"/></div></div><figcaption class="ic id fg fe ff ie if bd b be z ek"><a class="ae ig" href="https://arxiv.org/abs/1409.4842" rel="noopener ugc nofollow" target="_blank">Inception module</a></figcaption></figure><p id="6c84" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">论文中的上图展示了初始模块的基本结构，他们也称之为幼稚版本。看起来，Inception模块是两层之间的一堆并行连接。这些连接操作1×1、3×3和5×5卷积。1×1卷积能够检测局部斑块中的相关性，而像3×3和5×5这样的较大卷积检测相对较大的空间洞察力。但是出现的问题是，这种大量的滤波器使得整个过程在计算上很昂贵。通过引入所谓的瓶颈层来解决。</p><figure class="lb lc ld le fq hv fe ff paragraph-image"><div class="fe ff lj"><img src="../Images/6c40566fa1e5f890ecf64c347721fc48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*gGepuA1MGC-7IBy9UrroMg.jpeg"/></div><figcaption class="ic id fg fe ff ie if bd b be z ek"><a class="ae ig" href="https://arxiv.org/abs/1409.4842" rel="noopener ugc nofollow" target="_blank">Bottleneck layer of inception module</a></figcaption></figure><p id="316f" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">瓶颈层的基本思想是在3×3和5×5卷积之前应用1×1卷积，如上图中的初始模块所示。正如论文中所解释的，“一般来说，初始网络是由上述类型的模块相互堆叠而成的网络，偶尔会有跨度为2的最大池层，以将网格的分辨率减半”。瓶颈层所做的是，首先使用数量减少的输入特征执行1×1卷积，而不是使用计算量大的3×3或5×5将所有输入特征卷积到输出。然后对3×3和5×5的初始模块分支执行卷积。最后是输出特征的串联。如果我们用一个例子来理解这一点(如论文中所解释的)，让我们以一个有192个输入通道的初始模块(第一个模块)为例。该模块有128个3×3滤波器和32个5×5滤波器。每个滤波器的计算量将为192×3×3×128=221，184和192× 5×5×32=153，600，这是一个很大的数字，随着网络的深入，可能会越来越大。因此，如前所述，初始模块在应用更大的卷积之前应用1×1卷积。在这篇论文中，他们使用了16，1×1卷积，因此将发生192×1×1×16次计算。此外，使用3×3的总计算次数将是192×1×1×16+16×3×3×128=21，504，而使用5×5的总计算次数将是192×1×1×16+16×5×32 = 15，872。这些评估表明，与普通的CNN连接相比，Inception模块能够将操作数量减少近10倍。</p><p id="696c" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">除了卷积层中的所有这些修改之外，GoogLeNet还将完全连接的层替换为来自最后一个卷积层的简单全局平均池。这也有助于显著减少参数的数量。另一个重要的架构修改是，作者将辅助分类器连接到中间层，以成功地将梯度传播回每一层，从而调整可学习的参数并防止消失梯度问题。</p><h1 id="4278" class="lk kg ij bd kh ll lm ln kl lo lp lq kp lr ls lt ks lu lv lw kv lx ly lz ky ma dt translated"><strong class="ak">盗梦空间2015年初)</strong></h1><p id="c888" class="pw-post-body-paragraph jg jh ij ji b jj mb jl jm jn mc jp jq jr md jt ju jv me jx jy jz mf kb kc kd hm dt translated">2015年初，塞格迪等人带来了《盗梦空间》的第二版《盗梦V2》。根据新颖性，在初始阶段引入了<a class="ae ig" href="https://arxiv.org/pdf/1502.03167" rel="noopener ugc nofollow" target="_blank">批量规范化</a>。一般来说，批量标准化为学习到的特征引入了数据白化过程。这是通过计算所有特征图的平均值和标准偏差来完成的，并且通过使它们的平均值为零并且具有相同的范围来标准化响应。这有助于训练过程，不必担心学习下一层输入的数据偏移。</p><h1 id="f3c8" class="lk kg ij bd kh ll lm ln kl lo lp lq kp lr ls lt ks lu lv lw kv lx ly lz ky ma dt translated"><a class="ae ig" href="https://arxiv.org/pdf/1512.00567.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">《盗梦空间》V3</strong></a><strong class="ak">(2015年末)</strong></h1><p id="cc20" class="pw-post-body-paragraph jg jh ij ji b jj mb jl jm jn mc jp jq jr md jt ju jv me jx jy jz mf kb kc kd hm dt translated">下一个版本在同年年底发布了新版本的Inception，Inception V3。他们做了几处修改。用多个3×3分解5×5和7×7滤波器。这个过程也被称为卷积的因式分解。</p><figure class="lb lc ld le fq hv fe ff paragraph-image"><div role="button" tabindex="0" class="hw hx di hy bf hz"><div class="fe ff mg"><img src="../Images/87dcd75aeaff294064eb9ad082def9d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_qzeiVZQvr9MARcizjiJeg.jpeg"/></div></div><figcaption class="ic id fg fe ff ie if bd b be z ek">Factorization of convolutions from <a class="ae ig" href="https://arxiv.org/pdf/1512.00567.pdf" rel="noopener ugc nofollow" target="_blank">Inception V3</a></figcaption></figure><p id="9fd6" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">第二种方法是通过1×n卷积后接n×1卷积对n×n卷积进行因式分解。理论上，这种方法可以随着<em class="ke"> n </em>的增长而降低计算成本，但是在实践中，因式分解在中等网格尺寸上效果很好。因此，作者使用了1×7和随后的7×1卷积。</p><figure class="lb lc ld le fq hv fe ff paragraph-image"><div role="button" tabindex="0" class="hw hx di hy bf hz"><div class="fe ff mh"><img src="../Images/cd2c3e7bb3329c40e0ea005a479d4488.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NAweZsa7726u3LKMnLr6nA.jpeg"/></div></div><figcaption class="ic id fg fe ff ie if bd b be z ek">Factorization of <strong class="bd mi">nxn </strong>convolutions from <a class="ae ig" href="https://arxiv.org/pdf/1512.00567.pdf" rel="noopener ugc nofollow" target="_blank">Inception V3</a></figcaption></figure><p id="1ca6" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">在执行初始计算时，初始模块还可以通过提供池来减少数据的大小。这基本上等同于使用简单的池层并行执行跨距卷积。</p><p id="0350" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">第一个初始网络引入了辅助分类器，以确保梯度流过整个网络，并通过解决梯度消失问题来改善训练阶段的收敛性。来自第三版(论文)的作者声称，辅助分支的优势出现在接近训练结束时，并且认为辅助分类器更可能充当正则化器。</p><h1 id="45c0" class="lk kg ij bd kh ll lm ln kl lo lp lq kp lr ls lt ks lu lv lw kv lx ly lz ky ma dt translated"><a class="ae ig" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank"> ResNet </a> (2015)</h1><p id="e157" class="pw-post-body-paragraph jg jh ij ji b jj mb jl jm jn mc jp jq jr md jt ju jv me jx jy jz mf kb kc kd hm dt translated">从理论上讲，网络越深，学习复杂特征表示的能力就越强，因此模型精度也就越高。但在实践中，这些网络往往会过度拟合，并遭受梯度消失的问题。当训练这种更深的网络时，另一个问题是退化问题。在训练阶段，较深的网络有大量的参数要优化。增加更多的层或增加网络的深度将导致更高的训练误差的产生。这就是所谓的退化问题。作者通过引入深度剩余框架解决了这个问题。</p><figure class="lb lc ld le fq hv fe ff paragraph-image"><div class="fe ff mj"><img src="../Images/adac017f209754a960969b1b1cb70a3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*JlIRJObnfkrVrFWMGiW-rg.png"/></div><figcaption class="ic id fg fe ff ie if bd b be z ek"><a class="ae ig" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank">ResNet</a> residual module</figcaption></figure><p id="bfcc" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">假设我们在网络中增加了一层。训练误差要不要超出先前的误差(在添加层之前)可以有两个选项。新添加的层不执行学习或者仅执行身份映射。通过阐述这一思想，作者引入了剩余框架。这个框架包括提供退化问题解决方案的剩余模块。如果我们考虑一个模块，它在模块的输入和输出之间创建了一个直接路径(如上图所示)。这些快捷连接执行身份映射，且不包含任何参数或计算复杂性。由于这种身份映射在添加额外层之前提供了网络的原始条件，所以额外层只需学习残差。这被称为残差映射，并且它不会不期望地影响训练误差。如果我们看上面的图像，<em class="ke"> F(x) </em>是剩余映射，而<em class="ke"> x </em>提供了单位映射。残差模块最终将映射连接为元素相加，并通过非线性处理来生成输出。此外，该论文声称残差块可以包含两层或三层，因为残差(<em class="ke"> F </em>)函数可以灵活地这样做。因此，所提出的残差网络和平面网络(没有残差连接)具有相同数量的参数、深度、宽度和计算成本，除了可忽略的逐元素添加。作者评估并展示了具有152层深度的ResNet架构已经实现了比VGG网络和GoogLeNet更好的分类精度和计算效率。</p><h1 id="febb" class="lk kg ij bd kh ll lm ln kl lo lp lq kp lr ls lt ks lu lv lw kv lx ly lz ky ma dt translated"><a class="ae ig" href="https://arxiv.org/pdf/1704.04861.pdf" rel="noopener ugc nofollow" target="_blank">移动互联网</a> (2017)</h1><figure class="lb lc ld le fq hv fe ff paragraph-image"><div role="button" tabindex="0" class="hw hx di hy bf hz"><div class="fe ff mk"><img src="../Images/b647eb69e8bc57a9a49d6991c5c48552.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B58oYwA4W2E5KptwCjWtKQ.jpeg"/></div></div><figcaption class="ic id fg fe ff ie if bd b be z ek"><a class="ae ig" href="https://arxiv.org/pdf/1704.04861.pdf" rel="noopener ugc nofollow" target="_blank">MobileNet</a> architecture</figcaption></figure><p id="9330" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">对于硬件能力有限的实时应用，如移动应用或嵌入式系统，CNN固有地消耗资源且麻烦。因此，2017年由Howard <em class="ke">等人</em>推出MobileNet。来解决这个问题。核心思想是通过用深度方向上可分离的卷积代替标准卷积，使网络更有效。标准卷积的基本思想是组合输入区域(二维)和整个输入深度上的局部面片的输出。深度可分卷积是因子分解卷积的一种形式。它基本上将标准卷积过程分为两个不同的步骤。首先是深度方向卷积，其次是点方向卷积。深度方向卷积在单个深度切片内的正方形区域上执行卷积。逐点卷积使用1×1卷积在整个深度上合并从上一步获得的信息。MobileNet架构是使用所讨论的深度方向可分离卷积和用于第一层的单个全卷积来构建的。</p></div><div class="ab cl ml mm hb mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="hm hn ho hp hq"><p id="670a" class="pw-post-body-paragraph jg jh ij ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">在过去的十年中，进行了大量的研究，并引入了许多CNN架构变体。但是核心原则和框架保持相似。为了彻底理解所讨论的架构背后的直觉和算法，强烈建议浏览相关的公开论文(这里也提到了)。当我们考察一段时间内的总体发展时，可以清楚地看到这个领域的进展有多快。一般来说，CNN分类的分类误差(来自ILSVRC)每年都变得更小，2016年已经超过了5%的人为误差。本文仅讨论了影响当前艺术水平的最有影响力的CNN架构。但是，文献中还有许多其他架构发展和算法可供讨论。</p></div></div>    
</body>
</html>