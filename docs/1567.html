<html>
<head>
<title>Practical Reinforcement Learning pt. 3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实用强化学习。3</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/practical-reinforcement-learning-pt-3-7dc614e850c9?source=collection_archive---------2-----------------------#2018-09-28">https://medium.com/coinmonks/practical-reinforcement-learning-pt-3-7dc614e850c9?source=collection_archive---------2-----------------------#2018-09-28</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><div class=""><h2 id="e322" class="pw-subtitle-paragraph iq hs ht bd b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ek translated">q学习算法</h2></div><figure class="jj jk jl jm fq jn fe ff paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="fe ff ji"><img src="../Images/82172332aa765fdef3714383a8347811.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BzInqIy0r_H1HxgWOsgGWw.png"/></div></div><figcaption class="ju jv fg fe ff jw jx bd b be z ek">Get the Cheese!</figcaption></figure><blockquote class="jy jz ka"><p id="e9cb" class="kb kc kd ke b kf kg iu kh ki kj ix kk kl km kn ko kp kq kr ks kt ku kv kw kx hm dt translated">这篇文章是我之前关于这个主题的<a class="ae ky" rel="noopener" href="/coinmonks/practical-reinforcement-learning-pt-2-8196e4ccab0d">文章</a>的延续。如果您还没有阅读其他文章，建议您先阅读这些文章。</p></blockquote><h2 id="fcf3" class="kz la ht bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw dt translated">问——学习</h2><p id="e7cc" class="pw-post-body-paragraph kb kc ht ke b kf lx iu kh ki ly ix kk lk lz kn ko lo ma kr ks ls mb kv kw kx hm dt translated">在这一点上，如果你一直在跟进，你应该对强化学习的主要成分有一个直观的理解。在我们能够构建自己的强化学习代理之前，还剩下一些成分。我们的下一个要素是Q-Learning。</p><p id="7f36" class="pw-post-body-paragraph kb kc ht ke b kf kg iu kh ki kj ix kk lk km kn ko lo kq kr ks ls ku kv kw kx hm dt translated">让我们先来看看上一篇文章中的一个例子:</p><figure class="jj jk jl jm fq jn fe ff paragraph-image"><div class="fe ff mc"><img src="../Images/a6c2e790af759fd1f483f1d8b1f14aff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*sx-AS6DCWarlAbmcMzT4Vg.png"/></div></figure><p id="60a2" class="pw-post-body-paragraph kb kc ht ke b kf kg iu kh ki kj ix kk lk km kn ko lo kq kr ks ls ku kv kw kx hm dt translated">我们将从检查上面例子中突出显示的方块开始。在这个例子中，从高亮显示的方块“向下”的奖励设置是0.8。对于下面的讨论，我们将这个值称为“Q”。这种价值从何而来？回忆以下细节:</p><ul class=""><li id="861b" class="md me ht ke b kf kg ki kj lk mf lo mg ls mh kx mi mj mk ml dt translated">每个非障碍和非人物正方形的成本为-0.1。</li><li id="2e23" class="md me ht ke b kf mm ki mn lk mo lo mp ls mq kx mi mj mk ml dt translated">我们假设我们的每一步行动都能最大化潜在的回报。</li><li id="d4ba" class="md me ht ke b kf mm ki mn lk mo lo mp ls mq kx mi mj mk ml dt translated">如果代理到达一个障碍，或一个目标，插曲结束。</li></ul><p id="c5af" class="pw-post-body-paragraph kb kc ht ke b kf kg iu kh ki kj ix kk lk km kn ko lo kq kr ks ls ku kv kw kx hm dt translated">如果特工倒下了会发生什么？首先它得到了-0.1的回报。因为我们试图在这里建立一个形式化，让我们给这个奖励一个名字——“R”。接下来它将有两个选择，“左”和“下”。向“左”移动的代价是-0.1，击中桌子会得到-1.0的额外“奖励”。下降成本为-0.1，达到目标可获得1.0的额外奖励。</p><p id="3972" class="pw-post-body-paragraph kb kc ht ke b kf kg iu kh ki kj ix kk lk km kn ko lo kq kr ks ls ku kv kw kx hm dt translated">应该清楚的是，从突出显示的正方形正下方的位置获得的最大值是0.9 (-0.1 + 1.0)。让我们称这个值为Qmax。综上所述，我们有:</p><pre class="jj jk jl jm fq mr ms mt mu aw mv dt"><span id="1752" class="kz la ht ms b fv mw mx l my mz">Q = R + Qmax  (Eq. 1)</span></pre><p id="02e7" class="pw-post-body-paragraph kb kc ht ke b kf kg iu kh ki kj ix kk lk km kn ko lo kq kr ks ls ku kv kw kx hm dt translated">我们越来越接近了，但这仍然不够全面。为了完全推广这个等式，我们需要引入一些新的变量:</p><ul class=""><li id="2908" class="md me ht ke b kf kg ki kj lk mf lo mg ls mh kx mi mj mk ml dt translated">s:这将代表代理当前所在的方格(在以后的例子中，我们称之为代理的“状态”)。</li><li id="5dec" class="md me ht ke b kf mm ki mn lk mo lo mp ls mq kx mi mj mk ml dt translated">答:这指的是代理采取的动作——在上面的例子中，这个动作是“DOWN”。</li><li id="c6cc" class="md me ht ke b kf mm ki mn lk mo lo mp ls mq kx mi mj mk ml dt translated">s1:这将指代理在采取动作a后将处于的状态。在本例中，这指的是高亮显示的方块下面的方块。</li><li id="a67e" class="md me ht ke b kf mm ki mn lk mo lo mp ls mq kx mi mj mk ml dt translated">a1:这是指当座席处于状态s1时采取的操作。</li></ul><p id="e9e2" class="pw-post-body-paragraph kb kc ht ke b kf kg iu kh ki kj ix kk lk km kn ko lo kq kr ks ls ku kv kw kx hm dt translated">我们现在可以更新我们的等式:</p><pre class="jj jk jl jm fq mr ms mt mu aw mv dt"><span id="f138" class="kz la ht ms b fv mw mx l my mz">Q(s,a) = R + gamma * Qmax(s1, a1)   (Eq. 2)</span></pre><p id="2ab8" class="pw-post-body-paragraph kb kc ht ke b kf kg iu kh ki kj ix kk lk km kn ko lo kq kr ks ls ku kv kw kx hm dt translated">这里Qmax(s1，a1)指的是从状态s1的所有可用动作中可获得的最大分数。因此，在本例中，这将指向左(-1.1)或向下(0.9)获得的分数的最大值。代入表中的值，我们得到:</p><pre class="jj jk jl jm fq mr ms mt mu aw mv dt"><span id="a17f" class="kz la ht ms b fv mw mx l my mz">Q(s, "DOWN") = -0.1 + 0.9 = 0.8</span></pre><p id="a0b9" class="pw-post-body-paragraph kb kc ht ke b kf kg iu kh ki kj ix kk lk km kn ko lo kq kr ks ls ku kv kw kx hm dt translated">这与假设折扣系数为1的表格一致。</p><p id="88ba" class="pw-post-body-paragraph kb kc ht ke b kf kg iu kh ki kj ix kk lk km kn ko lo kq kr ks ls ku kv kw kx hm dt translated">我们几乎达到了可以为RL制定所谓的Q学习算法的程度。还缺少一部分—让我们考虑另一个例子:</p><figure class="jj jk jl jm fq jn fe ff paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="fe ff na"><img src="../Images/76d96e5004c2fb0d9bd2e4e1e5113bf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yexmBA7NDedz1hIXI_ZcYQ.png"/></div></div><figcaption class="ju jv fg fe ff jw jx bd b be z ek">Graph World</figcaption></figure><p id="bc3a" class="pw-post-body-paragraph kb kc ht ke b kf kg iu kh ki kj ix kk lk km kn ko lo kq kr ks ls ku kv kw kx hm dt translated">在这个“图形世界”中，代理从世界左侧的蓝色圆圈开始。然后，它可以移动到通过直线与之相连的任何相邻圆。这个世界有两个最终目标——由环境右侧的两个绿色圆圈表示。上面的绿色圆圈提供的奖励比下面的绿色圆圈少。和我们之前的世界一样，我们会用负奖励来惩罚代理人的每一个动作。</p><p id="61ef" class="pw-post-body-paragraph kb kc ht ke b kf kg iu kh ki kj ix kk lk km kn ko lo kq kr ks ls ku kv kw kx hm dt translated">现在，让我们考虑如果代理在探索环境的过程中陷入循环会发生什么:</p><figure class="jj jk jl jm fq jn fe ff paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="fe ff na"><img src="../Images/ae0f7ce523bda52903b59d5b555955ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KY6S26rG2yrAo97VCXAwkw.png"/></div></div><figcaption class="ju jv fg fe ff jw jx bd b be z ek">Going in Circles</figcaption></figure><p id="2614" class="pw-post-body-paragraph kb kc ht ke b kf kg iu kh ki kj ix kk lk km kn ko lo kq kr ks ls ku kv kw kx hm dt translated">这就产生了一个问题——每次代理通过这个循环时，它都会收集越来越多的负面奖励，这将对最终的策略产生不利的影响。考虑以下情况:</p><p id="b5f0" class="pw-post-body-paragraph kb kc ht ke b kf kg iu kh ki kj ix kk lk km kn ko lo kq kr ks ls ku kv kw kx hm dt translated">假设代理在访问更高价值更低的绿色圆圈之前，在环境中遍历循环一次。由于这是一个目标位置，代理结束其探索并重新开始。下一次它碰巧在真正完成一个完整的循环之前随机探索上面的绿色圆圈(具有较低的值)。在后一种情况下，代理人将因移动而获得较少的负回报，这将导致达到较高目标的回报大于达到较高价值的较低目标的回报！</p><blockquote class="jy jz ka"><p id="f208" class="kb kc kd ke b kf kg iu kh ki kj ix kk kl km kn ko kp kq kr ks kt ku kv kw kx hm dt translated">*:如果没有，我们可以让代理一遍又一遍地循环，直到它出现。由于它在前进的道路上收集负回报，最终必然会有一个点，在这个点上，较高目标的回报大于较低目标所采取的循环路径的回报。</p></blockquote><p id="efc6" class="pw-post-body-paragraph kb kc ht ke b kf kg iu kh ki kj ix kk lk km kn ko lo kq kr ks ls ku kv kw kx hm dt translated">这种情况不是很直观——我们不希望我们的代理人的策略对探索过程中采取的确切路径如此敏感。然而，我们可以通过改变方法来解决这个问题。</p><p id="f9e0" class="pw-post-body-paragraph kb kc ht ke b kf kg iu kh ki kj ix kk lk km kn ko lo kq kr ks ls ku kv kw kx hm dt translated">到目前为止，我们只是在代理人全面完成模拟后才计算我们的奖励。这使得计算回报变得相对简单——但是这也使得我们的方法容易受到上述问题的影响。此外，如果我们必须等到一集结束才更新奖励，我们将如何处理没有预定义“结束状态”的环境？</p><p id="5197" class="pw-post-body-paragraph kb kc ht ke b kf kg iu kh ki kj ix kk lk km kn ko lo kq kr ks ls ku kv kw kx hm dt translated">与其等这一集结束，不如我们一次更新一点奖励？我们怎么能这样做呢？</p><p id="43b7" class="pw-post-body-paragraph kb kc ht ke b kf kg iu kh ki kj ix kk lk km kn ko lo kq kr ks ls ku kv kw kx hm dt translated">想象一下，如果你愿意的话，某个值代表了任何给定状态的行为的“真实回报”。每当我们采取行动时，我们都会获得新的奖励，在这种情况下，比如说+1，而不是等到整个情节完成，我们会立即更新分数。诀窍是，我们不是将值完全更新为+1，而是简单地将分数向+1移动一点点。</p><figure class="jj jk jl jm fq jn fe ff paragraph-image"><div class="fe ff nb"><img src="../Images/40cf511890c255617097a97695b41295.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*QAn4_VJQrl4RhronRo3MwA.png"/></div></figure><p id="41a5" class="pw-post-body-paragraph kb kc ht ke b kf kg iu kh ki kj ix kk lk km kn ko lo kq kr ks ls ku kv kw kx hm dt translated">这可以看作是爬山。每采取一次行动，我们就向山顶迈进一步。当我们到达山顶时，我们停止攀登并插上我们的旗帜。</p><figure class="jj jk jl jm fq jn fe ff paragraph-image"><div class="fe ff nc"><img src="../Images/73f3d77a51db0ab5115ca24f7b1656b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*vThV6f7tK8p8GKS9XHz2xg.png"/></div><figcaption class="ju jv fg fe ff jw jx bd b be z ek">Climbing the Mountain</figcaption></figure><p id="3bce" class="pw-post-body-paragraph kb kc ht ke b kf kg iu kh ki kj ix kk lk km kn ko lo kq kr ks ls ku kv kw kx hm dt translated">这可以总结成一个等式:</p><pre class="jj jk jl jm fq mr ms mt mu aw mv dt"><span id="1356" class="kz la ht ms b fv mw mx l my mz">Q(s,a) = Q(s,a) + step_size * ( (R + gamma * Qmax(s1, a1) - Q(s,a) ) (Eq. 3)</span></pre><p id="413b" class="pw-post-body-paragraph kb kc ht ke b kf kg iu kh ki kj ix kk lk km kn ko lo kq kr ks ls ku kv kw kx hm dt translated">基本上这个等式说，每次我们采取行动，我们通过增加一点点(步长)我们得到的奖励和我们期望的奖励之间的差异来更新奖励。我们通常将这个步长称为“学习率”，因为它决定了系统适应奖励变化的速度。</p><p id="a487" class="pw-post-body-paragraph kb kc ht ke b kf kg iu kh ki kj ix kk lk km kn ko lo kq kr ks ls ku kv kw kx hm dt translated">将前几篇文章中的所有内容放在一起，我们得到了所谓的Q学习算法</p><figure class="jj jk jl jm fq jn fe ff paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="fe ff nd"><img src="../Images/5aa494d4cc2ae0236a4e0c884cae8722.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zDHjnA3xNU7WdqT7R7L8Sg.png"/></div></div></figure><p id="aadc" class="pw-post-body-paragraph kb kc ht ke b kf kg iu kh ki kj ix kk lk km kn ko lo kq kr ks ls ku kv kw kx hm dt translated">或者更正式地说:</p><pre class="jj jk jl jm fq mr ms mt mu aw mv dt"><span id="a037" class="kz la ht ms b fv mw mx l my mz">Q-Learning Algorithm</span><span id="6817" class="kz la ht ms b fv ne mx l my mz">Given exploration constant e<br/>Given a learning rate l<br/>Given an Environment S composed of state, action pairs<br/>Given a set of Terminal states T<br/>Given an initial state s0<br/>Given a reward function R</span><span id="03d3" class="kz la ht ms b fv ne mx l my mz">Let Q:S -&gt; Real be the zero function.</span><span id="81f3" class="kz la ht ms b fv ne mx l my mz">while s0 is not a terminal state:<br/>   Let p &lt;- a random value between 0 and 1<br/>   if p &lt; e:<br/>     choose a random action from the state s0<br/>   else:<br/>      choose the action "a" that maximizes Q(s0,a)</span><span id="f149" class="kz la ht ms b fv ne mx l my mz">    <br/>   Let s1 &lt;- the state obtained by performing action a at s0<br/>   Let a0 be the action that maximizes Q(s1, a)<br/>   Let r be the reward obtained by taking action a at s0.<br/>   <br/>   Q(s0, a) = Q(s0, a) + l * [r + gamma * Q(s1,a1) - Q(s0, a)]</span><span id="b38b" class="kz la ht ms b fv ne mx l my mz">   Let s0 &lt;- s1</span></pre><h2 id="6080" class="kz la ht bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw dt translated">下次</h2><p id="c0cc" class="pw-post-body-paragraph kb kc ht ke b kf lx iu kh ki ly ix kk lk lz kn ko lo ma kr ks ls mb kv kw kx hm dt translated">本文涵盖了开始编写Q-Learning算法所需的知识。下一篇文章将把这个算法转换成python，这样我们就可以感受一下它在实践中是如何工作的。</p><p id="3ca9" class="pw-post-body-paragraph kb kc ht ke b kf kg iu kh ki kj ix kk lk km kn ko lo kq kr ks ls ku kv kw kx hm dt translated">在那之前，</p><p id="78ae" class="pw-post-body-paragraph kb kc ht ke b kf kg iu kh ki kj ix kk lk km kn ko lo kq kr ks ls ku kv kw kx hm dt translated">分享享受！</p><blockquote class="nf"><p id="371b" class="ng nh ht bd ni nj nk nl nm nn no kx ek translated"><a class="ae ky" href="https://coincodecap.com/?utm_source=coinmonks" rel="noopener ugc nofollow" target="_blank">在您的收件箱中直接获得最佳软件交易</a></p></blockquote><figure class="nq nr ns nt nu jn fe ff paragraph-image"><a href="https://coincodecap.com/?utm_source=coinmonks"><div class="fe ff np"><img src="../Images/7c0b3dfdcbfea594cc0ae7d4f9bf6fcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/0*OJ-qb5G6i863msBB.png"/></div></a></figure></div></div>    
</body>
</html>