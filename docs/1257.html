<html>
<head>
<title>Character-To-Character RNN With Pytorch’s LSTMCell</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人物对人物的RNN与Pytorch的LSTMCell</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/character-to-character-rnn-with-pytorchs-lstmcell-cd923a6d0e72?source=collection_archive---------2-----------------------#2018-08-07">https://medium.com/coinmonks/character-to-character-rnn-with-pytorchs-lstmcell-cd923a6d0e72?source=collection_archive---------2-----------------------#2018-08-07</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><p id="73bb" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">我看了一些关于递归神经网络的教程，用的是using但是我找不到使用<code class="eh jo jp jq jr b">LSTMCell</code>类的，很多都使用更高级别的<code class="eh jo jp jq jr b">LSTM</code>类。所以我决定使用lstm单元建立一个字符到字符的模型。</p><p id="f71b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">问题陈述:</strong>我们想要构建一个RNN，其中我们将有一个字符序列作为输入，一个字符序列作为输出。您可能已经看到了许多使用许多库构建的模型，如Tensorflow/Keras、CNTK、MXNET或Pytorch。然后使用这种模型根据序列中的前一个字符对最可能出现的字符进行采样。因此，这是一个非常有趣的工具，可以根据现有的文本(一本书或一系列文章)进行文本采样。</p><p id="2035" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><a class="ae js" href="https://www.gutenberg.org/" rel="noopener ugc nofollow" target="_blank">古腾堡计划</a>是许多作者的免费书籍的极好来源(也许你能在那里找到你最喜欢的)。许多书籍都有<code class="eh jo jp jq jr b">.txt</code>格式，这对于培训C-to-C递归模型非常有用。我用的是高尔基的<a class="ae js" href="https://www.gutenberg.org/ebooks/55861" rel="noopener ugc nofollow" target="_blank">《弃儿》</a>。</p><p id="b93c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">正如您可能已经猜到的，从这样的RNN中抽取的结果文本将具有(或者至少应该具有)您可能在训练文本中遇到的句法结构和单词选择。这一概念在黑镜的<a class="ae js" href="https://en.wikipedia.org/wiki/Be_Right_Back" rel="noopener ugc nofollow" target="_blank">《马上回来》</a>一集中有部分描述，其中一家公司向遭受损失的人提供了一个死者的数字模拟。如果你想了解更多关于Luka聊天机器人的信息，请阅读这篇文章。</p><p id="3fd6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">然而，重要的是要注意，这种模型可以使用词到词模型来构建，使模型基于已经看到的词来预测最可能的词(因为句子对于词和字符都是连续的)。</p><p id="e625" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">模型定义:</strong>我们将使用一个两层LSTM模型，每层有512个隐藏节点。这个想法是将一个字符序列作为输入批量传递给模型，并使用相同的序列作为目标，但是移动了一个字符。每个字符都将被一次性编码，我们将在LSTM单元的输出之上添加一个完全连接的层，以适应输出的维度，这将是我们词汇的大小。</p><p id="74a2" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">主要术语:</strong></p><ul class=""><li id="d03a" class="jt ju ht is b it iu ix iy jb jv jf jw jj jx jn jy jz ka kb dt translated"><strong class="is hu">词汇</strong><strong class="is hu">——</strong>组成我们文本的每个单个字符的集合。它很可能包含许多字母和大写字母、所有的语法字符，有时还包含附加字符，如“:、-”等。</li><li id="1de3" class="jt ju ht is b it kc ix kd jb ke jf kf jj kg jn jy jz ka kb dt translated"><strong class="is hu">词汇的大小</strong> <strong class="is hu"> — </strong>词汇集的长度。我们需要这个大小来定义LSTM单元的输入大小。此外，我们需要这个大小作为模型的输出(位于LSTM单元第二层之上的全连接层的输出)；主要地，我们的目标是零维词汇大小的一键编码向量。</li><li id="7510" class="jt ju ht is b it kc ix kd jb ke jf kf jj kg jn jy jz ka kb dt translated"><strong class="is hu"> LSTM细胞— </strong>这个细胞位于模型的核心。你可能已经看过很多次了。这是克里斯托弗·奥拉的一个很好的图表:</li></ul><figure class="ki kj kk kl fq km fe ff paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="fe ff kh"><img src="../Images/cace770db820b756504598221c2c97d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J5W8FrASMi93Z81NlAui4w.png"/></div></div></figure><p id="f2d0" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">他有一篇关于LSTM细胞和RNN细胞背后的数学和直觉的精彩文章；如果你还没有读过，你应该读一读。</p><p id="2cbb" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">LSTM单元背后的基本思想是，信号以特定的方式流经单元，由输入、遗忘、更新和输出门定义。在任何给定的步骤t，单元有三个输入:单元输入、隐藏向量输入和单元状态输入。在任何给定时间都有来自单元的三个输出:新隐藏状态、新单元状态和输出，该输出又与新隐藏状态相同。所有这些都在克里斯托弗的帖子中有所描述。</p><ul class=""><li id="e62d" class="jt ju ht is b it iu ix iy jb jv jf jw jj jx jn jy jz ka kb dt translated"><strong class="is hu">隐藏状态— </strong>大小为<em class="kt"> (batch_size，hidden_size) </em>的向量。隐藏向量的维度越大——你的模型就越健壮，以计算成本为代价。隐藏状态向量作为你的短期记忆，在时间步长t被输入更新。</li><li id="1085" class="jt ju ht is b it kc ix kd jb ke jf kf jj kg jn jy jz ka kb dt translated"><strong class="is hu">细胞状态— </strong>大小为<em class="kt"> (batch_size，hidden_size)，</em>的向量充当你的长期记忆。</li><li id="4218" class="jt ju ht is b it kc ix kd jb ke jf kf jj kg jn jy jz ka kb dt translated"><strong class="is hu">LSTM的层— </strong>如果我们把LSTM细胞一个接一个的堆叠起来，我们就得到一个分层的LSTM模型。从技术上来说，<strong class="is hu">我们希望在任何给定的时间t将第一层的LSTM单元的输出作为输入传递给第二层的LSTM单元。</strong>如果我们这样做，我们最终会构建一个更深的网络。</li><li id="21ad" class="jt ju ht is b it kc ix kd jb ke jf kf jj kg jn jy jz ka kb dt translated"><strong class="is hu"> LSTM网络— </strong>现在，如果我们将时间t的隐藏状态输出向量传递给时间t+1的隐藏状态向量输入，我们将获得一系列LSTM单元，这形成了我们的LSTM模型。</li></ul></div><div class="ab cl ku kv hb kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="hm hn ho hp hq"><p id="b251" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">现在让我们来看看代码和构建模型。正如我提到的，我想使用pytorch库中的LSTM细胞类来构建这个模型。还有，值得一提的是，Keras在<code class="eh jo jp jq jr b">utils</code>模块中有一个很棒的工具:<code class="eh jo jp jq jr b">to_categorical</code>。这是一个伟大的方式一热编码矢量，非常直观。</p><p id="a975" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">首先</strong>:我们需要将文本读入io，并用整数对文本进行编码。</p><figure class="ki kj kk kl fq km"><div class="bz el l di"><div class="lb lc l"/></div></figure><p id="bd1f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">第二个</strong>:我们需要写批处理算法。正如我之前提到的，我们希望将目标设置为训练字符，但在时间上移动1以定义顺序。我使用了之前在Udacity上的一个课程中的批处理函数。在我看来，编写批处理算法是这类任务中最难的部分。</p><figure class="ki kj kk kl fq km"><div class="bz el l di"><div class="lb lc l"/></div></figure><p id="1373" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">第三个:我们想开始在pytorch中构建我们的模型。在这里，我们将使用LSTM细胞类来定义细胞的两层在我们的LSTM模型。通常将隐藏和单元状态初始化为零的张量，以传递给序列中的第一个LSTM单元。</p><figure class="ki kj kk kl fq km"><div class="bz el l di"><div class="lb lc l"/></div></figure><p id="a70e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">请注意，我们重用了同一个LSTM单元对象，但顺序不同，将隐藏和单元状态从上一个时间步长传递到当前时间步长单元，同时在时间步长上迭代输入(一键编码的字符序列)。</p><p id="9e0f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">第四个:现在我们要定义我们的模型对象以及优化器和损失函数。我们将使用Adam optimizer，因为它是此类任务的最常见选择。此外，我们将使用交叉熵损失，因为我们将测量输出和目标(即分布)之间的熵。</p><figure class="ki kj kk kl fq km"><div class="bz el l di"><div class="lb lc l"/></div></figure><p id="a741" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">第五</strong>:我们要训练我们的模特。我将把编码的数据集分成训练集和验证集，以监控损失，从而对拟合不足进行诊断。</p><figure class="ki kj kk kl fq km"><div class="bz el l di"><div class="lb lc l"/></div></figure><p id="924e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">您可能已经注意到，我还预测了一些字符，将初始字符设置为“A”。预测非常简单:模型在t=0时接收一个独热编码的“A”作为LSTM单元的输入。之后，我们从单元格中获得一个具有我们的词汇量的输出。如果我们将softmax函数应用于输出，我们获得了词汇空间上的分布；接下来，我们想根据模型选择5个最可能的字符。一旦我们获得了字符，我们就根据它们在这5个字符的空间中的概率抽取一个字符。这个被采样的字符现在将在时间t=1被输入到LSTM单元，等等。</p><figure class="ki kj kk kl fq km"><div class="bz el l di"><div class="lb lc l"/></div></figure><p id="fc4b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">注意:</strong>由于pytorch到处都需要一个批处理维度，所以很多时候你都要修改输入和输出的维度；因此，你要做更多的<code class="eh jo jp jq jr b">squeezing</code>和<code class="eh jo jp jq jr b">unsqueezing</code>。<strong class="is hu">还有</strong>，不要忘了将你的numpy数组转换成torch张量，然后再转换回来，因为我们最终处理的是整数(你需要它们来查找实际的字符)。<strong class="is hu">还有一点</strong>:内存布局有时会出现错位；因此你需要在某些地方使用<code class="eh jo jp jq jr b">contiguous</code>来重新排列内存。</p><p id="15fe" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">以下是RNN生成的一些样本:</p><figure class="ki kj kk kl fq km fe ff paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="fe ff ld"><img src="../Images/fde5bbfdef4aa172f2e2d0fa383b6bde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iumOYryal-ok-qYZJejM0Q.png"/></div></div><figcaption class="le lf fg fe ff lg lh bd b be z ek">First epoch sample</figcaption></figure><figure class="ki kj kk kl fq km fe ff paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="fe ff li"><img src="../Images/7181ede0011cc93aa77f83a0c6cab27b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uzvKGLm7mNckI-ID421Lxw.png"/></div></div><figcaption class="le lf fg fe ff lg lh bd b be z ek">50th epoch sample</figcaption></figure><p id="b416" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">我们可以看到，我在第50个纪元获得的结果样本没有多大意义；但是，我们可以看到，模型学到了很多东西:一些单词，一些句子结构，句法。现在我们需要做的就是调整模型的超参数，使它变得更好。</p><p id="43fa" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">完整的模型可以在<a class="ae js" href="https://github.com/halahup/Machine-Learning/tree/master/Recurrent%20Neural%20Networks/Character%20Level%20RNNs" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></div><div class="ab cl ku kv hb kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="hm hn ho hp hq"><p id="5803" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">我遇到的问题</strong>:</p><ul class=""><li id="3f3c" class="jt ju ht is b it iu ix iy jb jv jf jw jj jx jn jy jz ka kb dt translated">有些书(课文)在课文中间的某个地方埋了一个像“ì”“啊”“啊”之类的古怪字符。当这些字符只存在于训练集中时(在将数据分为训练数据和验证数据之后)，问题就变得很明显了。验证集的词汇长度变得不足1或2个字符，在验证时，您被迫传递它的词汇长度。不重要，但是很烦人。</li><li id="3af0" class="jt ju ht is b it kc ix kd jb ke jf kf jj kg jn jy jz ka kb dt translated">你需要足够大的文本来或多或少地适应模型。否则，你将会过度适应或者最终疯狂地规则化，伤害模型的能力。托尔斯泰的《安娜·卡列尼娜》是开始练习的一份非常好的资料。</li></ul><blockquote class="lj"><p id="c67d" class="lk ll ht bd lm ln lo lp lq lr ls jn ek translated">加入Coinmonks <a class="ae js" href="https://t.me/coincodecap" rel="noopener ugc nofollow" target="_blank">电报频道</a>和<a class="ae js" href="https://www.youtube.com/c/coinmonks/videos" rel="noopener ugc nofollow" target="_blank"> Youtube频道</a>获取每日<a class="ae js" href="http://coincodecap.com/" rel="noopener ugc nofollow" target="_blank">加密新闻</a></p></blockquote><h2 id="15d6" class="lt lu ht bd lv lw lx ly lz ma mb mc md jb me mf mg jf mh mi mj jj mk ml mm mn dt translated">另外，阅读</h2><ul class=""><li id="20fb" class="jt ju ht is b it mo ix mp jb mq jf mr jj ms jn jy jz ka kb dt translated"><a class="ae js" rel="noopener" href="/coinmonks/top-10-crypto-copy-trading-platforms-for-beginners-d0c37c7d698c">复制交易</a> | <a class="ae js" rel="noopener" href="/coinmonks/crypto-tax-software-ed4b4810e338">加密税务软件</a></li><li id="723e" class="jt ju ht is b it kc ix kd jb ke jf kf jj kg jn jy jz ka kb dt translated"><a class="ae js" href="https://coincodecap.com/grid-trading" rel="noopener ugc nofollow" target="_blank">电网交易</a> | <a class="ae js" rel="noopener" href="/coinmonks/the-best-cryptocurrency-hardware-wallets-of-2020-e28b1c124069">加密硬件钱包</a></li><li id="874f" class="jt ju ht is b it kc ix kd jb ke jf kf jj kg jn jy jz ka kb dt translated"><a class="ae js" href="http://Top 4 Telegram Channels for Crypto Traders" rel="noopener ugc nofollow" target="_blank">密码电报信号</a> | <a class="ae js" rel="noopener" href="/coinmonks/crypto-trading-bot-c2ffce8acb2a">密码交易机器人</a></li><li id="f33b" class="jt ju ht is b it kc ix kd jb ke jf kf jj kg jn jy jz ka kb dt translated"><a class="ae js" rel="noopener" href="/coinmonks/crypto-exchange-dd2f9d6f3769">最佳加密交易所</a> | <a class="ae js" rel="noopener" href="/coinmonks/bitcoin-exchange-in-india-7f1fe79715c9">印度最佳加密交易所</a></li><li id="6065" class="jt ju ht is b it kc ix kd jb ke jf kf jj kg jn jy jz ka kb dt translated"><a class="ae js" href="https://coincodecap.com/binance-vs-bitstamp" rel="noopener ugc nofollow" target="_blank">币安vs比特邮票</a> | <a class="ae js" href="https://coincodecap.com/bitpanda-coinbase-coinsbit" rel="noopener ugc nofollow" target="_blank">比特熊猫vs比特币基地vs Coinsbit </a></li><li id="5711" class="jt ju ht is b it kc ix kd jb ke jf kf jj kg jn jy jz ka kb dt translated"><a class="ae js" href="https://coincodecap.com/buy-ripple-india" rel="noopener ugc nofollow" target="_blank">如何购买Ripple (XRP) </a> | <a class="ae js" href="https://coincodecap.com/crypto-exchange-africa" rel="noopener ugc nofollow" target="_blank">非洲最好的加密交易所</a></li><li id="861b" class="jt ju ht is b it kc ix kd jb ke jf kf jj kg jn jy jz ka kb dt translated"><a class="ae js" href="https://coincodecap.com/crypto-exchange-africa" rel="noopener ugc nofollow" target="_blank">非洲最佳加密交易所</a> | <a class="ae js" href="https://coincodecap.com/hoo-exchange-review" rel="noopener ugc nofollow" target="_blank">胡交易所评论</a></li><li id="452c" class="jt ju ht is b it kc ix kd jb ke jf kf jj kg jn jy jz ka kb dt translated"><a class="ae js" href="https://coincodecap.com/etoro-robinhood" rel="noopener ugc nofollow" target="_blank">eToro vs robin hood</a>|<a class="ae js" href="https://coincodecap.com/bybit-bityard-moonxbt" rel="noopener ugc nofollow" target="_blank">MoonXBT vs by bit vs Bityard</a></li><li id="47a8" class="jt ju ht is b it kc ix kd jb ke jf kf jj kg jn jy jz ka kb dt translated">开发人员的最佳加密API</li><li id="b359" class="jt ju ht is b it kc ix kd jb ke jf kf jj kg jn jy jz ka kb dt translated">最佳<a class="ae js" rel="noopener" href="/coinmonks/top-5-crypto-lending-platforms-in-2020-that-you-need-to-know-a1b675cec3fa">密码借贷平台</a></li><li id="3c98" class="jt ju ht is b it kc ix kd jb ke jf kf jj kg jn jy jz ka kb dt translated"><a class="ae js" rel="noopener" href="/coinmonks/free-crypto-signals-48b25e61a8da">免费加密信号</a> | <a class="ae js" rel="noopener" href="/coinmonks/crypto-trading-bot-c2ffce8acb2a">加密交易机器人</a></li><li id="9487" class="jt ju ht is b it kc ix kd jb ke jf kf jj kg jn jy jz ka kb dt translated"><a class="ae js" rel="noopener" href="/coinmonks/leveraged-token-3f5257808b22">杠杆代币的终极指南</a></li></ul></div></div>    
</body>
</html>