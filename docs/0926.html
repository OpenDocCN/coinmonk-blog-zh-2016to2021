<html>
<head>
<title>Simplifying Rough Sketches using Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度学习简化草图</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/simplifying-rough-sketches-using-deep-learning-c404459622b9?source=collection_archive---------1-----------------------#2018-07-02">https://medium.com/coinmonks/simplifying-rough-sketches-using-deep-learning-c404459622b9?source=collection_archive---------1-----------------------#2018-07-02</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><blockquote class="iq ir is"><p id="4153" class="it iu iv iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hm dt translated">草图是表达艺术想法的基本第一步，也是设计改进迭代过程的开始。它允许艺术家在纸上快速呈现他们的想法。当务之急是快速表达概念和想法，而不是展示精细的细节，这会导致粗略的草图。在最初的草图之后，<br/>反馈被用来反复完善设计，直到最后的作品<br/>被制作出来。这种反复的细化迫使艺术家们不得不不断地将他们的草图整理成简化的图纸，因此意味着额外的工作量。正如<br/>所料，手动<br/>描摹草图以生成清晰图纸的过程相当繁琐耗时。</p></blockquote><p id="f6e3" class="pw-post-body-paragraph it iu ht iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hm dt translated">因此，如果有一种方法可以从我们的草图中立即得到清晰的草图，不管它有什么样的笔画，那不是更好吗？很迷人，不是吗？在这篇文章中，我将讨论一种深度学习技术，它使用完全卷积网络从粗糙的草图中生成干净的草图。</p><figure class="jw jx jy jz fq ka fe ff paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="fe ff jv"><img src="../Images/c4323fda3840b4e560923a6e5a558c5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UlswLQffcDxsI3xVpnonjQ.png"/></div></div><figcaption class="kh ki fg fe ff kj kk bd b be z ek">left : rough sketches right: clean sketch generated source: Original Paper, link in the footnotes.</figcaption></figure><p id="5d68" class="pw-post-body-paragraph it iu ht iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hm dt translated">嗯，并不是说更早的时候没有任何软件可以做到这一点，是有，但问题是他们以前只能处理<strong class="iw hu">矢量图像</strong>而不能处理<strong class="iw hu">光栅图像。</strong>让我们从什么是矢量和光栅图像开始！</p></div><div class="ab cl kl km hb kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hm hn ho hp hq"><h2 id="635c" class="ks kt ht bd ku kv kw kx ky kz la lb lc js ld le lf jt lg lh li ju lj lk ll lm dt translated"><strong class="ak">矢量和光栅图像？</strong></h2><p id="69d3" class="pw-post-body-paragraph it iu ht iw b ix ln iz ja jb lo jd je js lp jh ji jt lq jl jm ju lr jp jq jr hm dt translated"><strong class="iw hu">光栅图像，</strong>也称为<strong class="iw hu">位图，</strong>由单独的颜色像素组成。每个彩色像素对整个图像都有贡献。</p><p id="b817" class="pw-post-body-paragraph it iu ht iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hm dt translated">光栅图像可以比作点画，点画是由一系列单独着色的颜料点组成的。点彩绘画中的每个绘画点可能代表光栅图像中的一个像素。当看做一个单独的点时，它只是一种颜色；但是从整体来看，这些彩色的点组成了一幅生动而细致的画。光栅图像中的像素以相同的方式工作，这提供了丰富的细节和逐像素编辑。</p><figure class="jw jx jy jz fq ka fe ff paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="fe ff ls"><img src="../Images/bc68df642d769acdf742c21307fc0fb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v7jK_GwsgsRp9PCgzLuI0A.png"/></div></div><figcaption class="kh ki fg fe ff kj kk bd b be z ek">left: full picture(without zoom) right: on zooming the individual pixels can be seen</figcaption></figure><p id="fa93" class="pw-post-body-paragraph it iu ht iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hm dt translated">与光栅图形不同，光栅图形由排列显示图像的彩色像素组成，<strong class="iw hu">矢量图形由路径</strong>组成，每个路径都有一个数学公式(矢量)来告诉路径它是如何成形的，以及它的边框或填充颜色是什么。</p><p id="0bff" class="pw-post-body-paragraph it iu ht iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hm dt translated">由于数学公式规定了图像的渲染方式，因此无论大小如何，矢量图像都会保持其外观。它们可以无限缩放。</p><figure class="jw jx jy jz fq ka fe ff paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="fe ff ls"><img src="../Images/afc19e89ec088390f46a5e6634b6c4fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yND_92qcsGr4XOfJjvS7Tw.png"/></div></div><figcaption class="kh ki fg fe ff kj kk bd b be z ek">the difference between vector and raster on scaling up</figcaption></figure></div><div class="ab cl kl km hb kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hm hn ho hp hq"><h2 id="d91f" class="ks kt ht bd ku kv kw kx ky kz la lb lc js ld le lf jt lg lh li ju lj lk ll lm dt translated">模型架构</h2><figure class="jw jx jy jz fq ka fe ff paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="fe ff lt"><img src="../Images/12f2519e7a12906493ad0d2157f90dd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A92tCdTvmczZRAJc-hGMxQ.png"/></div></div><figcaption class="kh ki fg fe ff kj kk bd b be z ek">Model architecture source: original paper</figcaption></figure><p id="cf78" class="pw-post-body-paragraph it iu ht iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hm dt translated">这个模型最好的部分是，它可以处理光栅图像，并将多条粗略的草图线转换成一条清晰的线。</p><figure class="jw jx jy jz fq ka fe ff paragraph-image"><div class="fe ff lu"><img src="../Images/181e31ddce6fc2e6c7c8530df3654005.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*S4pjR6X9iY7JMXm06dsHdw.png"/></div><figcaption class="kh ki fg fe ff kj kk bd b be z ek">multiple lines to clean single line</figcaption></figure><p id="1901" class="pw-post-body-paragraph it iu ht iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hm dt translated">这种结构的另一个优点是，任何维数的图像都可以输入到网络中，并且输出与输入图像维数相同的图像。</p><p id="0a77" class="pw-post-body-paragraph it iu ht iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hm dt translated">该架构相当简单，<strong class="iw hu">第一部分充当编码器并对图像进行空间压缩，第二部分处理并提取图像中的基本线条，第三部分即最后一部分充当解码器，将更简单的小表示转换为与输入分辨率相同的灰度图像</strong>。这都是使用卷积完成的。</p><p id="1083" class="pw-post-body-paragraph it iu ht iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hm dt translated">下卷积和上卷积架构看起来类似于简单的滤波器组<br/>。然而，重要的是要认识到，在分辨率较低的处，通道的<br/> <strong class="iw hu">数量要大得多，例如，尺寸为1/8的<br/> 1024。<strong class="iw hu">这确保引导<br/>清理线路的信息通过低分辨率部分</strong>传送；网络<br/>被训练来选择由编码器- <br/>解码器架构携带哪些信息。</strong></p><p id="ab7d" class="pw-post-body-paragraph it iu ht iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hm dt translated">填充用于补偿内核大小，并确保使用步长1时输出与输入的大小相同。合并层被具有更大跨度的卷积层取代，以降低来自前一层的分辨率。</p><pre class="jw jx jy jz fq lv lw lx ly aw lz dt"><span id="b468" class="ks kt ht lw b fv ma mb l mc md"><strong class="lw hu"># The input dimensions can be replaced with the dimensions of the image.</strong></span><span id="38d2" class="ks kt ht lw b fv me mb l mc md"><strong class="lw hu">class</strong> <strong class="lw hu">Net</strong>(torch.nn.Module):<br/>    <strong class="lw hu">def</strong> __init__(self):<br/>        super(Net, self).__init__()<br/>        self.downconv1 = torch.nn.Sequential(<br/>            torch.nn.Conv2d(1, 48, 5, 2, 2),<br/>            torch.nn.BatchNorm2d(48),<br/>            torch.nn.ReLU(),<br/>            <br/>            torch.nn.Conv2d(48, 128, 3, 1, 1),<br/>            torch.nn.BatchNorm2d(128),<br/>            torch.nn.ReLU(),<br/>            <br/>            torch.nn.Conv2d(128, 128, 3, 1, 1),<br/>            torch.nn.BatchNorm2d(128),<br/>            torch.nn.ReLU(),<br/>        )<br/>        self.downconv2 = torch.nn.Sequential(<br/>            torch.nn.Conv2d(128,256, 3, 2, 1),<br/>            torch.nn.BatchNorm2d(256),<br/>            torch.nn.ReLU(),<br/>            <br/>            torch.nn.Conv2d(256, 256, 3, 1, 1),<br/>            torch.nn.BatchNorm2d(256),<br/>            torch.nn.ReLU(),<br/>            <br/>            torch.nn.Conv2d(256, 256, 3, 1, 1),<br/>            torch.nn.BatchNorm2d(256),<br/>            torch.nn.ReLU(),<br/>        )<br/>        self.downconv3 = torch.nn.Sequential(<br/>            torch.nn.Conv2d(256, 256, 3, 2, 1),<br/>            torch.nn.BatchNorm2d(256),<br/>            torch.nn.ReLU(),<br/>            <br/>            torch.nn.Conv2d(256, 512, 3, 1, 1),<br/>            torch.nn.BatchNorm2d(512),<br/>            torch.nn.ReLU(),<br/>            <br/>            torch.nn.Conv2d(512, 1024, 3, 1, 1),<br/>            torch.nn.BatchNorm2d(1024),<br/>            torch.nn.ReLU(),<br/>        )<br/>        self.flat = torch.nn.Sequential(<br/>            torch.nn.Conv2d(1024, 1024, 3, 1, 1),<br/>            torch.nn.BatchNorm2d(1024),<br/>            torch.nn.ReLU(),<br/>            <br/>            torch.nn.Conv2d(1024, 1024, 3, 1, 1),<br/>            torch.nn.BatchNorm2d(1024),<br/>            torch.nn.ReLU(),<br/>            <br/>            torch.nn.Conv2d(1024, 1024, 3, 1, 1),<br/>            torch.nn.BatchNorm2d(1024),<br/>            torch.nn.ReLU(),<br/>            <br/>            torch.nn.Conv2d(1024, 512, 3, 1, 1),<br/>            torch.nn.BatchNorm2d(512),<br/>            torch.nn.ReLU(),<br/><br/>            torch.nn.Conv2d(512, 256, 3, 1, 1),<br/>            torch.nn.BatchNorm2d(256),<br/>            torch.nn.ReLU(),<br/>        )<br/>        <br/>        self.upconv1 = torch.nn.Sequential(<br/><em class="iv">#             torch.nn.Conv2d(256, 256, 4, 0.5, 1),</em><br/>            torch.nn.ConvTranspose2d(256, 256, 4, 2, 1),<br/>            torch.nn.BatchNorm2d(256),<br/>            torch.nn.ReLU(),<br/>            <br/>            torch.nn.Conv2d(256, 256, 3, 1, 1),<br/>            torch.nn.BatchNorm2d(256),<br/>            torch.nn.ReLU(),<br/>            <br/>            torch.nn.Conv2d(256, 128, 3, 1, 1),<br/>            torch.nn.BatchNorm2d(128),<br/>            torch.nn.ReLU(),<br/>        )<br/>        self.upconv2 = torch.nn.Sequential(<br/><em class="iv">#             torch.nn.Conv2d(128, 128, 4, 0.5, 1),</em><br/>            torch.nn.ConvTranspose2d(128, 128, 4, 2, 1),            <br/>            torch.nn.BatchNorm2d(128),<br/>            torch.nn.ReLU(),<br/>            <br/>            torch.nn.Conv2d(128, 128, 3, 1, 1),<br/>            torch.nn.BatchNorm2d(128),<br/>            torch.nn.ReLU(),<br/>            <br/>            torch.nn.Conv2d(128, 48, 3, 1, 1),<br/>            torch.nn.BatchNorm2d(48),<br/>            torch.nn.ReLU(),<br/>        )<br/>        self.upconv3 = torch.nn.Sequential(<br/><em class="iv">#             torch.nn.Conv2d(48, 48, 4, 0.5, 1),</em><br/>            torch.nn.ConvTranspose2d(48, 48,4, 2, 1),<br/>            torch.nn.BatchNorm2d(48),<br/>            torch.nn.ReLU(),<br/>            <br/>            torch.nn.Conv2d(48, 24, 3, 1, 1),<br/>            torch.nn.BatchNorm2d(24),<br/>            torch.nn.ReLU(),<br/>            <br/>            torch.nn.Conv2d(24, 1, 3, 1, 1),<br/>            torch.nn.Sigmoid (),<br/>        )<br/><br/><br/>    <strong class="lw hu">def</strong> forward(self, x):<br/>        conv1_out = self.downconv1(x)<br/>        conv2_out = self.downconv2(conv1_out)<br/>        conv3_out = self.downconv3(conv2_out)<br/>        flat_out = self.flat(conv3_out)<br/>        upconv1_out = self.upconv1(flat_out)<br/>        upconv2_out = self.upconv2(upconv1_out)<br/>        upconv3_out = self.upconv3(upconv2_out)<br/>        <strong class="lw hu">return</strong> upconv3_out</span></pre><figure class="jw jx jy jz fq ka fe ff paragraph-image"><div class="fe ff mf"><img src="../Images/28677c4d62cd32e605003fab0ad352dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*Tx2tqgYbnN-Ak4fckPYu1Q.png"/></div><figcaption class="kh ki fg fe ff kj kk bd b be z ek">model summary where H and W are height and width of input image</figcaption></figure><h2 id="244c" class="ks kt ht bd ku kv kw kx ky kz la lb lc js ld le lf jt lg lh li ju lj lk ll lm dt translated">损失函数</h2><p id="9b14" class="pw-post-body-paragraph it iu ht iw b ix ln iz ja jb lo jd je js lp jh ji jt lq jl jm ju lr jp jq jr hm dt translated">使用加权均方标准计算模型损耗，</p><figure class="jw jx jy jz fq ka fe ff paragraph-image"><div class="fe ff mg"><img src="../Images/3a6fe724928f9535947d242fa7681fee.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*FpC60KToS5XmIs239NeWDQ.png"/></div><figcaption class="kh ki fg fe ff kj kk bd b be z ek">loss function</figcaption></figure><p id="5d21" class="pw-post-body-paragraph it iu ht iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hm dt translated">其中Y是模型输出，Y*是目标输出，M是损耗图，对它们执行逐元素矩阵乘法，以计算损耗。现在，根据论文作者对各种损失图的测试，他们发现下面给出的损失图表现更好。损失图减少了较粗线的损失，以避免模型聚焦于较粗线而忽略较细线。</p><figure class="jw jx jy jz fq ka fe ff paragraph-image"><div class="fe ff mh"><img src="../Images/ee24fdf41b672f75581b78dfe464a196.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*bAYQn-TKgSfux93WESPaZw.png"/></div><figcaption class="kh ki fg fe ff kj kk bd b be z ek">loss map</figcaption></figure><p id="a5af" class="pw-post-body-paragraph it iu ht iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hm dt translated">我们通过查看地面真实(目标)标签中每个像素周围的直方图来构建损失图。<em class="iv"> H(I，u，v)是像素I(u，v)所在的局部归一化直方图的柱的值。使用b_h仓，使用从中心开始的d_h像素内的所有像素来构建直方图。</em></p><figure class="jw jx jy jz fq ka fe ff paragraph-image"><div class="fe ff mi"><img src="../Images/3065f3e3f6eaabfe328f6acc32780ad4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*4pmlGT99oKVkFwsfCsH1zw.png"/></div><figcaption class="kh ki fg fe ff kj kk bd b be z ek">visualization of the training of the model</figcaption></figure><p id="e563" class="pw-post-body-paragraph it iu ht iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hm dt translated">由于用于训练的图像数量非常少，因此使用各种数据扩充技术来创建数据集。使用了传统的变换，如旋转等，但同时使用了Adobe Photoshop来改变色调，模糊图像，并添加噪声，以创建更多的样本。</p><blockquote class="mj"><p id="525e" class="mk ml ht bd mm mn mo mp mq mr ms jr ek translated">这是一个最先进的模型，其性能甚至比Portrace和Adobe Live Trace还要好。</p></blockquote><p id="2d46" class="pw-post-body-paragraph it iu ht iw b ix mt iz ja jb mu jd je js mv jh ji jt mw jl jm ju mx jp jq jr hm dt translated"><strong class="iw hu">参考文献<em class="iv"> : </em> </strong></p><p id="d369" class="pw-post-body-paragraph it iu ht iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hm dt translated">火炬代号:<a class="ae my" href="https://github.com/bobbens/sketch_simplification" rel="noopener ugc nofollow" target="_blank">https://github.com/bobbens/sketch_simplification</a></p><p id="0b86" class="pw-post-body-paragraph it iu ht iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hm dt translated">论文:<a class="ae my" href="http://hi.cs.waseda.ac.jp/~esimo/publications/SimoSerraSIGGRAPH2016.pdf" rel="noopener ugc nofollow" target="_blank">http://hi . cs . waseda . AC . jp/~ esimo/publications/simoserrasiggraph 2016 . pdf</a></p><p id="9c19" class="pw-post-body-paragraph it iu ht iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hm dt translated">我的实现:<a class="ae my" href="https://github.com/sinAshish/Rough-Sketch-Simplification-Using-FCNN" rel="noopener ugc nofollow" target="_blank">https://github . com/Sina shish/Rough-Sketch-Simplification-Using-FCNN</a></p><p id="a55f" class="pw-post-body-paragraph it iu ht iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hm dt translated">页（page的缩写）s:我将实现pytorch版本的代码，只是很难获得论文的数据集。同时，作者为他们的代码提供了预训练的权重。</p></div></div>    
</body>
</html>