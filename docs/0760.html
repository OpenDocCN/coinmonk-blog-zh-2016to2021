<html>
<head>
<title>Multiple Regression /Regression Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多元回归/回归第二部分</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/multiple-regression-regression-part-2-aeffa1843073?source=collection_archive---------3-----------------------#2018-06-12">https://medium.com/coinmonks/multiple-regression-regression-part-2-aeffa1843073?source=collection_archive---------3-----------------------#2018-06-12</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><p id="e433" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">正如我们在前面的后线性回归第1部分中所讨论的</p><div class="jo jp fm fo jq jr"><a href="https://dataneel.wordpress.com/2018/06/09/linear-regression-part-1/" rel="noopener  ugc nofollow" target="_blank"><div class="js ab ej"><div class="jt ab ju cl cj jv"><h2 class="bd hu fv z el jw eo ep jx er et hs dt translated">线性回归第一部分</h2><div class="jy l"><h3 class="bd b fv z el jw eo ep jx er et ek translated">线性回归是最简单的监督学习类型。回归分析的目的是探索…</h3></div><div class="jz l"><p class="bd b gc z el jw eo ep jx er et ek translated">dataneel.wordpress.com</p></div></div><div class="ka l"><div class="kb l kc kd ke ka kf kg jr"/></div></div></a></div><p id="f63d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">当存在多组输入要素时，我们使用多元回归，如下式所示:</p><pre class="kh ki kj kk fq kl km kn ko aw kp dt"><span id="c0b6" class="kq kr ht km b fv ks kt l ku kv"><strong class="km hu">Y=x0+(x1*w1+x2*w2+x3*w3+....+xn*wn)</strong></span></pre><p id="ec89" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">其中x1，x2，x3，…xn是输入特征。</p><p id="6c4e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">然而，在现实世界中，像简单的线性回归那样处理二维数据并不简单。</p><h2 id="5131" class="kq kr ht bd kw kx ky kz la lb lc ld le jb lf lg lh jf li lj lk jj ll lm ln lo dt translated">我的意思是。</h2><blockquote class="lp lq lr"><p id="5f4f" class="iq ir ls is b it iu iv iw ix iy iz ja lt jc jd je lu jg jh ji lv jk jl jm jn hm dt translated">2D数据具有x轴和y轴，这两个轴都具有一组值。如果您查看前面的示例,<a class="ae lw" href="https://github.com/neelindresh/NeelBlog/blob/master/HousePrice.csv" rel="noopener ugc nofollow" target="_blank"> HousePrice.csv </a>数据集，它包含8列。但是我们所做的是取两列，它们是<em class="ht">价格(旧)-&gt;x轴</em>和<em class="ht">价格(新)- &gt; Y轴。</em>由此我们创建了我们的线性模型。然而，这不是真实世界的情况。在现实世界中，一个数据集可以有多个特征。正确预测一个值需要这些特性。</p></blockquote><p id="72ee" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">这就是我们正在做的:我们现在考虑所有的8列，而不是8列中的2列(记住日期列不是一个特性，它只是信息。所以我们可以忽略它)。这样我们就有7个特征作为X轴，1个特征作为Y轴或<strong class="is hu"> <em class="ls">目标值。</em> </strong>下面给出代码。关于多重回归的任何信息，检查<a class="ae lw" href="http://dataneel.wordpress.com" rel="noopener ugc nofollow" target="_blank">线性回归部分:1 </a></p><pre class="kh ki kj kk fq kl km kn ko aw kp dt"><span id="70fb" class="kq kr ht km b fv ks kt l ku kv"><strong class="km hu">import </strong>pandas<br/><strong class="km hu">import </strong>numpy <strong class="km hu">as </strong>np<br/>#load csv file<br/>df=pandas.read_csv('./DataSet/HousePrice.csv')<br/>print(df.describe())<br/>df=df.drop(['Date'],axis=1)<br/>X=df[list(df.columns)[:-1]]<br/>Y=df[list(df.columns)[-1]]<br/>#print(X)<br/>#print(Y)<br/><strong class="km hu">from </strong>sklearn.model_selection <strong class="km hu">import </strong>train_test_split<br/>xtrain,xtest,ytrain,ytest=train_test_split(X,Y,random_state=0)<br/><strong class="km hu">from </strong>sklearn.linear_model <strong class="km hu">import </strong>LinearRegression<br/>reg=LinearRegression()<br/>reg.fit(xtrain,ytrain)<br/>print(list(reg.predict(xtrain))[:5])<br/>print(reg.score(xtest,ytest))</span></pre><p id="451d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"> <em class="ls">使用熊猫库导入CSV文件:</em> </strong></p><pre class="kh ki kj kk fq kl km kn ko aw kp dt"><span id="13e9" class="kq kr ht km b fv ks kt l ku kv">df=pandas.read_csv('./DataSet/HousePrice.csv')<br/>print(df.describe())</span></pre><blockquote class="lp lq lr"><p id="2e88" class="iq ir ls is b it iu iv iw ix iy iz ja lt jc jd je lu jg jh ji lv jk jl jm jn hm dt translated">熊猫。read_csv()从特定位置读取csv文件。其他选项也有参考:<a class="ae lw" href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html" rel="noopener ugc nofollow" target="_blank"> Pandas.read_csv() </a></p></blockquote><p id="5609" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"> <em class="ls">丢弃不需要的数据(在本表中是['日期']列)</em> </strong></p><pre class="kh ki kj kk fq kl km kn ko aw kp dt"><span id="9610" class="kq kr ht km b fv ks kt l ku kv">df.drop(['Date'],axis=1)</span></pre><blockquote class="lp lq lr"><p id="2842" class="iq ir ls is b it iu iv iw ix iy iz ja lt jc jd je lu jg jh ji lv jk jl jm jn hm dt translated">这是删除“日期”后数据集中存在的特征。</p><p id="f5a7" class="iq ir ls is b it iu iv iw ix iy iz ja lt jc jd je lu jg jh ji lv jk jl jm jn hm dt translated"><em class="ht"> [| </em>价格(全部)|变动(全部)|价格(新)|变动(新)|</p><p id="b1b4" class="iq ir ls is b it iu iv iw ix iy iz ja lt jc jd je lu jg jh ji lv jk jl jm jn hm dt translated">价格(现代)|变化(现代)|价格(旧)|变化(旧)| <em class="ht"> ] </em></p></blockquote><p id="f2c4" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"> <em class="ls">定义X(特征集)和Y(目标)</em> </strong></p><pre class="kh ki kj kk fq kl km kn ko aw kp dt"><span id="1a3b" class="kq kr ht km b fv ks kt l ku kv">X=df[list(df.columns)[:-1]]<br/>Y=df[list(df.columns)[-1]]</span></pre><blockquote class="lp lq lr"><p id="de2b" class="iq ir ls is b it iu iv iw ix iy iz ja lt jc jd je lu jg jh ji lv jk jl jm jn hm dt translated">list()-&gt;以列表格式转换列名</p></blockquote><p id="9d15" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">['价格(全部)'，'变更(全部)'，'价格(新)'，'变更(新)'，'价格(现代)'，'变更(现代)'，'价格(旧)']</p><blockquote class="lx"><p id="dc18" class="ly lz ht bd ma mb mc md me mf mg jn ek translated">[:-1]-&gt;表示列表中最多第n-1个元素</p><p id="4710" class="ly lz ht bd ma mb mc md me mf mg jn ek translated">[:1]-&gt;仅lisy=t中的第n-1个元素</p></blockquote><blockquote class="lp lq lr"><p id="5529" class="iq ir ls is b it mh iv iw ix mi iz ja lt mj jd je lu mk jh ji lv ml jl jm jn hm dt translated">因此，Y或我们的目标特性成为‘价格(新)’,其余的成为我们的特性集或X</p></blockquote><p id="2766" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">X=['价格(全部)'，'变化(全部)'，'价格(新)'，'变化(新)'，'价格(现代)'，'变化(现代)'，'价格(旧)']</p><p id="5806" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">y =[更改(旧)]</p><pre class="kh ki kj kk fq kl km kn ko aw kp dt"><span id="e968" class="kq kr ht km b fv ks kt l ku kv"><strong class="km hu">from </strong>sklearn.model_selection <strong class="km hu">import </strong>train_test_split<br/>xtrain,xtest,ytrain,ytest=train_test_split(X,Y,random_state=0)<br/><strong class="km hu">from </strong>sklearn.linear_model <strong class="km hu">import </strong>LinearRegression<br/>reg=LinearRegression()<br/>reg.fit(xtrain,ytrain)</span></pre><p id="8d56" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"><em class="ls">trainttestsplit将数据集划分为75%训练25%测试数据</em> </strong></p><blockquote class="lp lq lr"><p id="7b25" class="iq ir ls is b it iu iv iw ix iy iz ja lt jc jd je lu jg jh ji lv jk jl jm jn hm dt translated">xTrain，xTest，yTrain，yTest=train_test_split(X，Y)</p></blockquote><p id="e8b8" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"> <em class="ls"> LinearRegression()。fit(X，Y)- &gt;将X值和Y值分别放入给定函数</em> </strong></p><blockquote class="lp lq lr"><p id="20d6" class="iq ir ls is b it iu iv iw ix iy iz ja lt jc jd je lu jg jh ji lv jk jl jm jn hm dt translated">Lreg =线性回归()。fit(xTrain，yTrain)</p></blockquote><p id="2889" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"> <em class="ls">精度测量</em> </strong></p><pre class="kh ki kj kk fq kl km kn ko aw kp dt"><span id="bdbf" class="kq kr ht km b fv ks kt l ku kv">reg.predict(xTest)<br/>Lreg.score(xTest,yTest)</span></pre><p id="213e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"> <em class="ls">。score(预测值，测试数据的Y轴)</em> </strong>方法返回准确性得分或预测值与实际值匹配的百分比。</p><p id="3711" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"> <em class="ls">。predict(测试数据的x轴)</em> </strong>返回xTest中每一项的预测值列表。</p><blockquote class="lx"><p id="f6a6" class="ly lz ht bd ma mb mc md me mf mg jn ek translated">输出:</p></blockquote><blockquote class="lp lq lr"><p id="1bd6" class="iq ir ls is b it mh iv iw ix mi iz ja lt mj jd je lu mk jh ji lv ml jl jm jn hm dt translated">['价格(所有)'，'变化(所有)'，'价格(新)'，'变化(新)'，'价格(现代)'，'变化(现代)'，'价格(旧)'][T20，9.949265286326835，31，9 . 45653653565</p></blockquote><p id="9825" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">如你所见，准确率为99%</p><blockquote class="lx"><p id="c0f0" class="ly lz ht bd ma mb mc md me mf mg jn ek translated">我想在这里包括多项式回归。但是它应该有一个自己的职位。所以我将展示另一个使用波士顿数据集的多元回归的例子</p></blockquote></div><div class="ab cl mm mn hb mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="hm hn ho hp hq"><pre class="kh ki kj kk fq kl km kn ko aw kp dt"><span id="fb19" class="kq kr ht km b fv ks kt l ku kv"><strong class="km hu">#Code for Boston DataSet <br/>from </strong>sklearn.datasets <strong class="km hu">import </strong>load_boston<br/><strong class="km hu">from </strong>sklearn.linear_model <strong class="km hu">import </strong>LinearRegression<br/><strong class="km hu">from </strong>sklearn.model_selection <strong class="km hu">import </strong>train_test_split<br/><br/>boston = load_boston()<br/>X = boston.data<br/>print(type(X))<br/>print(boston.keys())<br/>print('Feature names:',boston['feature_names'])<br/>Y = boston.target<br/>X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, shuffle= <strong class="km hu">True</strong>)<br/><br/>lineReg = LinearRegression()<br/>lineReg.fit(X_train, y_train)<br/>print(lineReg.score(X_test, y_test ))</span></pre><p id="e0f8" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"> <em class="ls">从Scikit-Learn数据集加载波士顿数据集:</em> </strong></p><pre class="kh ki kj kk fq kl km kn ko aw kp dt"><span id="174a" class="kq kr ht km b fv ks kt l ku kv"><strong class="km hu">from </strong>sklearn.datasets <strong class="km hu">import </strong>load_boston<br/>boston=load_boston()</span></pre><p id="2a38" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"> <em class="ls">定义X和Y并浏览数据集</em> </strong></p><pre class="kh ki kj kk fq kl km kn ko aw kp dt"><span id="fba4" class="kq kr ht km b fv ks kt l ku kv">print(type(X))<br/>print(boston.keys())<br/>print('Feature names:',boston['feature_names'])</span><span id="7a01" class="kq kr ht km b fv mt kt l ku kv">X = boston.data<br/>Y = boston.target</span></pre><blockquote class="lp lq lr"><p id="1138" class="iq ir ls is b it iu iv iw ix iy iz ja lt jc jd je lu jg jh ji lv jk jl jm jn hm dt translated">当您给boston.keys()-&gt;这将返回波士顿数据的主要特征</p><p id="4f41" class="iq ir ls is b it iu iv iw ix iy iz ja lt jc jd je lu jg jh ji lv jk jl jm jn hm dt translated">字典关键字(['特征名称'，'描述'，'目标'，'数据'])</p><p id="f22e" class="iq ir ls is b it iu iv iw ix iy iz ja lt jc jd je lu jg jh ji lv jk jl jm jn hm dt translated">' feature _ names '--&gt;数据集的功能名称或列，或简单地称为' X '</p><p id="427e" class="iq ir ls is b it iu iv iw ix iy iz ja lt jc jd je lu jg jh ji lv jk jl jm jn hm dt translated">描述每个功能的描述</p><p id="289b" class="iq ir ls is b it iu iv iw ix iy iz ja lt jc jd je lu jg jh ji lv jk jl jm jn hm dt translated">目标'-&gt;目标值或' Y '</p><p id="93fe" class="iq ir ls is b it iu iv iw ix iy iz ja lt jc jd je lu jg jh ji lv jk jl jm jn hm dt translated">数据'-&gt;数据值</p></blockquote><pre class="kh ki kj kk fq kl km kn ko aw kp dt"><span id="c59c" class="kq kr ht km b fv ks kt l ku kv">print('Feature names:',boston['feature_names'])<br/>print('DESCR:',boston['DESCR'])</span><span id="2f5c" class="kq kr ht km b fv mt kt l ku kv">print('Target ':',boston['target'])</span><span id="9090" class="kq kr ht km b fv mt kt l ku kv">print('data:',boston['data'])</span></pre><p id="e237" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"> <em class="ls">和</em> </strong>同旧同旧线性回归</p><pre class="kh ki kj kk fq kl km kn ko aw kp dt"><span id="ee44" class="kq kr ht km b fv ks kt l ku kv">X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, shuffle= <strong class="km hu">True</strong>)<br/><br/>lineReg = LinearRegression()<br/>lineReg.fit(X_train, y_train)<br/>print(lineReg.score(X_test, y_test ))</span></pre><blockquote class="lx"><p id="be1f" class="ly lz ht bd ma mb mu mv mw mx my jn ek translated">为你自己尝试一下</p></blockquote></div><div class="ab cl mm mn hb mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="hm hn ho hp hq"><p id="29e1" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">关注我的youtube : python教程和高级python编程</p><div class="jo jp fm fo jq jr"><a href="https://www.youtube.com/channel/UCTJE1mGfe5qgO5OfWE6surg?view_as=subscriber" rel="noopener  ugc nofollow" target="_blank"><div class="js ab ej"><div class="jt ab ju cl cj jv"><h2 class="bd hu fv z el jw eo ep jx er et hs dt translated">尼尔·巴塔查里亚</h2><div class="jy l"><h3 class="bd b fv z el jw eo ep jx er et ek translated">编程爱情</h3></div><div class="jz l"><p class="bd b gc z el jw eo ep jx er et ek translated">www.youtube.com</p></div></div><div class="ka l"><div class="mz l kc kd ke ka kf kg jr"/></div></div></a></div><p id="0635" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">关注我的博客</p><div class="jo jp fm fo jq jr"><a href="https://dataneel.wordpress.com/" rel="noopener  ugc nofollow" target="_blank"><div class="js ab ej"><div class="jt ab ju cl cj jv"><h2 class="bd hu fv z el jw eo ep jx er et hs dt translated">面向所有人的数据科学</h2><div class="jy l"><h3 class="bd b fv z el jw eo ep jx er et ek translated">在建:等到周三。谢谢你的耐心！！！线性回归是最简单的…</h3></div><div class="jz l"><p class="bd b gc z el jw eo ep jx er et ek translated">dataneel.wordpress.com</p></div></div><div class="ka l"><div class="na l kc kd ke ka kf kg jr"/></div></div></a></div></div></div>    
</body>
</html>