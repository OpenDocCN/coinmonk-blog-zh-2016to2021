<html>
<head>
<title>An Introduction to PyTorch by working on the Moons dataset using Neural Networks.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过使用神经网络处理卫星数据集来介绍PyTorch。</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/an-introduction-to-pytorch-by-working-on-the-moons-dataset-using-neural-networks-369b1ac6ccad?source=collection_archive---------1-----------------------#2018-07-23">https://medium.com/coinmonks/an-introduction-to-pytorch-by-working-on-the-moons-dataset-using-neural-networks-369b1ac6ccad?source=collection_archive---------1-----------------------#2018-07-23</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><blockquote class="iq"><p id="40a4" class="ir is ht bd it iu iv iw ix iy iz ja ek translated">“迄今为止，人工智能最大的危险在于，人们过早地断定自己理解它。”<br/> ― <a class="ae jb" href="https://www.goodreads.com/author/show/4533716.Eliezer_Yudkowsky" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">埃利泽</strong> </a></p></blockquote></div><div class="ab cl jc jd hb je" role="separator"><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh"/></div><div class="hm hn ho hp hq"><p id="b285" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated">这篇文章是学习如何使用深度学习库<a class="ae jb" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>系列文章的第一篇。这是一种尝试，旨在帮助那些刚刚开始学习人工智能，并且在学习其他高水平教程时遇到困难的人。本系列中的方法是首先理解PyTorch内置函数背后的代码，这样可以更好地理解其背后的理论，从而获得更全面的知识。在每一篇教程中，更多的定制代码将被内置函数所取代，从而充分利用PyTorch的强大功能。</p><p id="ea5b" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated">以下是将用于本教程的代码:</p><div class="kg kh fm fo ki kj"><a href="https://github.com/romanovacca/PyTorch-Moons-Neural-Nets/blob/master/Pytorch-moons-neural-nets.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="kk ab ej"><div class="kl ab km cl cj kn"><h2 class="bd hu fv z el ko eo ep kp er et hs dt translated">romanovacca/PyTorch-Moons-神经网络</h2><div class="kq l"><h3 class="bd b fv z el ko eo ep kp er et ek translated">PyTorch-Moons-Neural-Nets -教程的一部分“通过使用……</h3></div><div class="kr l"><p class="bd b gc z el ko eo ep kp er et ek translated">github.com</p></div></div><div class="ks l"><div class="kt l ku kv kw ks kx ky kj"/></div></div></a></div><h1 id="28d0" class="kz la ht bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw dt translated">PyTorch是如何工作的？</h1><p id="ef91" class="pw-post-body-paragraph jj jk ht jl b jm lx jo jp jq ly js jt ju lz jw jx jy ma ka kb kc mb ke kf ja hm dt translated">PyTorch目前相当新，因为他们最近刚刚发布了1.0版本。该框架有许多组件，但其中一个重要的组件是GPU的利用。这意味着PyTorch可以做和numpy一样的事情，但是使用GPU的能力来计算更快，这对神经网络非常有用。这里要提到的重要部分是，这只需要一行代码就可以激活，而Tensorflow和其他程序需要更多的工作。</p><p id="2d83" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated"><strong class="jl hu">所需包</strong></p><ul class=""><li id="c26c" class="mc md ht jl b jm jn jq jr ju me jy mf kc mg ja mh mi mj mk dt translated">PyTorch</li><li id="165c" class="mc md ht jl b jm ml jq mm ju mn jy mo kc mp ja mh mi mj mk dt translated">熊猫</li><li id="c888" class="mc md ht jl b jm ml jq mm ju mn jy mo kc mp ja mh mi mj mk dt translated">Numpy</li><li id="af37" class="mc md ht jl b jm ml jq mm ju mn jy mo kc mp ja mh mi mj mk dt translated">Matplotlib</li><li id="d3f7" class="mc md ht jl b jm ml jq mm ju mn jy mo kc mp ja mh mi mj mk dt translated">Sklearn</li><li id="c03b" class="mc md ht jl b jm ml jq mm ju mn jy mo kc mp ja mh mi mj mk dt translated">海生的</li></ul></div><div class="ab cl jc jd hb je" role="separator"><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh"/></div><div class="hm hn ho hp hq"><h1 id="a81a" class="kz la ht bd lb lc mq le lf lg mr li lj lk ms lm ln lo mt lq lr ls mu lu lv lw dt translated">了解PyTorch</h1><p id="0945" class="pw-post-body-paragraph jj jk ht jl b jm lx jo jp jq ly js jt ju lz jw jx jy ma ka kb kc mb ke kf ja hm dt translated">PyTorch使用称为张量的矩阵状结构。这些张量可以看作是矩阵的推广，看起来像n维数组(numpy中的ndarrays)。如果你想更深入地了解张量，点击<a class="ae jb" rel="noopener" href="/@quantumsteinke/whats-the-difference-between-a-matrix-and-a-tensor-4505fbdc576c">这里</a>。</p><figure class="mw mx my mz fq na fe ff paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="fe ff mv"><img src="../Images/cd01358d040ed5a445641567def86fc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3XVLl714RPatVq07uMseOw.jpeg"/></div></div><figcaption class="ng nh fg fe ff ni nj bd b be z ek">Vector/matrix/tensor visualisation from : <a class="ae jb" href="https://hackernoon.com/learning-ai-if-you-suck-at-math-p4-tensors-illustrated-with-cats-27f0002c9b32" rel="noopener ugc nofollow" target="_blank">https://hackernoon.com/learning-ai-if-you-suck-at-math-p4-tensors-illustrated-with-cats-27f0002c9b32</a></figcaption></figure><p id="c0f5" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated">要创建张量，我们首先要导入Pytorch模块，名为torch。</p><pre class="mw mx my mz fq nk nl nm nn aw no dt"><span id="6716" class="np la ht nl b fv nq nr l ns nt">import torch</span><span id="14da" class="np la ht nl b fv nu nr l ns nt">x0 = torch.tensor(10) # 0-dimensional tensor (single value)<br/>x1 = torch.tensor([10,2,5,2,10]) # 1-dimensional tensor (vector)<br/>x2 = torch.tensor([[10,2,4],[5,2,10]]) # 2-dimensional tensor (vector)<br/>x3 = torch.tensor([[[10,2],[5,2]], [[3,1],[15,8]]]) # 3-dimensional tensor (matrix)</span></pre><p id="c37b" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated">我们创建4个变量x0，x1，x2，x3，它们都有不同的形状，我们可以通过调用。形状，就像我们对普通numpy数组所做的那样。</p><pre class="mw mx my mz fq nk nl nm nn aw no dt"><span id="08ae" class="np la ht nl b fv nq nr l ns nt">(torch.Size([]), torch.Size([5]), torch.Size([2, 3]),<br/> torch.Size([2, 2, 2]))</span></pre><p id="26dd" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated">火炬张量也有如下类型:</p><ul class=""><li id="f92f" class="mc md ht jl b jm jn jq jr ju me jy mf kc mg ja mh mi mj mk dt translated">火炬。龙腾传感器</li><li id="4162" class="mc md ht jl b jm ml jq mm ju mn jy mo kc mp ja mh mi mj mk dt translated">火炬。漂浮者</li><li id="649f" class="mc md ht jl b jm ml jq mm ju mn jy mo kc mp ja mh mi mj mk dt translated">火炬。双张量</li></ul><p id="bd5e" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated"><strong class="jl hu">小心:</strong>张量类型在用它们做计算时需要匹配。如果得到关于类型不匹配的错误，可能需要设置张量的dtype。</p><pre class="mw mx my mz fq nk nl nm nn aw no dt"><span id="249b" class="np la ht nl b fv nq nr l ns nt">x1 = torch.tensor([[10,2,4],[5,2,10]])<br/>x2 = torch.tensor([[0.1, 10.67],[0.15,0.22]])<br/>x3 = torch.tensor([[0.1, 10.67],[0.15,0.22]], dtype=torch.double)</span></pre><p id="28b5" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated"><strong class="jl hu">将numpy转换为torch张量</strong></p><p id="0d8e" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated">PyTorch中一个强大的转换是从numpy数组到Torch张量的转换，反之亦然。</p><pre class="mw mx my mz fq nk nl nm nn aw no dt"><span id="cb90" class="np la ht nl b fv nq nr l ns nt">import numpy as np</span><span id="fe7b" class="np la ht nl b fv nu nr l ns nt">x_numpy = np.random.randn(10,2)<br/>x_torch = torch.tensor(x_numpy)<br/>type(x_numpy), type(x_torch), type(x_torch.numpy())</span><span id="cf61" class="np la ht nl b fv nu nr l ns nt">&gt;&gt;&gt; (numpy.ndarray, torch.Tensor, numpy.ndarray)</span></pre><p id="bed9" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated">现在您已经看到了PyTorch的一些功能，让我们从Moons数据集开始，探索PyTorch的其他特性。</p></div><div class="ab cl jc jd hb je" role="separator"><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh"/></div><div class="hm hn ho hp hq"><h1 id="6dd1" class="kz la ht bd lb lc mq le lf lg mr li lj lk ms lm ln lo mt lq lr ls mu lu lv lw dt translated">卫星数据集</h1><figure class="mw mx my mz fq na fe ff paragraph-image"><div class="fe ff nv"><img src="../Images/2656048a52a3c3289d0fa97855f7f94b.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*l-JofnGy39uEhvZwC0uvKQ.jpeg"/></div><figcaption class="ng nh fg fe ff ni nj bd b be z ek">Example Moons distribution</figcaption></figure><p id="472e" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated">moons数据集是来自<a class="ae jb" href="http://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>的一个简单的内置数据集。我们使用一个神经网络(我们将自己创建)来解决这个问题。</p><pre class="mw mx my mz fq nk nl nm nn aw no dt"><span id="216e" class="np la ht nl b fv nq nr l ns nt">import matplotlib.pyplot as plt<br/>from sklearn import datasets<br/># plt.style.use('seaborn') <br/># %matplotlib inline</span></pre><p id="3049" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated">我们导入所有必要的库。注意:如果你正在使用<a class="ae jb" href="http://jupyter.org/" rel="noopener ugc nofollow" target="_blank"> Jupyter笔记本</a>，你可以取消最后两行的注释，这样图形就可以在笔记本中生成。</p><pre class="mw mx my mz fq nk nl nm nn aw no dt"><span id="194d" class="np la ht nl b fv nq nr l ns nt">X,y = datasets.make_moons(n_samples=200, shuffle=True, noise=0.2, random_state=1234)<br/>y = np.reshape(y, (len(y),1))</span></pre><p id="8a6c" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated">我们创建两个变量，X和y，它们将存储数据点和相应的标签。我们希望有一个shape (200，1)的y数组，这样我们就可以将数据点匹配到正确的标签，从而进行整形。</p><p id="04eb" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated">现在我们有了数据，是时候制作我们自己的神经网络了！</p><p id="276f" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated"><strong class="jl hu">神经网络</strong></p><p id="d1cb" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated">PyTorch有自己内置的神经网络类，但出于演示和学习的目的，我们将构建自己的类。更多关于PyTorch神经网络的信息可以在<a class="ae jb" href="https://pytorch.org/docs/stable/nn.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><pre class="mw mx my mz fq nk nl nm nn aw no dt"><span id="a46f" class="np la ht nl b fv nq nr l ns nt">input_size = 2 <br/>hidden_size = 3 # randomly chosen<br/>output_size = 1 # we want it to return a number that can be used to calculate the difference from the actual number</span><span id="1dcd" class="np la ht nl b fv nu nr l ns nt">class NeuralNetwork():<br/>    def __init__(self, input_size, hidden_size, output_size):</span><span id="f280" class="np la ht nl b fv nu nr l ns nt"># weights <br/>        self.W1 = torch.randn(input_size, hidden_size, requires_grad=True)<br/>        self.W2 = torch.randn(hidden_size, hidden_size, requires_grad=True)<br/>        self.W3 = torch.randn(hidden_size, output_size, requires_grad=True)</span><span id="6ddd" class="np la ht nl b fv nu nr l ns nt"># Add bias<br/>        self.b1 = torch.randn(hidden_size, requires_grad=True)<br/>        self.b2 = torch.randn(hidden_size, requires_grad=True)<br/>        self.b3 = torch.randn(output_size, requires_grad=True)</span><span id="10b8" class="np la ht nl b fv nu nr l ns nt">    def forward(self, inputs):<br/>        z1 = inputs.mm(self.W1).add(self.b1)<br/>        a1 = 1 / (1 + torch.exp(-z1))<br/>        z2 = a1.mm(self.W2).add(self.b2)<br/>        a2 = 1 / (1 + torch.exp(-z2))<br/>        z3 = a2.mm(self.W3).add(self.b3)<br/>        output = 1 / (1 + torch.exp(-z3))<br/>        return output</span></pre><p id="62c4" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated">这里发生了很多事情，通过可视化可以更好地理解这些代码。让我们从__init__开始:</p><figure class="mw mx my mz fq na fe ff paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="fe ff nw"><img src="../Images/f201822d0b3a5102ce2743e88dbbef1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eC9xSzCZwdrr0E52LbmVJQ.png"/></div></div><figcaption class="ng nh fg fe ff ni nj bd b be z ek">Neural net visualization from <a class="ae jb" href="https://playground.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">https://playground.tensorflow.org/</a></figcaption></figure><p id="e1e4" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated">这张图片是我们正在构建的神经网络的可视化。输入大小(2)与图像中的两个特征(x1，x2)匹配。我们将隐藏大小指定为3，这可以看作是图像中的三个垂直块或神经元(每个隐藏层三个)。在代码中，我们指定了<strong class="jl hu"> W </strong> 1、<strong class="jl hu"> W </strong> 2、<strong class="jl hu"> W </strong> 3，它们是矩阵，因此大写字母W。这意味着例如<strong class="jl hu"> W </strong> 1存在于[w1，1 | w1，2 | w1，3 | w2，1 | w2，2 | w2，3]中。此外，还增加了一个偏差。</p><figure class="mw mx my mz fq na fe ff paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="fe ff nx"><img src="../Images/8ba9bbb6dd32e2d128c20b5ea728b02a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fvKb9llJKnPBU6OgGjH5CA.png"/></div></div><figcaption class="ng nh fg fe ff ni nj bd b be z ek">Forward propagation of another neural network</figcaption></figure><pre class="mw mx my mz fq nk nl nm nn aw no dt"><span id="aa16" class="np la ht nl b fv nq nr l ns nt">z1 = inputs.mm(self.W1).add(self.b1)<br/>a1 = 1 / (1 + torch.exp(-z1))</span></pre><p id="d0a8" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated">正向函数可以通过取<strong class="jl hu"> X </strong>乘以<strong class="jl hu"> W1 </strong>的和以及偏差来计算。在PyTorch中，我们使用函数"。“mm”是矩阵乘法的缩写(因为<strong class="jl hu"> X </strong> 1和<strong class="jl hu"> W </strong> 1是矩阵),我们给它加上偏差。然后，我们使用激活函数，在这种情况下，sigmoid在模型中引入非线性。这样做是为了让模型能够了解数据中更复杂的关系。该输出然后被用作下一层的输入，依此类推。</p><p id="53c5" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated">到目前为止，我们得到的代码只完成了一半的工作，它使我们能够通过网络提供输入，但网络需要适应以获得更好的结果，这是通过反向传播完成的。</p><pre class="mw mx my mz fq nk nl nm nn aw no dt"><span id="0bb8" class="np la ht nl b fv nq nr l ns nt">epochs = 10000<br/>learning_rate = 0.005</span><span id="7411" class="np la ht nl b fv nu nr l ns nt">model = NeuralNetwork(input_size, hidden_size, output_size)<br/>inputs = torch.tensor(X, dtype=torch.float)<br/>labels = torch.tensor(y, dtype=torch.float)</span><span id="4cda" class="np la ht nl b fv nu nr l ns nt">#store all the loss values<br/>losses = []</span></pre><p id="0537" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated">我们设置变量“model”，向它传递输入大小2、隐藏大小3和输出大小1。在这种情况下，输入和标签都是浮点数(x和y值)。我们创建了一个变量“损失”,这样我们可以在以后显示损失是如何沿着道路发展的。</p><p id="01ad" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated">然后，我们为反向传播创建一个循环:</p><pre class="mw mx my mz fq nk nl nm nn aw no dt"><span id="d2f4" class="np la ht nl b fv nq nr l ns nt">for epoch in range(epochs):</span><span id="72f4" class="np la ht nl b fv nu nr l ns nt"># forward function<br/>    output = model.forward(inputs)</span><span id="2947" class="np la ht nl b fv nu nr l ns nt">#BinaryCrossEntropy formula<br/>    loss = -((labels * torch.log(output)) + (1 - labels) * torch.log(1 - output)).sum()</span><span id="25dc" class="np la ht nl b fv nu nr l ns nt">#Log the log so we can plot it later<br/>    losses.append(loss.item())</span><span id="2e6f" class="np la ht nl b fv nu nr l ns nt">#calculate the gradients of the weights wrt to loss<br/>    loss.backward()</span><span id="5f21" class="np la ht nl b fv nu nr l ns nt">#adjust the weights based on the previous calculated gradients<br/>    model.W1.data -= learning_rate * model.W1.grad<br/>    model.W2.data -= learning_rate * model.W2.grad<br/>    model.W3.data -= learning_rate * model.W3.grad<br/>    model.b1.data -= learning_rate * model.b1.grad<br/>    model.b2.data -= learning_rate * model.b2.grad<br/>    model.b3.data -= learning_rate * model.b3.grad</span><span id="357c" class="np la ht nl b fv nu nr l ns nt">#clear the gradients so they wont accumulate<br/>    model.W1.grad.zero_()<br/>    model.W2.grad.zero_()<br/>    model.W3.grad.zero_()<br/>    model.b1.grad.zero_()<br/>    model.b2.grad.zero_()<br/>    model.b3.grad.zero_()</span><span id="3816" class="np la ht nl b fv nu nr l ns nt">print("Final loss: ", losses[-1])<br/>plt.plot(losses)</span></pre><p id="b867" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated">首先，我们通过模型传递输入，这给了我们一定的输出。PyTorch有一个内置函数来计算BinaryCrossEntropy，但是为了更好地理解它，我们自己构建了它。PyTorch可以通过调用。backward()，自动计算所有的梯度。现在我们有了权重应该改变的方向，为了更准确地预测某个输入的结果，我们必须实际更新它们。</p><pre class="mw mx my mz fq nk nl nm nn aw no dt"><span id="e464" class="np la ht nl b fv nq nr l ns nt">model.W1.data -= learning_rate * model.W1.grad</span></pre><p id="dcf9" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated">这里我们说W1的所有权重都要减(学习率*梯度)。这样做之后，我们需要将计算出的梯度设置为零，否则这些梯度会累积起来，搞乱我们的网络。</p><p id="f677" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated">最后，我们可以绘制损失图，看看我们的表现如何。</p><figure class="mw mx my mz fq na fe ff paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="fe ff ny"><img src="../Images/9d8d16209fa3298ae702a4bdfa28e60c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uEfi-e_LB9pf_4AiL61Hbw.png"/></div></div></figure></div><div class="ab cl jc jd hb je" role="separator"><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh"/></div><div class="hm hn ho hp hq"><h1 id="6011" class="kz la ht bd lb lc mq le lf lg mr li lj lk ms lm ln lo mt lq lr ls mu lu lv lw dt translated">丰富</h1><p id="2c3f" class="pw-post-body-paragraph jj jk ht jl b jm lx jo jp jq ly js jt ju lz jw jx jy ma ka kb kc mb ke kf ja hm dt translated">为了测试你对PyTorch实现的理解程度，我建议增加更多的层，或者改变隐藏的大小。添加另一个激活功能也是可能的。</p><h1 id="6ab1" class="kz la ht bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw dt translated">结论</h1><p id="035a" class="pw-post-body-paragraph jj jk ht jl b jm lx jo jp jq ly js jt ju lz jw jx jy ma ka kb kc mb ke kf ja hm dt translated">本文展示了如何通过使用低级内置函数开始使用PyTorch。在接下来的教程中，我们将使用PyTorch更多的内置函数。本文的目标是更好地理解神经网络是如何实现的。</p><p id="98bb" class="pw-post-body-paragraph jj jk ht jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf ja hm dt translated">在下一个系列中，我们将在<a class="ae jb" href="https://www.kaggle.com/c/titanic" rel="noopener ugc nofollow" target="_blank"> Titanic </a>数据集上使用PyTorch和神经网络。</p><div class="kg kh fm fo ki kj"><a href="https://github.com/romanovacca/PyTorch-Moons-Neural-Nets/blob/master/Pytorch-moons-neural-nets.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="kk ab ej"><div class="kl ab km cl cj kn"><h2 class="bd hu fv z el ko eo ep kp er et hs dt translated">romanovacca/PyTorch-Moons-神经网络</h2><div class="kq l"><h3 class="bd b fv z el ko eo ep kp er et ek translated">PyTorch-Moons-Neural-Nets -教程的一部分“通过使用……</h3></div><div class="kr l"><p class="bd b gc z el ko eo ep kp er et ek translated">github.com</p></div></div><div class="ks l"><div class="kt l ku kv kw ks kx ky kj"/></div></div></a></div></div></div>    
</body>
</html>