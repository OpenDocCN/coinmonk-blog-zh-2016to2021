# 家用机器人的强化学习。一

> 原文：<https://medium.com/coinmonks/reinforcement-learning-for-home-robotics-pt-1-43b8e32b7ad5?source=collection_archive---------3----------------------->

## 基于 ROS 的示例

![](img/fdefcfebba74579a65f33fbd89bb59c7.png)

Teach an Old Dog new Tricks?

## 介绍

在之前的文章中，我已经介绍了 Q 学习算法背后的基础知识。在这一系列的文章中，我想把迄今为止在这个博客中所做的事情结合起来，使用强化学习在 Gazebo 中模拟的环境中创建一个家庭机器人平台的模拟例子。

这可能会也可能不会完全成功。机器人学的部分乐趣在于尝试新想法，而新想法有时会失败。然而，至少这组文章将介绍大量关于使用 ROS、机器学习的有用概念，以及我们如何能够在实际项目中使用 RL，而不必求助于训练 DeepRL 模型。

> 注意:请不要认为我在说 DeepRL 有什么问题——将深度学习用于 RL 是一个令人惊叹的主题，也是我可能会在这篇博客中涉及的一个主题——它只是不是我将用于这个特定实验的技术。

## Q 学习的方法

从这一点出发，现在我们可以采取几个方向。传统的方向似乎是开始将 RL 方法扩展到更复杂的系统，例如从头开始学习如何玩 Atari 游戏，或者在没有事先知道如何控制手臂的情况下，教机器人手臂捡起盒子。

这些方法很有趣，产生了很多研究(特别是在应用于 RL 的深度学习领域)，并且理所当然地在现代研究中获得了很多关注。在我看来，这不是我们可以走的唯一方向，在我们开始研究深度学习(首先需要有自己的一套文章)之前，我想看一个例子，看看我们可以用迄今为止描述的工具做什么。

让我们从另一个想象的世界开始这个过程——这一次，保持这个博客调查家庭服务机器人设计的主题，我们将从一所房子开始。

## 马文的家

![](img/88596e69298169b16bdda01a4f2e14b3.png)

A virtual Home for Marvin

在上面的图片中，我们可以看到一个假想家庭的平面图模型。这个房子目前相当稀疏——它有三个人(由简笔画代表)，一罐苏打水(由红色椭圆形代表)，以及由图像左上角的小机器人图标代表的马文。

在这种情况下，马文是我们的室内机器人。为了与本博客致力于开发一个可用的家庭机器人平台的目标保持一致，Marvin 将被赋予一些与之前[篇文章](/@genefoxwell/introducing-marvin-3316f59b92ec)中分配给它的职责一致的家庭职责:

1.  它应该能够在被召唤时出现——因此，如果用户发出希望机器人出现的信号，它就会向它们走来。
2.  它应该能够为用户交付(在我们的例子中也可以拾取)对象。
3.  它应该在用户已经表示不需要的地方留出空间(可能是出于隐私原因)。
4.  当需要充电时，它应该回到它的原位(左上角)。

显而易见，在许多情况下，这些目标可以被认为是直接竞争的。我们这篇文章以及之后几篇文章的目标是尝试使用强化学习来教会机器人如何处理出现的任何情况。

我们开始吧！

## 假设

在这个实验中，我将假设一个机器人是用我在博客中介绍的工具[制造出来的。因此，在应用 RL 之前，我们已经有了以下可用功能:](/@genefoxwell/marvins-head-pt-1-faf260831883)

*   完全配置了 ROS 导航包的支持 ROS 的机器人。
*   这个机器人能够对简单的口头请求做出反应。
*   机器人能够创建自己的地图。
*   该机器人能够创建和维护一个“[黑板](/coinmonks/blackboard-pattern-ed3981551908)，用于整理各种 ML 和感觉系统收集的信息。

本质上，除了“包容”系统这一显著的例外，我们假设一个机器人具有本博客迄今为止描述的“马文”机器人所展示的所有能力。

## 环境

假设已经完成，让我们从 RL 代理的角度来看一下环境。简单地将房子本身作为我们 RL 代理的环境可能很有诱惑力，但我认为这会造成一些困难，用我们目前掌握的工具很难克服。

> *:使用传统的深度 RL 方法也许可以实现这一点，但这正是我在接下来的几篇文章中试图避免的。

相反，我们的代理将“生活”在由我们的[黑板子系统](/coinmonks/blackboard-pattern-ed3981551908)生成的图形世界中。从这个角度来看，一旦地图绘制完成，我们的世界可以更好地用下图来表示:

![](img/01ed83117f210a2c060f5a3ba86651a4.png)

A “Blackboard”

所以上面的“黑板”在地图上以节点的形式表示了家中不同的可到达区域。然后，每个节点都用机器人各种图像识别算法识别为存在的对象列表来装饰。为了这个实验的目的，我们对马文如何做到这一点不感兴趣(也许是一个神奇的精灵告诉它哪些物体存在，以及地图看起来像什么),只是它做到了。马文在“黑板”上的位置由绿色节点表示。

## 代理人

![](img/cc67220c314551f8be9e9c2790b83827.png)

Home Services RL Robot Robot Architecture V0

这次公式化我们的 RL 问题的主要问题将是代理本身。在上面的图片中，我提供了一个我想要着手的架构的例子。这将 RL 代理置于机器人决策系统的“中间”,其输入首先被处理(可能通过深度神经网络或经典机器学习算法),并且该处理的结果被放置在共享黑板上。然后，RL 代理可以访问黑板上的后处理数据，并从中做出决策。

在这里所示的抽象层次上，RL 代理不能直接访问任何机器人的低级系统——它通过决定哪个子进程应该是活动的，以及哪个子进程应该是不活动的来控制机器人。这些子过程将控制机器人。

对于这个问题的第一次尝试，让我们从以下行为开始:

1.  代理可以进入“获取”模式，在这种模式下，它识别其所在节点中的 pop-can 并拾取它。
2.  代理可以进入“查找人”模式，在这种模式下，它搜索最近的用户。
3.  代理可以进入充电模式，在这种模式下，它返回到它的充电器。
4.  代理可以进入等待模式，在该模式下等待指令。

对于 Q-Learning 来说，这些过程可能太复杂而无法学习，但这是即将到来的实验的一部分，以找出我们可以从物理机器人中抽象出多少 RL 学习过程。

我们可以合理地找到方法，让一个标准的家庭服务机器人拥有这些行为中的大部分——拿起汽水罐这一点可能会造成一些困难，但不足以成为这个实验的关注点。决定导航到用户、转到 pop-can 节点或等待指令都可以通过向 ROS 导航堆栈提供合理的参数来处理，并且我们将假设系统正在这样做。

此外，至少对于这个问题的第一次迭代，我们将假设机器人生活的世界是静态的——因此我们的“人”基本上是放置在世界中的不动的人体模型，让我们的机器人有事可做。

根据我们在本文中提到的，我们的代理可以采取的“行动”相当于激活或取消机器人预编程的行为。在此基础上，RL 算法取代了先前设计中的[包容](/@genefoxwell/decentralized-subsumption-8237316fb335)。

## 计划

因此，这是我们在接下来的几篇文章中试图解决的问题。以下是我们将尝试遵循的计划:

1.  我们将为马文建造一个新的露台世界。
2.  我们将为马文机器人建造一个“RL”节点，它将允许我们控制马文并训练它。
3.  我们将向 Marvin 的设置中添加额外的节点，以便获得我们需要的信息:可乐罐的存在、个人的存在等。
4.  我们将使用 Q-Learning 算法来训练 Marvin，该算法在我以前就此主题撰写的文章中有介绍。

同样，值得注意的是，这可能不会创造出一个成功的机器人——它可能会以各种不良的方式表现出来。本系列的目标不是成功，而是探索这种方法是否可行。

在下一篇文章中，我将指导你在 Gazebo 中创建一个环境，这个环境接近目前为止所描述的世界。

在那之前，

分享享受！

> [在您的收件箱中直接获得最佳软件交易](https://coincodecap.com/?utm_source=coinmonks)

[![](img/7c0b3dfdcbfea594cc0ae7d4f9bf6fcb.png)](https://coincodecap.com/?utm_source=coinmonks)