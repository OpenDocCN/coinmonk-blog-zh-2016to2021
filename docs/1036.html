<html>
<head>
<title>The Mathematics of Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络的数学</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/the-mathematics-of-neural-network-60a112dd3e05?source=collection_archive---------0-----------------------#2018-07-14">https://medium.com/coinmonks/the-mathematics-of-neural-network-60a112dd3e05?source=collection_archive---------0-----------------------#2018-07-14</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><p id="7757" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">所以我的上一篇文章是对MLP的一个非常基本的描述。在这篇文章中，我将处理MLP中涉及的所有数学问题。没看过上一篇文章的，可以在这里看<a class="ae jo" rel="noopener" href="/@temi.ayo.babs/multi-layer-perceptron-for-beginners-6aee246c6a03">。不浪费时间，让我们开始吧。</a></p><h1 id="ba9a" class="jp jq ht bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km dt translated"><strong class="ak">基础知识</strong></h1><p id="5d8c" class="pw-post-body-paragraph iq ir ht is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hm dt translated">关于神经网络数学，你必须知道的第一件事是，它非常简单，任何人都可以用笔、纸和计算器来解决它(不是你想要的)。然而，你可能有超过几十万个神经元，所以可能要花很长时间才能解决。其次，大量的计算涉及矩阵。如果你对矩阵不熟悉，你可以在这里找到一篇很棒的文章<a class="ae jo" href="http://www.maths.manchester.ac.uk/~kd/ma2m1/matrices.pdf" rel="noopener ugc nofollow" target="_blank"/>，它很有解释力。</p><h2 id="5b2f" class="ks jq ht bd jr kt ku kv jv kw kx ky jz jb kz la kd jf lb lc kh jj ld le kl lf dt translated">1.砝码</h2><p id="b799" class="pw-post-body-paragraph iq ir ht is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hm dt translated">正如上一篇文章所强调的，权重是神经元之间的连接，它承载着一个值。值越高，权重越大，我们越重视权重输入端的神经元。此外，在数学和编程中，我们以矩阵格式查看权重。我们用一个图像来说明。</p><figure class="lh li lj lk fq ll fe ff paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="fe ff lg"><img src="../Images/2ebfc87fe576b6467edc43597d125dee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1JCfvimyyFm1yqWMYmnm2g.png"/></div></div><figcaption class="ls lt fg fe ff lu lv bd b be z ek">Image taken from <a class="ae jo" href="https://www.researchgate.net/figure/A-simple-neural-network-and-the-mapping-of-the-first-hidden-layer-onto-a-43-Weight_fig2_292077006" rel="noopener ugc nofollow" target="_blank">here</a></figcaption></figure><p id="7105" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">如图所示，输入层有3个神经元，下一层(隐藏层)有4个。我们可以创建一个3行4列的矩阵，并将每个权重的值插入到矩阵中，如上所述。这个矩阵将被称为<strong class="is hu"> W1 </strong>。在我们有更多层的情况下，我们将有更多的权重矩阵W2、W3等。<br/>一般来说，如果一层L有N个神经元，而下一层L+1有M个神经元，那么<strong class="is hu">权重矩阵</strong>就是一个N乘M的矩阵(N行M列)。</p><p id="165c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">再次仔细观察图像，您会发现矩阵中最大的数字是<strong class="is hu"><em class="lw">【W22】</em></strong>，其值为<strong class="is hu"> <em class="lw"> 9 </em> </strong>。我们的<strong class="is hu"> <em class="lw"> W22 </em> </strong>将输入层的<strong class="is hu"> <em class="lw"> IN2 </em> </strong>连接到隐藏层的<strong class="is hu"> <em class="lw"> N2 </em> </strong>。这意味着<strong class="is hu"><em class="lw">在这个状态下】</em> </strong>或<strong class="is hu"> <em class="lw">目前</em> </strong>，我们的<strong class="is hu"><em class="lw"/></strong>认为输入<strong class="is hu"> <em class="lw"> IN2 </em> </strong>是它在做出自己的微小决定时所收到的所有3个输入中最重要的。</p><h2 id="f194" class="ks jq ht bd jr kt ku kv jv kw kx ky jz jb kz la kd jf lb lc kh jj ld le kl lf dt translated">2.偏见</h2><p id="3a2d" class="pw-post-body-paragraph iq ir ht is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hm dt translated">偏见也是一个砝码。想象你正在思考一个情况(试图做出决定)。你必须考虑所有可能的(或可观察到的)因素。但是那些你没有遇到过的参数呢？那些你没有考虑到的因素呢？在神经网络中，我们试图迎合这些不可预见或不可观察的因素。这就是偏见。每个不在输入层上的神经元都有一个偏置，这个偏置就像权重一样，带有一个值。下图就是一个很好的例证。</p><figure class="lh li lj lk fq ll fe ff paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="fe ff lx"><img src="../Images/671e6deb3cabf48c56114a2ad0feb08a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3w6P6OHcU3j0T_lRUmnjQw.jpeg"/></div></div><figcaption class="ls lt fg fe ff lu lv bd b be z ek">Image taken from <a class="ae jo" href="https://www.google.com.ng/url?sa=i&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiNrsqCtJ7cAhUJmRoKHTYxBJcQjhx6BAgBEAM&amp;url=https%3A%2F%2Fvisualstudiomagazine.com%2Farticles%2F2014%2F11%2F01%2Fuse-python-with-your-neural-networks.aspx&amp;psig=AOvVaw1GAUpVNKTwPerWNaKmwH_C&amp;ust=1531651320014699" rel="noopener ugc nofollow" target="_blank">here</a></figcaption></figure><p id="24d5" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">仔细观察隐藏层和输出层中的层(分别有4个和2个神经元)，您会发现每个神经元都有一个指向它的红色/蓝色小箭头。你还会发现这些小箭头没有源神经元。</p><p id="5cf5" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">就像权重可以被视为一个矩阵一样，偏差也可以被视为一列矩阵(一个<strong class="is hu">向量</strong>)。例如，上面隐藏层的偏差可以表示为[[0.13]，[0.14]，[0.15]，[0.16]]。</p><h2 id="c748" class="ks jq ht bd jr kt ku kv jv kw kx ky jz jb kz la kd jf lb lc kh jj ld le kl lf dt translated">3.激活</h2><p id="203f" class="pw-post-body-paragraph iq ir ht is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hm dt translated">在这一节，让我们把注意力集中在单个神经元上。将所有输入聚合到其中后，我们称这个聚合为<em class="lw"> z </em> <strong class="is hu">(不要担心聚合，我稍后会解释。现在，只要把进入神经元的所有东西都表示为z) </strong>，神经元应该对那个输出做出一个<strong class="is hu">微小的</strong>决定，并返回另一个输出。这个过程(或功能)称为激活。我们将其表示为<em class="lw"> f(z) </em>，其中<em class="lw"> z </em>是所有输入的聚合。有两大类激活，线性和非线性。如果<em class="lw"> f(z)=z </em>，我们说<em class="lw"> f(z) </em>是一个<strong class="is hu">线性</strong>激活(即什么也没发生)。其余的是非线性的，描述如下。</p><figure class="lh li lj lk fq ll fe ff paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="fe ff ly"><img src="../Images/8ff77a1629ec302fbd242baea2e0850d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0lejoYyyQWjYzEP_BNW2nw.jpeg"/></div></div><figcaption class="ls lt fg fe ff lu lv bd b be z ek">Image taken from <a class="ae jo" href="https://www.google.com.ng/url?sa=i&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiJzvu-uJ7cAhUPyoUKHRiADuIQjhx6BAgBEAM&amp;url=https%3A%2F%2Fwww.learnopencv.com%2Funderstanding-activation-functions-in-deep-learning%2F&amp;psig=AOvVaw3pihV0B7X71yg__4FwbpKv&amp;ust=1531652449551299" rel="noopener ugc nofollow" target="_blank">here</a></figcaption></figure><p id="c644" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">我们的神经元可以通过几种方式做出决定，几种选择f(z) 可能是什么。这里重点介绍了一些流行的方法:</p><ul class=""><li id="44b8" class="lz ma ht is b it iu ix iy jb mb jf mc jj md jn me mf mg mh dt translated"><strong class="is hu">整流线性单元(ReLU)——</strong>有了ReLU，我们确保我们的输出不会低于零(或负)。因此，如果<em class="lw"> z </em>大于零，我们的输出保持为<em class="lw"> z </em>，否则，如果<em class="lw"> z </em>为负，我们的输出为零。公式为<strong class="is hu"> <em class="lw"> f(z) = max(0，z) </em> </strong> <em class="lw">。</em>长话短说，我们选择0和<em class="lw"> z </em>之间的最大值。这里有一篇关于ReLU的可爱的文章。</li><li id="6a56" class="lz ma ht is b it mi ix mj jb mk jf ml jj mm jn me mf mg mh dt translated"><strong class="is hu"> Tanh </strong> —这里我们的<strong class="is hu"> <em class="lw"> f(z) = tanh(z) </em> </strong>。就这么简单。我们求出<em class="lw"> z </em>的双曲正切并返回。别担心，你的科学计算器可以做到这一点。</li><li id="ff8c" class="lz ma ht is b it mi ix mj jb mk jf ml jj mm jn me mf mg mh dt translated"><strong class="is hu">乙状结肠激活— </strong>这次我们用的公式是:<br/><strong class="is hu"><em class="lw">f(x)= 1/(1+e^(-1*z))</em></strong><em class="lw">。</em>按照以下步骤:<br/> 1。通过乘以-1对z<em class="lw">求反。<br/> 2。求1中输出的指数。同样，你的计算器可以做到这一点。<br/> 3。在2的输出中添加1。<br/> 4。用1除以3的输出。<br/> <strong class="is hu">如此而已。</strong></em></li></ul><p id="1308" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">请注意，还有更多非线性激活函数，这些恰好是最广泛使用的。此外，函数的选择很大程度上取决于你试图解决的问题或你的神经网络试图学习什么。</p><h1 id="ed26" class="jp jq ht bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km dt translated">数学</h1><p id="2580" class="pw-post-body-paragraph iq ir ht is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hm dt translated">既然你知道了基础知识，是时候做数学了。还记得这个吗？</p><figure class="lh li lj lk fq ll fe ff paragraph-image"><div class="fe ff mn"><img src="../Images/c58fbfca7fe750c6ead14ce713b81dff.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*4MVN69gdM72BtTtY75tntg.png"/></div></figure><p id="f4fc" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">是的，你在上面关于激活函数的图片中看到了。下面是我承诺的关于聚合的解释:</p><p id="433a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">看到括号里的东西了吗？称之为你的<em class="lw"> z </em>。然后；</p><ul class=""><li id="639d" class="lz ma ht is b it iu ix iy jb mb jf mc jj md jn me mf mg mh dt translated">b  =偏见</li><li id="22c3" class="lz ma ht is b it mi ix mj jb mk jf ml jj mm jn me mf mg mh dt translated"><em class="lw"> x </em> =神经元的输入</li><li id="4cc2" class="lz ma ht is b it mi ix mj jb mk jf ml jj mm jn me mf mg mh dt translated">w =重量</li><li id="0088" class="lz ma ht is b it mi ix mj jb mk jf ml jj mm jn me mf mg mh dt translated"><em class="lw"> n </em> =来自输入层的输入数量</li><li id="0537" class="lz ma ht is b it mi ix mj jb mk jf ml jj mm jn me mf mg mh dt translated"><em class="lw"> i </em> =从1到<em class="lw"> n </em>的计数器</li></ul><p id="043f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">在我们继续之前，请注意'<strong class="is hu">最初是'</strong>，唯一具有值的神经元是输入层上的输入神经元(它们是从我们用来训练网络的数据中观察到的值)。那么，这是如何工作的呢？</p><ol class=""><li id="d3bf" class="lz ma ht is b it iu ix iy jb mb jf mc jj md jn mo mf mg mh dt translated">将每个传入的神经元乘以其相应的权重。</li><li id="7a35" class="lz ma ht is b it mi ix mj jb mk jf ml jj mm jn mo mf mg mh dt translated">把这些值加起来。</li><li id="d289" class="lz ma ht is b it mi ix mj jb mk jf ml jj mm jn mo mf mg mh dt translated">添加有问题的神经元的偏置项。</li></ol><p id="8a04" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">对于我们的神经元来说，这就是评估<em class="lw"> z </em>的全部内容。简单吧？</p><blockquote class="mp mq mr"><p id="824b" class="iq ir lw is b it iu iv iw ix iy iz ja ms jc jd je mt jg jh ji mu jk jl jm jn hm dt translated">但是想象一下，你必须对每一层(你可能有数百个)中的每一个神经元(你可能有数千个神经元)都这样做，这将需要花很长时间才能解决。这是我们使用的技巧:</p></blockquote><p id="beed" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">还记得我们说过的矩阵(<em class="lw">和向量</em>)吗？这是我们开始使用它们的时候。请遵循以下步骤:</p><ol class=""><li id="f0fb" class="lz ma ht is b it iu ix iy jb mb jf mc jj md jn mo mf mg mh dt translated">如前所述，创建从输入层到输出层的权重矩阵；例如N乘M矩阵。</li><li id="7117" class="lz ma ht is b it mi ix mj jb mk jf ml jj mm jn mo mf mg mh dt translated">根据偏差创建一个M乘1矩阵。</li><li id="b523" class="lz ma ht is b it mi ix mj jb mk jf ml jj mm jn mo mf mg mh dt translated">将输入图层视为一个N乘1的矩阵(或大小为N的向量，就像偏差一样)。</li><li id="e7df" class="lz ma ht is b it mi ix mj jb mk jf ml jj mm jn mo mf mg mh dt translated">转置权重矩阵，现在我们有一个M乘N的矩阵。</li><li id="e681" class="lz ma ht is b it mi ix mj jb mk jf ml jj mm jn mo mf mg mh dt translated">求转置权重和输入的点积。根据点积法则，如果你找到一个M乘N矩阵和一个N乘1矩阵的点积，你就得到一个M乘1矩阵。</li><li id="4e8f" class="lz ma ht is b it mi ix mj jb mk jf ml jj mm jn mo mf mg mh dt translated">将第5步的输出添加到偏差矩阵中(如果您做得正确，它们肯定会有相同的大小)。</li><li id="c384" class="lz ma ht is b it mi ix mj jb mk jf ml jj mm jn mo mf mg mh dt translated">最后，你有神经元的值，它应该是一个M乘1的矩阵(大小为M的向量)</li></ol><p id="03c5" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">之后，对向量中的每个值运行您选择的激活函数。</p><p id="acfe" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">对你拥有的每一个权重矩阵都这样做，在前进的过程中找到神经元/单元的值。继续前进，直到到达网络的末端(输出层)。</p><p id="d05f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">就这样。但不是终点。在这一部分，我们计算距离原始输出有多远，并尝试纠正错误。我将在接下来的文章中描述这些。</p><blockquote class="mp mq mr"><p id="0915" class="iq ir lw is b it iu iv iw ix iy iz ja ms jc jd je mt jg jh ji mu jk jl jm jn hm dt translated"><strong class="is hu">警告:</strong>此方法仅适用于<strong class="is hu">全连接网络</strong>。其他类型网络的权重矩阵不同。</p></blockquote><p id="1b21" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">现在，您可以构建一个神经网络，并根据给定的输入计算其输出。正如你所看到的，这非常非常容易。拍拍自己的背，吃个冰淇淋，不是每个人都能做到的。</p></div><div class="ab cl mv mw hb mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="hm hn ho hp hq"><p id="c06e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">感谢您阅读本文，请注意即将发表的文章，因为您还没有完成。</p><blockquote class="nc"><p id="5237" class="nd ne ht bd nf ng nh ni nj nk nl jn ek translated">加入Coinmonks <a class="ae jo" href="https://t.me/coincodecap" rel="noopener ugc nofollow" target="_blank">电报频道</a>和<a class="ae jo" href="https://www.youtube.com/c/coinmonks/videos" rel="noopener ugc nofollow" target="_blank"> Youtube频道</a>获取每日<a class="ae jo" href="http://coincodecap.com/" rel="noopener ugc nofollow" target="_blank">加密新闻</a></p></blockquote><h2 id="15d6" class="ks jq ht bd jr kt nm kv jv kw nn ky jz jb no la kd jf np lc kh jj nq le kl lf dt translated">此外，请阅读</h2><ul class=""><li id="20fb" class="lz ma ht is b it kn ix ko jb nr jf ns jj nt jn me mf mg mh dt translated"><a class="ae jo" href="http://Top 4 Telegram Channels for Crypto Traders" rel="noopener ugc nofollow" target="_blank">密码电报信号</a> | <a class="ae jo" rel="noopener" href="/coinmonks/crypto-trading-bot-c2ffce8acb2a">密码交易机器人</a></li><li id="14e6" class="lz ma ht is b it mi ix mj jb mk jf ml jj mm jn me mf mg mh dt translated"><a class="ae jo" rel="noopener" href="/coinmonks/top-10-crypto-copy-trading-platforms-for-beginners-d0c37c7d698c">拷贝交易</a> | <a class="ae jo" rel="noopener" href="/coinmonks/crypto-tax-software-ed4b4810e338">密码税务软件</a></li><li id="723e" class="lz ma ht is b it mi ix mj jb mk jf ml jj mm jn me mf mg mh dt translated"><a class="ae jo" href="https://coincodecap.com/grid-trading" rel="noopener ugc nofollow" target="_blank">网格交易</a> | <a class="ae jo" rel="noopener" href="/coinmonks/the-best-cryptocurrency-hardware-wallets-of-2020-e28b1c124069">加密硬件钱包</a></li><li id="f33b" class="lz ma ht is b it mi ix mj jb mk jf ml jj mm jn me mf mg mh dt translated"><a class="ae jo" rel="noopener" href="/coinmonks/crypto-exchange-dd2f9d6f3769">最佳加密交换机</a> | <a class="ae jo" rel="noopener" href="/coinmonks/bitcoin-exchange-in-india-7f1fe79715c9">印度最佳加密交换机</a></li><li id="47a8" class="lz ma ht is b it mi ix mj jb mk jf ml jj mm jn me mf mg mh dt translated"><a class="ae jo" rel="noopener" href="/coinmonks/best-crypto-apis-for-developers-5efe3a597a9f">开发人员的最佳加密API</a></li><li id="b359" class="lz ma ht is b it mi ix mj jb mk jf ml jj mm jn me mf mg mh dt translated">最佳<a class="ae jo" rel="noopener" href="/coinmonks/top-5-crypto-lending-platforms-in-2020-that-you-need-to-know-a1b675cec3fa">密码借出平台</a></li><li id="9487" class="lz ma ht is b it mi ix mj jb mk jf ml jj mm jn me mf mg mh dt translated"><a class="ae jo" rel="noopener" href="/coinmonks/leveraged-token-3f5257808b22">杠杆令牌</a>终极指南</li><li id="95d1" class="lz ma ht is b it mi ix mj jb mk jf ml jj mm jn me mf mg mh dt translated"><a class="ae jo" href="https://coincodecap.com/best-vpns-for-crypto-trading" rel="noopener ugc nofollow" target="_blank">加密交易的最佳虚拟专用网络</a></li><li id="918f" class="lz ma ht is b it mi ix mj jb mk jf ml jj mm jn me mf mg mh dt translated"><a class="ae jo" href="https://coincodecap.com/blockchain-analytics" rel="noopener ugc nofollow" target="_blank">链上数据的最佳加密分析</a> | <a class="ae jo" href="https://coincodecap.com/bexplus-review" rel="noopener ugc nofollow" target="_blank"> Bexplus Review </a></li><li id="51af" class="lz ma ht is b it mi ix mj jb mk jf ml jj mm jn me mf mg mh dt translated"><a class="ae jo" href="https://coincodecap.com/nft-marketplaces" rel="noopener ugc nofollow" target="_blank">NFT十大市场造币集锦</a></li><li id="f72f" class="lz ma ht is b it mi ix mj jb mk jf ml jj mm jn me mf mg mh dt translated"><a class="ae jo" href="https://coincodecap.com/ascendex-staking" rel="noopener ugc nofollow" target="_blank">AscendEx Staking</a>|<a class="ae jo" href="https://coincodecap.com/bot-ocean-review" rel="noopener ugc nofollow" target="_blank">Bot Ocean Review</a>|<a class="ae jo" href="https://coincodecap.com/bitcoin-wallets-india" rel="noopener ugc nofollow" target="_blank">最佳比特币钱包</a></li><li id="4ec1" class="lz ma ht is b it mi ix mj jb mk jf ml jj mm jn me mf mg mh dt translated"><a class="ae jo" href="https://coincodecap.com/bitget-review" rel="noopener ugc nofollow" target="_blank"> Bitget回顾</a>|<a class="ae jo" href="https://coincodecap.com/gemini-vs-blockfi" rel="noopener ugc nofollow" target="_blank">Gemini vs block fi</a>|<a class="ae jo" href="https://coincodecap.com/okex-futures-trading" rel="noopener ugc nofollow" target="_blank">OKEx期货交易</a></li></ul></div></div>    
</body>
</html>