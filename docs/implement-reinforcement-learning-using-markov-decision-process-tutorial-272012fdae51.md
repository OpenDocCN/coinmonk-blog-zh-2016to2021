# 使用马尔可夫决策过程实现强化学习[教程]

> 原文：<https://medium.com/coinmonks/implement-reinforcement-learning-using-markov-decision-process-tutorial-272012fdae51?source=collection_archive---------3----------------------->

**马尔科夫** **决策过程**，更广为人知的是 **MDP** ，是一种在网格世界环境中进行决策的强化学习方法。gridworld 环境由网格形式的状态组成。

MDP 试图通过将世界划分为状态、动作、模型/过渡模型和奖励来以网格的形式捕捉世界。MDP 的解决方案称为策略，目标是为 MDP 任务找到最佳策略。

因此，任何由一组状态、动作和奖励组成的、遵循马尔可夫特性的强化学习任务都将被认为是 MDP。

在本教程中，我们将深入探讨 MDP、状态、行动、奖励、政策，以及如何使用贝尔曼方程来解决它们。

> 本文是摘自《TensorFlow 强化学习[一书的强化学习教程。](https://www.packtpub.com/big-data-and-business-intelligence/reinforcement-learning-tensorflow)

# 马尔可夫决策过程

MDP 被定义为以下各项的集合:

*   **状态** : S
*   **动作** : A(s)，A
*   **转换模型** : T(s，a，s') ~ P(s'|s，a)
*   **奖励** : R(s)，R(s，a)，R(s，a，s ')
*   **政策**:

![](img/42c4f86075c17020c8af991fae990bd1.png)![](img/d827dd9f5dba4e82f8e6f64629cf60f6.png)

*   是最佳策略

在 MDP 的情况下，环境是完全可观察的，也就是说，无论智能体在任何时间点进行什么观察，都足以做出最优决策。在部分可观察环境的情况下，代理需要一个存储器来存储过去的观察结果，以做出最好的可能决策。

让我们试着把这个分解成不同的乐高积木，来理解这个整体过程意味着什么。

# 马尔可夫性质

简而言之，根据**马尔可夫性质**，为了知道近期的信息(比如在 *t+1* 时刻的信息)，在 *t* 时刻的当前信息是重要的。

给定一个序列，

![](img/4963dd23eccdca8549085502b5640a8f.png)

，一阶马尔可夫说，

![](img/ef574fe1edf5133fdb4a62aca41dd556.png)

，也就是，

![](img/1137898299244678b9ef645a52cbc3a8.png)

仅取决于

![](img/3e9aab5c3cd9384fa6e0bafc112dd172.png)

。因此，

![](img/d5da99622da07ad6c085a5edf71b67da.png)

将只取决于

![](img/95acf3bbe1510b17bdc059b379359cec.png)

。马尔可夫的二阶理论认为，

![](img/17da2313d6ffc2349d9ab1a9ce153437.png)

，也就是，

![](img/b1b30c27a52d44ccc6262af2104ca9f4.png)

仅取决于

![](img/4334174b17645ca36248e973dd5aa249.png)

和

![](img/ce13cd3b1ee682b660ba3c832d7b3c7f.png)

在我们的上下文中，从现在起我们将遵循马尔可夫性质的一阶。因此，如果新状态的概率，比如说

![](img/1d21da70a15c3e74cba13026c6d7e07d.png)

，只取决于当前状态，

![](img/b63459f73a33b6e472ec31cd9df400c1.png)

，使得当前状态捕获并记住来自过去的属性和知识。因此，根据马尔可夫性质，世界(即环境)被认为是静止的，即世界中的规则是固定的。

# S 状态集

**S 状态集**是不同状态的集合，表示为 **s** ，构成环境。状态是从环境中获得的数据的特征表示。因此，来自代理传感器的任何输入都可以在状态形成中发挥重要作用。状态空间可以是离散的，也可以是连续的。从起始状态开始，必须以最佳路径到达目标状态，而不会以不良状态结束(如下图所示的红色状态)。

假设下面的 gridworld 有 12 个离散的状态，其中绿色网格是目标状态，红色是要避免的状态，黑色是一堵墙，如果你迎面撞上它，你会从墙上反弹回来:

![](img/a144bc80c4852ef34bebf7f6e6464ce6.png)

这些状态可以表示为 1，2，…..，12 或通过坐标，(1，1)，(1，2)，…..(3,4).

# 行动

**动作**是代理在特定状态下可以执行的事情。换句话说，动作是代理在给定环境中被允许做的一系列事情。像状态一样，动作也可以是离散的或连续的。

考虑以下 gridworld 示例，该示例具有 12 个离散状态和 4 个离散动作(**向上**、**向下、向右**和向左**):**

![](img/362dbbfcfaa2e4ed6f3a7db5c97db693.png)

前面的例子显示了动作空间是一个离散集合空间，即 *a*

![](img/1e1c1490815a8d29c7d0ec75325ecbcb.png)

*A* 其中，*A = {上下左右}* 。它也可以被视为状态的函数，即 *a = A(s)* ，其中取决于状态函数，它决定哪个动作是可能的。

# 过渡模型

转换模型 *T(s，a，s’)*是三个变量的函数，它们是当前状态( *s* )、动作( *a* )和新状态(*s’*)，并且定义了在环境中玩游戏的规则。它给出了概率 *P(s'|s，a)* ，即假定代理人在给定状态 *s* 采取行动 *a* ，到达新的 *s'* 状态的概率。

过渡模型在随机世界中起着至关重要的作用，这与确定性世界的情况不同，在确定性世界中，除了确定状态之外，任何着陆状态的概率都为零。

让我们考虑以下环境(世界),并考虑不同的情况，确定的和随机的:

![](img/91cf0e80e070d3457719c728798bddf4.png)

自动作*一*

![](img/2ad570e29cc6b2831fdfd92ca9ddebf0.png)

*A* 其中，*A = {上下左右}* 。

这两种情况的行为取决于某些因素:

*   **确定的环境**:在确定的环境中，如果你采取某个动作，比如说*向上*，你一定会以概率 1 执行那个动作。
*   **随机环境**:在随机环境中，如果你采取同样的动作，比如说*向上*，有一定的概率比如说 0.8 实际执行给定的动作，有 0.1 的概率它可以执行一个垂直于给定动作的动作(或者*向左*或者*向右*)，*向上*。这里对于 s 状态和 *UP* 动作跃迁模型， *T(s '，UP，s) = P(s'| s，UP) = 0.8* 。

因为 *T(s，a，s') ~ P(s'|s，a)* ，这里新状态的概率只取决于当前状态和动作，而不取决于过去的状态。因此，转移模型遵循一阶马尔可夫性质。

我们也可以说，我们的宇宙也是一个随机环境，因为宇宙是由原子组成的，这些原子处于由位置和速度定义的不同状态。每个原子执行的动作改变了它们的状态，并引起宇宙的变化。

# 奖励

状态的**奖励**量化了进入状态的有用性。有三种不同的形式来表示奖励，即， *R(s)* ， *R(s，a)* 和 *R(s，a，s’)*，但它们都是等价的。

对于特定的环境，领域知识在不同状态的奖励分配中起着重要的作用，因为奖励的微小变化对于找到 MDP 问题的最优解很重要。

在采取某项行动时，我们会奖励我们的代理人两种方法。它们是:

*   **功劳分配问题**:我们回顾过去，检查哪些行为导致了现在的奖励，也就是说，哪个行为得到了功劳
*   相比之下，在目前的状态下，我们会检查采取哪种行动会给我们带来潜在的回报

延迟奖励形成了远见计划的想法。因此，这个概念被用来计算不同状态的预期回报。我们将在后面的章节中讨论这一点。

# 政策

到目前为止，我们已经讨论了产生 MDP 问题的模块，即状态、动作、转换模型和奖励，现在来看解决方案。这项政策是解决 MDP 问题的办法。

![](img/14b632dc1182b51ed0911ccf4fdc8835.png)

策略是将状态作为输入并输出要采取的动作的函数。因此，策略是代理必须遵守的命令。

![](img/7b73c6d19d5db7450f7cf34a2c469990.png)

被称为最优策略，它使期望报酬最大化。在采取的所有政策中，最优政策是优化以最大化一生中获得或预期获得的奖励金额的政策。对于 MDP 人来说，生命没有尽头，你必须决定结束的时间。

因此，政策只不过是告诉给定州采取何种行动的指南。它不是一个计划，而是通过返回每个状态要采取的行动来揭示环境的底层计划。

# 贝尔曼方程

自优

![](img/2dce46e7d003b83a7da76f7da2bf81f6.png)

政策是使预期回报最大化的政策，因此，

![](img/010e3fa8baaedf3db160fabe75d05b33.png)

,

在哪里

![](img/c244026fc9430c8a6aff7f730e0f8c1f.png)

指从代理人观察的状态序列中获得的预期回报值，如果它遵循

![](img/4d0dddbe91762f5250d8e39a9bc16563.png)

政策。因此，

![](img/f38fb812adf350094e8bf98ba825997e.png)

输出

![](img/5e5ff4076a03a3950899e4be2fe86b93.png)

具有最高预期回报的政策。

同样，我们也可以计算一个状态的政策的**效用，即如果我们处于*的*状态，给定一个**

![](img/8b1af6090aeaf44666e0ac70517d36a5.png)

政策，然后，的效用

![](img/8f6280b00d0ca4a8c20660cf42ab6135.png)

对于 *s* 州的政策，也就是，

![](img/f81dde7177f34b77f26c6998eec7cd4f.png)

会是从那个州开始的预期回报:

![](img/04ea13ff36183b583ee4c36e367deee8.png)

国家的直接奖励，也就是说，

![](img/d20351341e76c6d6a477df289c9cda07.png)

不同于

![](img/013cbc1133e623ef69bdd2f33276315e.png)

状态(即最优策略的效用)

![](img/9877672876265d3c5e8949075a0699f1.png)

state)因为延迟奖励的概念。从现在开始，使用

![](img/b9ae0290893b9d7d0d3e9bd80a7d59f1.png)

国家将指的是国家的最优政策的效用，即

![](img/d4cc53ca1b566d6d82ef9320bff6af9f.png)

状态。

而且，最优策略也可以看作是使期望效用最大化的策略。因此，

![](img/109c2055986c02953fac8163b39363de.png)

其中， *T(s，a，s')* 为转移概率，即 *P(s'|s，a)* ， *U(s')* 为 *a* 动作作用于 *s* 状态后新落地状态的效用。

![](img/f7e3f410325fe8434f991615762139e9.png)

指的是所采取的特定动作的所有可能的新状态结果的总和，则无论哪一个动作给出最大值

![](img/b0b8bad81ea6142a3fa75427cdaee46c.png)

这被认为是最优策略的一部分，因此,“s”状态的效用由下面的**贝尔曼方程**给出，

![](img/6573bf4ab7057d10ba6b854c4f7a0767.png)

在哪里，

![](img/b2f73164e6ac9ac9b3e3aed6f7efbaa0.png)

是直接的回报

![](img/4fd10c03bd7ef683fa753ba8b456e2b5.png)

是来自未来的回报，即如果采取行动 a，代理人可以从给定的 s 状态到达的 s 状态的贴现效用。

# 求解贝尔曼方程寻找政策

假设我们在给定的环境中有 n 个状态，如果我们看到贝尔曼方程，

![](img/3a7f280cfe53c94fa0b79affa9b5b281.png)

我们发现给定了 n 个 T21 状态；因此，我们将有 *n 个*方程和 *n 个*未知但

![](img/8eda34f6b38b920518dbc914373867f6.png)

函数使它成为非线性的。因此，我们不能将它们作为线性方程来求解。

因此，为了解决:

*   从任意的实用程序开始
*   基于邻域更新效用直到收敛，即，基于来自给定状态的着陆状态的效用，使用贝尔曼方程更新状态的效用

重复多次，得出状态的真实值。这个迭代收敛到状态真值的过程叫做**值迭代**。

对于游戏结束的终端状态，这些终端状态的效用等于代理人在进入终端状态时收到的即时奖励。

让我们通过实现一个例子来理解这一点。

## 利用贝尔曼方程进行数值迭代的一个例子

考虑以下环境和给定信息:

![](img/43be086e4bb5dc02ec949a1eda38f85a.png)

给定信息:

*   *A* 、 *C* 和 *X* 是一些州的名称。
*   绿色的状态是目标状态， *G* ，奖励+1。
*   红色的状态是不好的状态， *B* ，奖励-1，尽量阻止你的代理进入这个状态
*   因此，绿色和红色状态是终端状态，输入任何一个，游戏就结束了。如果代理人遇到绿色状态，即目标状态，代理人获胜，而如果他们进入红色状态，则代理人输掉游戏。

![](img/384fd85b77f47a4566ef83fa27520838.png)

*   ,

![](img/0198284212452779e9bd69230c6f65e7.png)

*   (即除了 *G* 和 *B* 状态之外的所有状态的奖励为-0.04)，

![](img/1841f36ec12ac65f362bd30050151622.png)

*   (即第一时间步的效用为 0，除了 *G* 和 *B* 状态)。
*   如果朝着期望的方向前进，转移概率 *T(s，a，s’)*等于 0.8；否则，如果垂直于所需方向，则每个值为 0.1。例如，如果动作是*向上*，那么代理以 0.8 的概率向上*移动*，但是以 0.1 的概率向右*移动*并向左*移动 0.1*。

问题:

1.  发现

![](img/03208b54abb6a92cd6419570a8a09fa9.png)

1.  ，时间步骤 1 的 *X* 状态的效用，即代理将经历一次迭代
2.  同样，找到

![](img/966267e274b23e27ab71b817b76c5223.png)

解决方案:

![](img/e8722ac565c7640bf7b7a34fe0f38d5b.png)![](img/efbff26d59cfedcc728fec814c4e6188.png)

*R(X) = -0.04*

**动作同**

![](img/72a8cc1dc5050d390c94371f0f68b658.png)![](img/2ad56885034249e64a2a17463e64c1de.png)![](img/a8e61ed7895116016b12b08c119c2ce6.png)

正确

G

0.8+10.8 x 1 = 0.8 右 0.100.1 x 0 = 0 右 0.100.1 x 0 = 0

因此，对于动作 *a =右*，

![](img/cd8849348ec7d0a6b298d8f808e80ab6.png)

**动作同**

![](img/4290c31a1b6e562645e5386e63e18bbc.png)![](img/2c448975af185cdff8152579ff61ef31.png)![](img/11110b8e192d6c6dccb4489cda75aebe.png)

向下

C

0 . 800 . 8 x 0 = 0 下降 0 . 1+10.1 x 1 = 0.1 下降 0.100.1 x 0 = 0

因此，对于动作 *a =向下*，

![](img/ab0c5718c45fe457fc7759eaf638ead2.png)

**动作同**

![](img/08ab5d60345f65de82feece398865599.png)![](img/6831d48d7b7c460e491c91262f829f24.png)![](img/3f7b8a48bcf5e0f455034da46261075c.png)

向上

X

0.800.8 x 0 = 0 upg 0.1+10.1 x 1 = 0.1 upa 0.100.1 x 0 = 0

因此，对于动作 *a =上升*，

![](img/849d5cc8837180bc0e87d6e0c918a7f3.png)

**动作同**

![](img/817ce9a8425f33c0b376b8ba5d1bbacd.png)![](img/253b70e66145e3446d851e55f02fb759.png)![](img/1f34fb8ada12172ce7e378db92f5bd1c.png)

左边的

A

0.800.8 x 0 = 0 leftx 0.100.1 x 0 = 0 leftc 0.100.1 x 0 = 0

因此，对于动作 *a =左*，

![](img/36f52fd265f0dbd793b407e51782736a.png)

因此，在所有的行动中，

![](img/0ecdf865594e9925fbd7d0a821533d6d.png)

因此，

![](img/23118e1daefe81c47b62dcd4b7852e31.png)

，在哪里

![](img/a5efa719125b40434784819cc0be3c10.png)

和

![](img/6722d95ede34a9f3240f6b36d0c143c8.png)

同样，计算

![](img/4e13b85519bed0602c0024d0a8fff621.png)

和

![](img/90e1da765541c9b2ff74f0dac384498a.png)

我们得到了

![](img/e60d83cd5222bf5c4799017b001eb815.png)

和

![](img/336108379929de33fa54bea2bcca65d4.png)

因为，

![](img/4bcbc40967c14e6792b5ee496f46cdd8.png)

，而且，

![](img/72d0b453a27a366aafd9eee791c40e98.png)

*R(X) = -0.04*

**动作同**

![](img/6ac88ad4bd3b1c0eca1fed5d0a3fab5f.png)![](img/16870d2dda15305df6eba9728bc49c86.png)![](img/9b84d9282e59be1ee46579c5bfcb6136.png)

正确

G

0.8+10.8 x 1 = 0.8 右 0.1–0.040.1 x-0.04 =-0.004 右 X0.10.360.1 x 0.36 = 0.036

因此，对于动作 *a =右*，

![](img/22ef2870d1df023b724edf80af901c89.png)

**动作同**

![](img/0416e6e951d7b89ed139c85752a2d263.png)![](img/9be56d9a7a66c44505e00f53d04bf952.png)![](img/64e70534d5d3ded04e26b67fc6a80175.png)

向下

C

0.8–0.040 . 8 x-0.04 =-0.032 下降 0.1+10.1 x 1 = 0.1 下降 0.1–0.040 . 1 x-0.04 =-0.004

因此，对于动作 *a =向下*，

![](img/692d986a426e68380e932277e5b633b4.png)

**动作同**

![](img/9abc6065ea3b1d4a2abbae003fc5d6a2.png)![](img/77ddef7bd00cdf7714cb94aae559b3ac.png)![](img/3bc8fd082b70e0e821607b2e4b8231df.png)

向上

X

0.80 . 360.8 x 0.36 = 0.288 upg 0.1+10.1 x 1 = 0.1 upa 0.1–0.040.1 x-0.04 =-0.004

因此，对于动作 *a =上升*，

![](img/2eba4737191cf49c509cc40d955add4b.png)

**动作同**

![](img/ef2fdd10614d4fc6fe597e0206a5ed97.png)![](img/3cb767b50b7fb62a4c1b6f684630761c.png)![](img/0d6836a1c4abd5246b46f378800a5ab4.png)

左边的

A

0.8–0.040.8 x-0.04 =-0.032 leftx 0.10 . 360.1 x 0.36 = 0.036 leftc 0.1–0.040.1 x-0.04 =-0.004

因此，对于动作 *a =左*，

![](img/52aad3051c92441c87b3c4e1f6462953.png)

因此，在所有的行动中，

![](img/1c03da26a56112b40278ebd697bf8fd4.png)

因此，

![](img/fe181b8a7fdc9ce35e08d8985cae9135.png)

，在哪里

![](img/d65eae4669ae7e61fbdf8512d83fe2e7.png)

和

![](img/68bc7dac5f98eee497bbae7234547af9.png)

因此，对上述问题的回答是:

![](img/11d87440a01da9974bcaca246b14aa1f.png)![](img/23be307f20943762c750e8b87025b40f.png)

## 策略迭代

通过迭代策略并更新策略本身而不是值直到策略收敛到最优来获得最优效用的过程被称为**策略迭代**。策略迭代的过程如下:

*   从随机策略开始，

![](img/6150ecd92ee19c9205b1336ee9eb11b2.png)

*   给定的

![](img/bf7613401d77648134d94787ed576189.png)

*   迭代步骤 *t* 的策略，计算

![](img/001dd9d5437098777a6742e9e7bfd78c.png)

*   通过使用以下公式:

![](img/f78bb3d2c7ed4a097aeb3ffde9d63f17.png)

*   提高

![](img/d08431cae958788dc6ec03d323121118.png)

*   政策制定者

![](img/d7d1fd44ec997f9f060e0e47904079fe.png)

这就结束了一个有趣的强化学习教程。想要从零开始实现最先进的强化学习算法吗？得到这个最畅销的标题，[用 TensorFlow](https://www.packtpub.com/big-data-and-business-intelligence/reinforcement-learning-tensorflow) 强化学习。

# 接下来阅读:

[*强化学习如何工作*](https://hub.packtpub.com/reinforcement-learning-works/)

[*带强化学习的卷积神经网络*](https://hub.packtpub.com/convolutional-neural-networks-reinforcement-learning/)

[*使用 TensorFlow 开始 Q-learning*](https://hub.packtpub.com/getting-started-with-q-learning-using-tensorflow/)