<html>
<head>
<title>Application of ML: Building indices?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML的应用:建立指数？</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/building-efficient-learned-indices-using-machine-learning-96890c0fa948?source=collection_archive---------4-----------------------#2018-09-03">https://medium.com/coinmonks/building-efficient-learned-indices-using-machine-learning-96890c0fa948?source=collection_archive---------4-----------------------#2018-09-03</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><div class=""><h2 id="8f67" class="pw-subtitle-paragraph iq hs ht bd b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ek translated">介绍</h2></div><p id="3f95" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">在本帖中，我们将介绍Google的一篇探索性论文，该论文提出了一种非常新颖的方法,通过将机器学习技术应用于底层数据来构建更加时空高效的数据结构，例如B树索引。传统上，诸如B树或散列图之类的索引是用静态试探法和假设构建的。静态试探法不考虑底层数据分布。如果知道键的范围是从1到100M，记录的大小是固定的，那么B树可能不是最佳的数据结构。在这种情况下，只使用键本身作为偏移量的索引会更好。显然，这个例子很简单，但是很有启发性。总是不可能先验地手动理解所有的数据分布，并构建那些可以对所有类型的数据表现良好的优化索引。</p><p id="b601" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">该论文提出了一个有趣的问题，即如果数据结构实际上是通过学习数据分布而不是静态地预先构建的，是否会有类似的改进。本文关注的是大量读取的工作负载，并不提倡现在就替换这些数据结构，而是致力于理解描述底层数据的连续函数是否可能产生更高效的数据结构，特别是对于大规模场景。</p><h2 id="a192" class="kf kg ht bd kh ki kj kk kl km kn ko kp jr kq kr ks jv kt ku kv jz kw kx ky kz dt translated">想法的直觉——指数是模型吗？</h2><p id="404e" class="pw-post-body-paragraph ji jj ht jk b jl la iu jn jo lb ix jq jr lc jt ju jv ld jx jy jz le kb kc kd hm dt translated">通常，每当需要快速查找时，在计算机科学中就使用索引。在数据库中，对于基于范围的查询，使用B树。对于点查询，使用散列映射。对于集合成员，使用布隆过滤器。</p><p id="17e5" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">如果我们暂时关注B树，B树遍历可以被认为是一个预测可能有您正在寻找的键的磁盘页面的模型。所以树的最初遍历(从根到叶)是预测(确定性地)排序数组中的起始位置。从这个起始位置开始的顺序搜索最大限度地减少了找到实际键的时间。(见下图“a”)。如果机器学习模型可以学习关键字的整体模式，那么它可以类似地预测关键字在排序数组中的位置(参见下面的图“b”)。类似地，布隆过滤器可以被认为是一个分类器，如果找到该键，则预测1，如果该键不存在，则预测0。</p><figure class="lg lh li lj fq lk fe ff paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="fe ff lf"><img src="../Images/4803c821599a8099710724c6e70b1834.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jfpb-9XlJwkj0wSLNxP3Kw.png"/></div></div><figcaption class="lr ls fg fe ff lt lu bd b be z ek">B-tree predicts the page in which the key can be present. For a learned index, similarly a model can predict the page where key might be present within some error bounds.</figcaption></figure><p id="afcf" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">Jeff Dean在<a class="ae ke" href="https://youtu.be/Nj6uxDki6-0" rel="noopener ugc nofollow" target="_blank"> SysML talk </a>中概述的另一个趋势，可能会使基于ML的数据结构更加引人注目，那就是摩尔定律正在趋于稳定。因此，可以使用SIMD或GPU等矢量化指令的工作负载可以从硬件中提取更多信息。请看显示单核性能趋于平稳的蓝线。</p><figure class="lg lh li lj fq lk fe ff paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="fe ff lv"><img src="../Images/078ee206db816d2021c1427a9bff92e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RH1LYwHxkz2pZ33vxKAh2Q.png"/></div></div><figcaption class="lr ls fg fe ff lt lu bd b be z ek"><a class="ae ke" href="https://www.karlrupp.net/wp-content/uploads/2015/06/40-years-processor-trend.png" rel="noopener ugc nofollow" target="_blank">https://www.karlrupp.net/wp-content/uploads/2015/06/40-years-processor-trend.png</a></figcaption></figure><h2 id="b640" class="kf kg ht bd kh ki kj kk kl km kn ko kp jr kq kr ks jv kt ku kv jz kw kx ky kz dt translated">范围指数(B树)模型和CDF</h2><p id="90d0" class="pw-post-body-paragraph ji jj ht jk b jl la iu jn jo lb ix jq jr lc jt ju jv ld jx jy jz le kb kc kd hm dt translated">类似b树的索引有利于查询一系列数据。这种索引是建立在排序数据集之上的。已排序的数据被分成页面。由于空间限制，每个页面在索引中都有一个键。所以B树将一个键映射到磁盘上的一个页面。因此，可以将它视为最小误差为零(页面中的第一个关键字)和最大误差为页面大小(页面中的最后一个关键字)的回归树。因此，只要误差在相同的限度内，人们可以潜在地将这些指数构建为ML模型，如回归和神经网络。</p><p id="b889" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">基于机器学习的模型可以通过使用对一个关键字的预测并记录该预测偏离了多少来训练。只要这个误差在相同的范围内，这个模型在B树提供的语义保证上是等价的。此外，对于B树，由于数据是排序的，所以可以进行局部搜索来快速纠正错误。对于键的插入，B树需要被平衡，类似地，ML模型需要被重新训练。由于神经网络擅长学习各种各样的数据分布，它们可以用于为各种数据访问模式建立有效的数据结构。挑战在于平衡模型的复杂性与准确性和语义保证。本文提供了一些关于B树相对于ML模型的复杂度的计算。借助大多数硬件中可用的SIMD指令集或不断增长的GPU，ML模型可以运行得更好。</p><p id="3cce" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">假设我们暂时关心固定大小记录的简单密集内存中索引。这些在很多系统中都很常见。此外，本文中的大部分研究都是针对整数/实数值，而不是字符串。字符串建模需要进一步研究。</p><p id="e4f8" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated"><strong class="jk hu">作为CDF模型的范围指数模型:</strong></p><p id="a805" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">累积密度函数学习数据分布。给定一个数据集，它计算概率，如果你选择一个随机阈值，那么数据集中任何数字小于或等于该阈值的概率是多少。假设我们记录了100个人的体重，如果我们为其建模一个CDF F(X)。现在，如果我们选择250磅作为阈值，那么F(250)将给出多少人的体重小于或等于250磅的概率。在这种情况下，概率可能是95%左右。</p><p id="83e7" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">如果我们把这个想法推广到范围索引，我们要寻找的是，给定一个键，这个键在数据集中的位置是什么。在排序的数据集中，它转换为:给定一个键，这个键大于或等于数据集中的键的概率是多少。例如，如果我们为从10到1M的键访问建模一个CDF，然后在这个CDF索引中寻找键999的位置。然后选择关键字999作为阈值，CDF将给出数据集中任何关键字小于或等于999的概率。在这种情况下，可能是10%，甚至更小。因此位置估计函数可以变成:p = F(key) * N其中N是键的数量。所以这个CDF预测，对于一个排序的数组，键位于数据集的10%。然后一次可以往后退一点或者往前一点找到实际位置。出于训练目的，回归模型可以通过最小化最小平方误差来学习数据分布。</p><h2 id="ca24" class="kf kg ht bd kh ki kj kk kl km kn ko kp jr kq kr ks jv kt ku kv jz kw kx ky kz dt translated">一个天真的指数:</h2><p id="bd2c" class="pw-post-body-paragraph ji jj ht jk b jl la iu jn jo lb ix jq jr lc jt ju jv ld jx jy jz le kb kc kd hm dt translated">作者最初建立了一个简单的索引，但结果证明比B树昂贵得多。由此得出的主要结论是:神经网络或回归模型可以在高粒度上很好地概括。但是当你放大时，更多的不规则现象出现了。看下面的例子，这个函数看起来非常平滑，但是当你放大到“最后一英里”时，这个模式非常分散。</p><figure class="lg lh li lj fq lk fe ff paragraph-image"><div class="fe ff lw"><img src="../Images/2bbeabf3cff072b0663eba55f332b8f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*Tr_mg8Ijmzsu5120i-I3vg.png"/></div><figcaption class="lr ls fg fe ff lt lu bd b be z ek">Smooth CDF function, but wide variety at a finer granularity</figcaption></figure><h2 id="e09d" class="kf kg ht bd kh ki kj kk kl km kn ko kp jr kq kr ks jv kt ku kv jz kw kx ky kz dt translated">一个实验性的索引生成框架</h2><p id="2a5d" class="pw-post-body-paragraph ji jj ht jk b jl la iu jn jo lb ix jq jr lc jt ju jv ld jx jy jz le kb kc kd hm dt translated">为了优化上述瓶颈，本文构建了一个新的学习索引框架(LIF ),该框架可以根据给定的配置生成索引代码。它自己建立回归模型，而神经网络使用Tensorflow。然后，它提取神经网络生成的权重，并建立一个有效的C++代码，可用作索引。</p><p id="1fa8" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">为了解决上面提到的最后一英里准确性问题，该框架构建了一个模型层次结构— <strong class="jk hu">递归模型索引(RMI) </strong>。阶段<em class="lx"> l </em>，有M <em class="lx"> l </em>款。每个阶段都预测了键的某个位置。当您在层次结构中向下移动时，模型会使用上一阶段模型的先前位置预测来预测较新的位置。所以每一个阶段都在迭代改进上一个阶段所做的预测。例如，对于以百万计的键的数量，阶段1可以以一万的精度预测键的位置。下一阶段将研究这些10K，然后将范围缩小到100个，以此类推。递归的终止条件可以是针对某个均方误差训练的朴素模型。</p><figure class="lg lh li lj fq lk fe ff paragraph-image"><div class="fe ff ly"><img src="../Images/4afa7c5321fd07aae991f94f2d32ede2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*Td9zURziwHxp2h2_yNXc3A.png"/></div><figcaption class="lr ls fg fe ff lt lu bd b be z ek">Each stage consists of multiple models and they are narrowing down the range of positions for the given key by using a model in the next stage. Note that in this scheme there can be overlapping ranges Model 2.2 refers to 3.2 and 3.3. both. While 2.1 also refers to Model 3.2</figcaption></figure><p id="1baf" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">这种方法的另一个优点是，在层次结构的顶部可以有复杂的神经网络，而在底部是简单的线性回归。这可以在空间-时间之间实现更优的折衷。在这样的混合模型中，如果误差太大，那么几乎所有的中间模型都可以变成B树。这种选择可以通过这些模型的误差测量来实现。</p><p id="5fea" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">论文中的一些实验结果相当惊人。在某些情况下，B树需要50MB来存储，而学习的索引只需要0.15MB。此外，它们比B树的查找速度快2.7倍。实验使用两级RMI，在第二级使用10k、50K模型。</p><h2 id="7075" class="kf kg ht bd kh ki kj kk kl km kn ko kp jr kq kr ks jv kt ku kv jz kw kx ky kz dt translated">点索引—哈希映射</h2><p id="acd8" class="pw-post-body-paragraph ji jj ht jk b jl la iu jn jo lb ix jq jr lc jt ju jv ld jx jy jz le kb kc kd hm dt translated">传统的哈希映射是使用哈希函数实现的。根据哈希函数的效率，哈希映射中可能会有冲突。冲突的数量对存储和查找的哈希映射的性能有很大影响。在散列映射中，键不是按排序顺序存储的，键只是被映射到某个位置。</p><p id="0518" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">该论文提出通过学习CDF来构建散列函数。然后，可以缩放CDF以获得给定键的散列。假设数据集中有N个关键字，哈希映射的预期大小是M，其中M&lt; N. Then a potential hash function could be: <strong class="jk hu"><em class="lx">H(key)= F(key)* M .</em></strong>如果学习模型完美地预测了CDF，那么就不会有冲突。</p><figure class="lg lh li lj fq lk fe ff paragraph-image"><div class="fe ff lz"><img src="../Images/487c63d8d6ade5dd09de0146227d4604.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*juYcPpXFd1c0NDE2v4f_jA.png"/></div><figcaption class="lr ls fg fe ff lt lu bd b be z ek">Traditional hash map with lots of free slots and lots of collisions. The learned hash map with better utilization of the hash map and fewer collisions.</figcaption></figure><p id="baab" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">考虑一个关键字为10，15，15，17，20的数据集。每个键的CDFs分别为1/5、3/5、4/5、5/5。如果我们想要一个大小为3的散列图，那么H(15)将是3/5* 3。其他的也可以类似计算。如您所见，哈希映射实现与此是分离的。可以使用探测或链接，具体取决于哪种方式有效。这种机制只提供了一种学习哈希函数的方法，可以最大限度地减少冲突并提高表的整体利用率。</p><p id="eda9" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">这一方案可能并不总是奏效。如果数据集遵循均匀分布，那么数据集中的所有键具有相等的概率。因此，可以通过简单的线性回归很容易地对学习到的函数进行建模，因为对于给定的数据，均匀分布的CDF是一条直线。在这种情况下，上面提到的散列函数不是很有优势——它将与任何常用的散列函数执行相同的操作。</p><p id="c0e2" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">在实现方面，上面提到的RMI可以再次用于生成散列函数。对于实验，作者在第二阶段使用100K模型的2阶段RMI。大规模密钥空间的一个大问题是哈希映射的内存限制。虽然基于模型的方法速度稍慢，但内存节省是巨大的——在20%到60%的范围内。</p><p id="9be5" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">总的来说，结论似乎是，根据数据集和场景，学习哈希函数可能值得考虑。</p><h2 id="d1a4" class="kf kg ht bd kh ki kj kk kl km kn ko kp jr kq kr ks jv kt ku kv jz kw kx ky kz dt translated">存在指数或集合成员资格—布隆过滤器</h2><p id="ae45" class="pw-post-body-paragraph ji jj ht jk b jl la iu jn jo lb ix jq jr lc jt ju jv ld jx jy jz le kb kc kd hm dt translated">布隆过滤器传统上用于使用存储器优化结构来检查集合成员资格。布隆过滤器保证他们将有零假阴性——如果一个键不在布隆过滤器中，那么它将以100%的成功率进行预测。如果一个键在布隆过滤器中，那么它可能有误报，需要进一步检查。</p><p id="2e0e" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated"><strong class="jk hu">分类问题:</strong>可以把Bloom filters看成分类问题。对于数据集中的所有键，模型预测为1，而对于所有非键，模型预测为0。一个非常简单的例子是，如果我们的集合由从0到N的所有整数组成，那么f(x) = 0 ≤ x ≤ N可以非常有效地为所有键返回1。对于更复杂的数据，可以潜在地训练具有sigmoid激活函数的RNN，因为这是二元分类。此外，sigmoid中使用的f(x)可以用作概率指标，我们可以设置一些阈值。任何高于阈值的值都表示存在于数据集中。因此，我们的数据集中不会考虑任何低于阈值的数据。因此，可以为这些关键字创建另一个溢出布隆过滤器，以实施上述的假阴性标准。需要注意的是，这种机制需要对非键有所了解。</p><p id="1a43" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated"><strong class="jk hu">学习bloom filter的哈希函数:</strong>这里有一个简洁的想法，我们需要的bloom filter哈希是键之间的大量冲突和非键之间的大量冲突。但是键和非键之间的冲突很少。这可以用上面提到的神经网络模型来完成。将M视为位集的大小。如果我们使用上一节中提到的相同的f(x ),并且使用f(x) * M作为散列函数，那么较高概率的f(x ),即关键字将映射到位集中的较高位置，而较低概率的非关键字映射到位集中的较低位置。</p><p id="5c4b" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">从本文所做的实验来看，当使用基于模型的方法时，bloom filter的大小似乎减少了大约15%。</p><h2 id="1481" class="kf kg ht bd kh ki kj kk kl km kn ko kp jr kq kr ks jv kt ku kv jz kw kx ky kz dt translated">结论</h2><p id="5574" class="pw-post-body-paragraph ji jj ht jk b jl la iu jn jo lb ix jq jr lc jt ju jv ld jx jy jz le kb kc kd hm dt translated">我认为这是一篇非常酷的论文，概述了一种使用ML技术构建数据结构的非常新颖的方法。在它们变得司空见惯之前，肯定还需要发生更多的事情，但是这是一个非常令人兴奋的ML技术的应用。</p><blockquote class="ma"><p id="0736" class="mb mc ht bd md me mf mg mh mi mj kd ek translated"><a class="ae ke" href="https://coincodecap.com/?utm_source=coinmonks" rel="noopener ugc nofollow" target="_blank">在您的收件箱中直接获得最佳软件交易</a></p></blockquote><figure class="ml mm mn mo mp lk fe ff paragraph-image"><a href="https://coincodecap.com/?utm_source=coinmonks"><div class="fe ff mk"><img src="../Images/7c0b3dfdcbfea594cc0ae7d4f9bf6fcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/0*OJ-qb5G6i863msBB.png"/></div></a></figure></div></div>    
</body>
</html>