<html>
<head>
<title>Review: SPPNet —1st Runner Up (Object Detection), 2nd Runner Up (Image Classification) in ILSVRC 2014</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾:sp pnet—2014年ILSVRC亚军(目标检测)，季军(图像分类)</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/review-sppnet-1st-runner-up-object-detection-2nd-runner-up-image-classification-in-ilsvrc-906da3753679?source=collection_archive---------1-----------------------#2018-09-01">https://medium.com/coinmonks/review-sppnet-1st-runner-up-object-detection-2nd-runner-up-image-classification-in-ilsvrc-906da3753679?source=collection_archive---------1-----------------------#2018-09-01</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><p id="b7c7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt jo translated"><span class="l jp jq jr bm js jt ju jv jw di">在</span>这个故事中，SPPNet被回顾。SPPNet在CNN引入了一种新技术，称为<strong class="is hu">空间金字塔池(SPP) </strong>，位于卷积层和全连接层的过渡处。这是来自<strong class="is hu">微软</strong>的作品。</p><p id="bc46" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">在<strong class="is hu"> ILSVRC 2014 </strong>中，SPPNet在目标检测中获得<strong class="is hu">亚军，在图像分类</strong>中获得<strong class="is hu">亚军，在定位任务</strong>中获得<strong class="is hu">第五名。在<strong class="is hu"> 2014年ECCV【1】和2015年TPAMI【2】</strong>分别有2篇论文<strong class="is hu">被引用超过1000次和600次</strong>。由此可见，SPPNet是值得一读的深度学习论文之一。(<a class="jx jy gr" href="https://medium.com/u/aff72a0c1243?source=post_page-----906da3753679--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</strong></p></div><div class="ab cl jz ka hb kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hm hn ho hp hq"><h1 id="bc08" class="kg kh ht bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated">资料组</h1><p id="d063" class="pw-post-body-paragraph iq ir ht is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hm dt translated"><strong class="is hu">分类</strong>:超过1500万张带标签的高分辨率图像，约22000个类别。ILSVRC使用ImageNet的一个子集，在<strong class="is hu"> 1000个类别</strong>的每个类别中包含大约1000幅图像。总共大约有<strong class="is hu"> 1.3M/50k/100k图像</strong>用于<strong class="is hu">训练/验证/测试集</strong></p><p id="c026" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">检测</strong> : <strong class="is hu"> 200类</strong>。<strong class="is hu"> 450k/20k/40k </strong>图像用于<strong class="is hu">训练/验证/测试集</strong></p></div><div class="ab cl jz ka hb kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hm hn ho hp hq"><h1 id="e1cf" class="kg kh ht bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated">涵盖哪些内容</h1><ol class=""><li id="e449" class="lj lk ht is b it le ix lf jb ll jf lm jj ln jn lo lp lq lr dt translated"><strong class="is hu">空间金字塔池</strong></li><li id="6ff2" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated"><strong class="is hu">多尺寸训练</strong></li><li id="3fd1" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated"><strong class="is hu">完整图像表示</strong></li><li id="9520" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated"><strong class="is hu">多视角测试</strong></li><li id="68eb" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated"><strong class="is hu">与最先进方法的比较(分类)</strong></li><li id="11a9" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated"><strong class="is hu">物体检测中的spp net</strong></li><li id="b345" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated"><strong class="is hu">与最先进方法的比较(检测)</strong></li></ol></div><div class="ab cl jz ka hb kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hm hn ho hp hq"><h1 id="8a6f" class="kg kh ht bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated"><strong class="ak"> 1。空间金字塔池(SPP) </strong></h1><figure class="ly lz ma mb fq mc fe ff paragraph-image"><div class="fe ff lx"><img src="../Images/1ff5c49a4bb00883b005825c6d0c6244.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*Af0rCJ67rVYdfIfhwnwi3A.png"/></div><figcaption class="mf mg fg fe ff mh mi bd b be z ek"><strong class="bd mj">Three-Level Spatial Pyramid Pooling (SPP) in SPPNet with Pyramid {4×4, 2×2, 1×1}.</strong></figcaption></figure><p id="612e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">传统上，在conv层和FC层的过渡处，只有一个池层或者甚至没有池层。在SPPNet中，建议使用<strong class="is hu">不同规模的多个池层</strong>。</p><p id="dead" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">上图中使用了<strong class="is hu"> 3级SPP </strong>。假设conv5层有256个特征地图。然后在SPP层，</p><ol class=""><li id="075b" class="lj lk ht is b it iu ix iy jb mk jf ml jj mm jn lo lp lq lr dt translated">首先，将每个特征图<strong class="is hu">汇集成一个值(灰色)</strong>，从而形成<strong class="is hu"> 256-d向量</strong>。</li><li id="37f3" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated">然后，将每个特征图<strong class="is hu">汇集成4个值(绿色)</strong>，形成一个<strong class="is hu"> 4×256-d矢量</strong>。</li><li id="9b3e" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated">类似地，每个特征图<strong class="is hu">汇集成16个值(蓝色)</strong>，形成一个<strong class="is hu"> 16×256维向量</strong>。</li><li id="c270" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated">上述3个矢量的<strong class="is hu">被连接起来形成一个一维矢量</strong>。</li><li id="f255" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated">最后，这个一维向量像往常一样进入FC层。</li></ol><p id="ff08" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">有了SPP，在进入CNN之前，我们不需要像AlexNet那样将图像裁剪成固定大小。<strong class="is hu">可以输入任何图像尺寸。</strong></p></div><div class="ab cl jz ka hb kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hm hn ho hp hq"><h1 id="5245" class="kg kh ht bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated"><strong class="ak"> 2。多尺寸训练</strong></h1><figure class="ly lz ma mb fq mc fe ff paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="fe ff mn"><img src="../Images/36756fb90cbb66ede51329a3f0717039.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xE-CtZ5FXixVEO89lAT46g.png"/></div></div><figcaption class="mf mg fg fe ff mh mi bd b be z ek"><strong class="bd mj">SPPNet supports any sizes due to the use of SPP</strong></figcaption></figure><p id="5f1d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">对于SPP，可变大小被接受作为输入，不同的大小应该被输入到网络中以增加网络在训练期间的鲁棒性。</p><p id="e80c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">然而，为了训练过程的有效性，<strong class="is hu">仅使用224×224和180×180图像作为输入。</strong>两个网络，180-网络和240-网络使用共享参数进行训练。</p><p id="9652" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">作者复制了ZFNet [3]、AlexNet [4]和Overfeat [5]，并做了如下修改(其后的数字是conv层数):</p><figure class="ly lz ma mb fq mc fe ff paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="fe ff ms"><img src="../Images/75aea1e88157f88decb0883e15197356.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5wdSWu7jtBQ9jqYsIVvZAg.png"/></div></div><figcaption class="mf mg fg fe ff mh mi bd b be z ek"><strong class="bd mj">Replicated Model as Baseline</strong></figcaption></figure><figure class="ly lz ma mb fq mc fe ff paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="fe ff mt"><img src="../Images/5fd588281d2574fa1e0ec1cc21a63bfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hDjV-dmgRVem_xlV4Mjq0A.png"/></div></div><figcaption class="mf mg fg fe ff mh mi bd b be z ek"><strong class="bd mj">Top-5 Error Rates for SPP and Multi-Size Training</strong></figcaption></figure><p id="642a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"> 4级SPPNet </strong>这里用的是金字塔<strong class="is hu"> {6×6，3×3，2×2，1×1}。</strong></p><p id="9fd2" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">如上所示，<strong class="is hu">仅使用SPP，所有型号的错误率均有所改善。通过多尺寸训练，错误率进一步提高。</strong>(10-视图表示从【四个角+ 1个中心】开始的10次裁剪测试以及相应的水平翻转)</p></div><div class="ab cl jz ka hb kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hm hn ho hp hq"><h1 id="bf32" class="kg kh ht bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated"><strong class="ak"> 3。完整图像表示</strong></h1><p id="a084" class="pw-post-body-paragraph iq ir ht is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hm dt translated">由于使用SPP也可以将完整图像输入CNN，作者<strong class="is hu">将完整图像输入</strong> <strong class="is hu">与仅使用1个中心裁剪输入</strong>进行比较:</p><figure class="ly lz ma mb fq mc fe ff paragraph-image"><div class="fe ff mu"><img src="../Images/86f509ac14823e8c048c18d2132835f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*Bih63DsdU4onGCeTT5sWiA.png"/></div><figcaption class="mf mg fg fe ff mh mi bd b be z ek"><strong class="bd mj">Top-1 Error Rates for Full Image Representation</strong></figcaption></figure><p id="b207" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">使用完整图像作为输入，Top-1错误率都得到改善。</p></div><div class="ab cl jz ka hb kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hm hn ho hp hq"><h1 id="e589" class="kg kh ht bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated"><strong class="ak"> 4。多视图测试</strong></h1><p id="c0b4" class="pw-post-body-paragraph iq ir ht is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hm dt translated">通过使用SPP提供完整的图像支持，可以轻松实现多视图测试。</p><ol class=""><li id="9131" class="lj lk ht is b it iu ix iy jb mk jf ml jj mm jn lo lp lq lr dt translated">作者将图像调整到6种比例:{224，256，300，360，448，560} </li><li id="b4cb" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated">对于每个比例，<strong class="is hu">生成18个视图:{1个中心，4个角，每边中间4个}和相应的翻转。</strong> <br/>这样，总共有<strong class="is hu">96个视图</strong>。</li><li id="fbe7" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated">还包括<strong class="is hu"> 2个全图视图和相应的翻转</strong>。</li></ol><figure class="ly lz ma mb fq mc fe ff paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="fe ff mv"><img src="../Images/3fd1ec6ced20851e77ac22d69daf2ff9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hHJ2YSmSu_cu3-CjKgM_kA.png"/></div></div><figcaption class="mf mg fg fe ff mh mi bd b be z ek"><strong class="bd mj">Error Rates in ILSVRC 2012 (All are Single Model Results)</strong></figcaption></figure><p id="bda0" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"> SPPNet使用OverFeat-7在验证/测试集上获得9.14/9.08% </strong>的前5名错误率，是<strong class="is hu">表中唯一一个低于10% </strong>的错误率。</p></div><div class="ab cl jz ka hb kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hm hn ho hp hq"><h1 id="b429" class="kg kh ht bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated"><strong class="ak"> 5。与最先进方法的比较(分类)</strong></h1><p id="9280" class="pw-post-body-paragraph iq ir ht is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hm dt translated"><strong class="is hu">测试中使用了11种型号的SPPNet。输出被平均以获得更精确的预测。这是一种在许多CNN模型中使用的增强或集合技术，如LeNet、AlexNet、ZFNet。</strong></p><figure class="ly lz ma mb fq mc fe ff paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="fe ff mw"><img src="../Images/ab429ede9336e4bf20e44e96c2758b35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sO7GfqM16cJ9MyKQZpjzJw.png"/></div></div><figcaption class="mf mg fg fe ff mh mi bd b be z ek"><strong class="bd mj">2nd Runner Up in Image Classification (ILSVRC 2014)</strong></figcaption></figure><p id="51ec" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">得到了8.06%的错误率。遗憾的是，VGGNet和GoogLeNet在使用深度模型的情况下性能更好。<strong class="is hu">最后，SPPNet在分类任务中只能获得亚军。</strong></p></div><div class="ab cl jz ka hb kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hm hn ho hp hq"><h1 id="81b9" class="kg kh ht bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated"><strong class="ak"> 6。物体检测中的spp net</strong></h1><ol class=""><li id="2f0e" class="lj lk ht is b it le ix lf jb ll jf lm jj ln jn lo lp lq lr dt translated"><strong class="is hu">选择性搜索[6]用于生成大约2k个区域提议(边界框)，就像R-CNN [7]中一样。</strong></li><li id="37e1" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated"><strong class="is hu">输入图像仅使用ZFNet通过SPPNet一次。</strong></li><li id="08d7" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated"><strong class="is hu">在最后一个conv层，由每个区域提议界定的特征地图将进入SPP层，然后进入FC层</strong>，如下所示:</li></ol><figure class="ly lz ma mb fq mc fe ff paragraph-image"><div class="fe ff mx"><img src="../Images/31b4132731bb5e9bde8298261f1c63aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*EMhHR_g4UWEYpxsVWdpKdA.png"/></div><figcaption class="mf mg fg fe ff mh mi bd b be z ek"><strong class="bd mj">SPPNet for Object Detection</strong></figcaption></figure><p id="4cf2" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">与R-CNN相比，<strong class="is hu"> SPPNet只对conv层的图像处理一次，而R-CNN对conv层的图像处理了2k次</strong>，因为有2k个区域提案。下图说明了这个想法:</p><figure class="ly lz ma mb fq mc fe ff paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="fe ff my"><img src="../Images/20095ffd682d35874ba62666e5177dd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n4LE9idyGJX_efOsS-FNvw.png"/></div></div><figcaption class="mf mg fg fe ff mh mi bd b be z ek"><strong class="bd mj">R-CNN (Left) and SPPet (Right)</strong></figcaption></figure><p id="5931" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">在每个包围盒的FC层之后，还需要SVM和包围盒回归器，这不是端到端的学习架构。</p></div><div class="ab cl jz ka hb kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hm hn ho hp hq"><h1 id="da04" class="kg kh ht bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated"><strong class="ak"> 7。与最先进方法的比较(检测)</strong></h1><h2 id="f652" class="mz kh ht bd ki na nb nc km nd ne nf kq jb ng nh ku jf ni nj ky jj nk nl lc nm dt translated">7.1 VOC 2007</h2><figure class="ly lz ma mb fq mc fe ff paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="fe ff nn"><img src="../Images/5cfd594a1752e729e07d2ed319a43519.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IKMq2G65wrxtF55-ICDVbQ.png"/></div></div><figcaption class="mf mg fg fe ff mh mi bd b be z ek"><strong class="bd mj">VOC 2007 Results</strong></figcaption></figure><figure class="ly lz ma mb fq mc fe ff paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="fe ff no"><img src="../Images/b863246dfee9fc53414c562ccccb5ab1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9RFCDM7OUWr-f3uJRY7Ifg.png"/></div></div><figcaption class="mf mg fg fe ff mh mi bd b be z ek"><strong class="bd mj">Some Amazing Results in VOC 2017</strong></figcaption></figure><p id="adb0" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">如上图VOC 2007中，<strong class="is hu">与R-CNN相比，5个尺度的SPPNet获得了更高的mAP 59.2%。</strong></p><h2 id="ec54" class="mz kh ht bd ki na nb nc km nd ne nf kq jb ng nh ku jf ni nj ky jj nk nl lc nm dt translated">7.2 ILSVRC 2014</h2><figure class="ly lz ma mb fq mc fe ff paragraph-image"><div class="fe ff np"><img src="../Images/84cc8add6197ee65a20162048f7aeb89.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*6Q6hgTUudB3_DXURi891bw.png"/></div><figcaption class="mf mg fg fe ff mh mi bd b be z ek"><strong class="bd mj">SPPNet got 1st Runner-Up in ILSVRC 2014 Object Detection</strong></figcaption></figure><p id="f2d2" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">在ILSVRC 2014中，SPPNet获得35.1%的mAP，在物体检测任务中获得亚军。</strong></p></div><div class="ab cl jz ka hb kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hm hn ho hp hq"><p id="315f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">事实上，微软后来在ILSVRC中贡献了许多最先进的深度学习方法，如PReLUNet和ResNet。我稍后会回顾它们！</p><p id="9da7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">当然，其他网络也将被审查，请保持关注！！！！</p></div><div class="ab cl jz ka hb kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hm hn ho hp hq"><h1 id="7451" class="kg kh ht bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated"><strong class="ak">参考文献</strong></h1><ol class=""><li id="6ed2" class="lj lk ht is b it le ix lf jb ll jf lm jj ln jn lo lp lq lr dt translated">【2014 ECCV】【sp pnet】<br/><a class="ae nq" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.699.8052&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">用于视觉识别的深度卷积网络中的空间金字塔池</a></li><li id="41e9" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated">【2015 TPAMI】【SPPNet】<br/><a class="ae nq" href="https://arxiv.org/pdf/1406.4729.pdf" rel="noopener ugc nofollow" target="_blank">用于视觉识别的深度卷积网络中的空间金字塔池</a></li><li id="19a6" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated">【2014 ECCV】【ZFNet】<br/><a class="ae nq" href="https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf" rel="noopener ugc nofollow" target="_blank">可视化和理解卷积网络</a></li><li id="36c4" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated">【2012 NIPS】【Alex net】<br/><a class="ae nq" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank">使用深度卷积神经网络的ImageNet分类</a></li><li id="542c" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated">【2014 ICLR】【过吃】<br/> <a class="ae nq" href="https://arxiv.org/pdf/1312.6229" rel="noopener ugc nofollow" target="_blank">过吃:使用卷积网络的综合识别、定位和检测</a></li><li id="ee58" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated">【2013 IJCV】【选择性搜索】<br/> <a class="ae nq" href="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf" rel="noopener ugc nofollow" target="_blank">选择性搜索对象识别</a></li><li id="b46a" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated">【2014 CVPR】【R-CNN】<br/><a class="ae nq" href="https://arxiv.org/pdf/1311.2524" rel="noopener ugc nofollow" target="_blank">丰富的特征层次，用于精确的对象检测和语义分割</a></li></ol><h1 id="2036" class="kg kh ht bd ki kj nr kl km kn ns kp kq kr nt kt ku kv nu kx ky kz nv lb lc ld dt translated">我的评论</h1><ol class=""><li id="6e07" class="lj lk ht is b it le ix lf jb ll jf lm jj ln jn lo lp lq lr dt translated">【2013年ILSVRC(图像分类)获奖者ZFNet点评</li><li id="fc2e" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated"><a class="ae nq" rel="noopener" href="/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160">回顾:AlexNet，CaffeNet——ils vrc 2012(图像分类)获奖者</a></li><li id="0dcf" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated"><a class="ae nq" rel="noopener" href="/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754">回顾:over feat——ils vrc 2013定位任务(目标检测)冠军</a></li><li id="113a" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated"><a class="ae nq" rel="noopener" href="/coinmonks/review-r-cnn-object-detection-b476aba290d1">回顾:R-CNN(目标检测)</a></li></ol><blockquote class="nw"><p id="ea22" class="nx ny ht bd nz oa ob oc od oe of jn ek translated">加入Coinmonks <a class="ae nq" href="https://t.me/coincodecap" rel="noopener ugc nofollow" target="_blank">电报频道</a>和<a class="ae nq" href="https://www.youtube.com/c/coinmonks/videos" rel="noopener ugc nofollow" target="_blank"> Youtube频道</a>获取每日<a class="ae nq" href="http://coincodecap.com/" rel="noopener ugc nofollow" target="_blank">加密新闻</a></p></blockquote><h2 id="15d6" class="mz kh ht bd ki na og nc km nd oh nf kq jb oi nh ku jf oj nj ky jj ok nl lc nm dt translated">另外，阅读</h2><ul class=""><li id="20fb" class="lj lk ht is b it le ix lf jb ll jf lm jj ln jn ol lp lq lr dt translated"><a class="ae nq" rel="noopener" href="/coinmonks/top-10-crypto-copy-trading-platforms-for-beginners-d0c37c7d698c">复制交易</a> | <a class="ae nq" rel="noopener" href="/coinmonks/crypto-tax-software-ed4b4810e338">加密税务软件</a></li><li id="723e" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn ol lp lq lr dt translated"><a class="ae nq" href="https://coincodecap.com/grid-trading" rel="noopener ugc nofollow" target="_blank">网格交易</a> | <a class="ae nq" rel="noopener" href="/coinmonks/the-best-cryptocurrency-hardware-wallets-of-2020-e28b1c124069">加密硬件钱包</a></li><li id="874f" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn ol lp lq lr dt translated"><a class="ae nq" href="http://Top 4 Telegram Channels for Crypto Traders" rel="noopener ugc nofollow" target="_blank">密码电报信号</a> | <a class="ae nq" rel="noopener" href="/coinmonks/crypto-trading-bot-c2ffce8acb2a">密码交易机器人</a></li><li id="f33b" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn ol lp lq lr dt translated"><a class="ae nq" rel="noopener" href="/coinmonks/crypto-exchange-dd2f9d6f3769">最佳加密交易所</a> | <a class="ae nq" rel="noopener" href="/coinmonks/bitcoin-exchange-in-india-7f1fe79715c9">印度最佳加密交易所</a></li><li id="47a8" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn ol lp lq lr dt translated"><a class="ae nq" rel="noopener" href="/coinmonks/best-crypto-apis-for-developers-5efe3a597a9f">面向开发人员的最佳加密API</a></li><li id="b359" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn ol lp lq lr dt translated">最佳<a class="ae nq" rel="noopener" href="/coinmonks/top-5-crypto-lending-platforms-in-2020-that-you-need-to-know-a1b675cec3fa">密码借贷平台</a></li><li id="3c98" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn ol lp lq lr dt translated"><a class="ae nq" rel="noopener" href="/coinmonks/free-crypto-signals-48b25e61a8da">免费加密信号</a> | <a class="ae nq" rel="noopener" href="/coinmonks/crypto-trading-bot-c2ffce8acb2a">加密交易机器人</a></li><li id="9487" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn ol lp lq lr dt translated"><a class="ae nq" rel="noopener" href="/coinmonks/leveraged-token-3f5257808b22">杠杆代币的终极指南</a></li></ul></div></div>    
</body>
</html>