<html>
<head>
<title>Practical Reinforcement Learning pt. 4</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实用强化学习。四</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/practical-reinforcement-learning-pt-4-76022bfab5d8?source=collection_archive---------3-----------------------#2018-10-07">https://medium.com/coinmonks/practical-reinforcement-learning-pt-4-76022bfab5d8?source=collection_archive---------3-----------------------#2018-10-07</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><div class=""><h2 id="f0fa" class="pw-subtitle-paragraph iq hs ht bd b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ek translated">一个简单的实现</h2></div><h2 id="c640" class="ji jj ht bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf dt translated">介绍</h2><p id="4bb6" class="pw-post-body-paragraph kg kh ht ki b kj kk iu kl km kn ix ko jt kp kq kr jx ks kt ku kb kv kw kx ky hm dt translated">这篇文章是这个<a class="ae kz" rel="noopener" href="/coinmonks/practical-reinforcement-learning-pt-3-7dc614e850c9">系列</a>的上一篇文章的延续。在本文中，我们将专注于构建一个简单的基于python的实现，实现到目前为止已经介绍过的思想。</p><h2 id="77b8" class="ji jj ht bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf dt translated">问题是</h2><figure class="lb lc ld le fq lf fe ff paragraph-image"><div class="fe ff la"><img src="../Images/f52d70dc27250a617e801eef18f4b555.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*qv5jPh4pi9z9rAoH5y_x6g.png"/></div><figcaption class="li lj fg fe ff lk ll bd b be z ek">Living Room</figcaption></figure><p id="4810" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hm dt translated">让我们来布置舞台。我们想教我们的机器人(位于左上角)在不撞到任何桌子的情况下找到它的主人(小简笔画)。为了简单起见，我们的机器人世界被分成格子，如果它和主人占据相同的格子，我们就认为它成功了。如果它和一张桌子占据同样的面积，我们会认为它是失败的。</p><p id="517c" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hm dt translated">现在，如果我们可以完全访问上面的地图，那么解决这个问题的显而易见的方法是使用一个路径查找算法(如前几篇文章中所提到的)，但是我们将假设代理对它所处的世界没有先验知识，并且将我们自己限制在使用到目前为止在第1、2和3篇文章中介绍的RL技术。</p><h2 id="bced" class="ji jj ht bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf dt translated">设计</h2><p id="4935" class="pw-post-body-paragraph kg kh ht ki b kj kk iu kl km kn ix ko jt kp kq kr jx ks kt ku kb kv kw kx ky hm dt translated">一个好的起点是将问题分解成各个部分。我们将实施分为四个主要部分:</p><ul class=""><li id="3d76" class="lr ls ht ki b kj lm km ln jt lt jx lu kb lv ky lw lx ly lz dt translated"><strong class="ki hu">环境</strong>:代表代理将与之交互的世界。我们将它建模为一个5x 5的网格，其中“空白”空间是横向的，“T”空间是表格，“G”空间代表人。</li><li id="1e7f" class="lr ls ht ki b kj ma km mb jt mc jx md kb me ky lw lx ly lz dt translated"><strong class="ki hu">代理:</strong>代表代理本身。这将接收当前状态(由环境产生)并输出要采取的下一个动作。</li><li id="9174" class="lr ls ht ki b kj ma km mb jt mc jx md kb me ky lw lx ly lz dt translated"><strong class="ki hu">模型:</strong>代表代理的Q值函数。在我们的例子中，这将被表示为一个简单的状态动作对表，其中包含每个动作对的预期回报。其他更复杂的表示也是可能的，将在后面的文章中探讨。</li><li id="d280" class="lr ls ht ki b kj ma km mb jt mc jx md kb me ky lw lx ly lz dt translated"><strong class="ki hu">Q-Learning:</strong>表示Q-Learning算法本身的容器。它管理学习过程和代理与环境之间的接口。</li></ul><figure class="lb lc ld le fq lf fe ff paragraph-image"><div class="fe ff mf"><img src="../Images/91078306cdc34841816a6ad33680bff8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*LoGRXcA8EjkUTJC053ttHg.png"/></div><figcaption class="li lj fg fe ff lk ll bd b be z ek">Q-Learning Design</figcaption></figure><p id="a15b" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hm dt translated">好了，我们有了一个高级方法，我们可以构建一个简单的RL系统，让我们开始研究一些python代码。</p><h2 id="1b0d" class="ji jj ht bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf dt translated">环境</h2><p id="c006" class="pw-post-body-paragraph kg kh ht ki b kj kk iu kl km kn ix ko jt kp kq kr jx ks kt ku kb kv kw kx ky hm dt translated">由于环境是任何RL问题的核心，我们将从构建代码开始。通常，我们的环境需要处理一些行为:</p><ul class=""><li id="9b1a" class="lr ls ht ki b kj lm km ln jt lt jx lu kb lv ky lw lx ly lz dt translated">它需要跟踪模拟的当前状态(如果我们使用物理机器人，那么我们只需要跟踪那些与机器人传感器交互所需的状态)</li><li id="faf3" class="lr ls ht ki b kj ma km mb jt mc jx md kb me ky lw lx ly lz dt translated">它需要在代理和环境之间提供一个接口。代理需要能够执行操作并观察其当前状态。(就可观察到的状态而言)</li><li id="114d" class="lr ls ht ki b kj ma km mb jt mc jx md kb me ky lw lx ly lz dt translated">我们需要一种方法来重置环境，以便我们可以运行另一个训练集。</li></ul><p id="5251" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hm dt translated">出于我们的目的，我们将环境表示为一个五乘五的令牌矩阵，如前一节所述。我们通过下一个方法为代理提供一个到环境的接口。</p><p id="7f9f" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hm dt translated">下一个方法接受一个操作并输出一个填充了以下信息的元组:</p><ul class=""><li id="6c3e" class="lr ls ht ki b kj lm km ln jt lt jx lu kb lv ky lw lx ly lz dt translated"><strong class="ki hu">状态:</strong>这将被表示为代理在世界上的当前网格坐标。</li><li id="b45a" class="lr ls ht ki b kj ma km mb jt mc jx md kb me ky lw lx ly lz dt translated"><strong class="ki hu">动作:</strong>这只是返回与提供的动作相同的动作。如果我们想用概率来模拟行为，这在以后会很有用。即机器人要求向上走，但实际上向左走了。在大多数用例中(包括概率性的，我们将忽略这一点)。</li><li id="8979" class="lr ls ht ki b kj ma km mb jt mc jx md kb me ky lw lx ly lz dt translated"><strong class="ki hu">奖励:</strong>从环境中获得的奖励。在这个特定的环境中，击中桌子的奖励是-1，走过一个空格的奖励是-0.1，找到主人的奖励是100。</li><li id="5cd4" class="lr ls ht ki b kj ma km mb jt mc jx md kb me ky lw lx ly lz dt translated"><strong class="ki hu"> done: </strong>这标志着一集是否已经到达终止状态，例如，如果代理运行到一个表中，我们想要重置该集。</li></ul><p id="f355" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hm dt translated">下面包含了实现该环境的Python代码。这个类中还包含了一个辅助函数“draw_self ”,以便于学习过程的可视化。</p><pre class="lb lc ld le fq mg mh mi mj aw mk dt"><span id="f934" class="ji jj ht mh b fv ml mm l mn mo"># Environment<br/># Observation:<br/>#    state<br/>#    action<br/>#    reward<br/>#    done<br/>class Environment():<br/><br/>    # Encoding:<br/>    # "*": agent position<br/>    # " ": empty square<br/>    # "T": Table<br/>    # "G": Goal<br/>    def __init__(self):<br/>        self.agent_position = (0, 0)<br/>        self.map = [<br/>            [" ", " ", " ", " ", " "],<br/>            [" ", " ", "T", " ", " "],<br/>            [" ", "T", " ", " ", " "],<br/>            [" ", " ", " ", " ", "G"],<br/>            [" ", " ", " ", " ", " "]<br/>        ]<br/><br/>    def draw_env(self):<br/><br/>        x = self.agent_position[1]<br/>        y = self.agent_position[0]<br/><br/>        last_token = self.map[y][x]<br/><br/>        self.map[y][x] = "*"<br/>        print '----------------------'<br/>        for l in self.map:<br/>            print l<br/><br/>        self.map[y][x] = last_token<br/><br/>    # get the token from the current position<br/>    def get_token(self):<br/>        x = self.agent_position[1]<br/>        y = self.agent_position[0]<br/><br/>        return self.map[y][x]<br/><br/>    # reward mapping:<br/>    #  " " -&gt;  0<br/>    #  "T" -&gt; -1<br/>    #  "G" -&gt; +1<br/>    def reward(self):<br/>        token = self.get_token()<br/><br/>        if token == " ":<br/>            return -0.1<br/><br/>        if token == "T":<br/>            return -1<br/><br/>        if token == "G":<br/>            return 100<br/><br/>        return 0<br/><br/>    # clamp a value between 0 and 4<br/>    def clamp_to_map(self, value):<br/>        if value &lt; 0:<br/>            return 0<br/><br/>        if value &gt; 4:<br/>            return 4<br/><br/>        return value<br/><br/>    # action:<br/>    #   UP, DOWN, LEFT, RIGHT<br/>    #   state_position, action, reward, done<br/>    def next(self, action):<br/><br/>        start_position = self.agent_position<br/><br/>        x = self.agent_position[1]<br/>        y = self.agent_position[0]<br/><br/>        # move the agent<br/>        if action == "U":<br/>            y = y - 1<br/><br/>        if action == "D":<br/>            y = y + 1<br/><br/>        if action == "L":<br/>            x = x - 1<br/><br/>        if action == "R":<br/>            x = x + 1<br/><br/>        # clamp it to the environment<br/>        x = self.clamp_to_map(x)<br/>        y = self.clamp_to_map(y)<br/><br/>        self.agent_position = (y, x)<br/><br/>        # determine the reward<br/>        reward = self.reward()<br/><br/>        # is episode complete ?<br/>        token = self.get_token()<br/>        done = (token == "G" or token == "T")<br/><br/>        return (start_position, action, reward, done)<br/><br/>    # sets the agent position back to (0,0)<br/>    def reset(self):<br/>        self.agent_position = (0, 0)</span></pre><h2 id="1307" class="ji jj ht bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf dt translated">代理人</h2><p id="cdb8" class="pw-post-body-paragraph kg kh ht ki b kj kk iu kl km kn ix ko jt kp kq kr jx ks kt ku kb kv kw kx ky hm dt translated">好了，是时候建立我们的代理了！在本文遵循的设计中，我们的代理负责接受环境的当前状态，并选择将要采取的下一步操作。</p><p id="6a2b" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hm dt translated">选择下一个行动会带来所谓的<em class="mp">探索</em>因素。正如在前面的文章中所解释的，这是一个介于0和1之间的值，表示代理将采取随机行动的可能性。用于该实现的方法将使该值从1开始，并在每次学习迭代后略微减少。鼓励读者尝试这个值，看看他们是否能得到更好的结果！</p><p id="9fd6" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hm dt translated">为了选择一个行动，我们需要对代理期望从环境中获得的回报进行建模。因为具有改变模型的灵活性是有帮助的，所以模型作为参数被接受。预计模型将公开一个<em class="mp">预测</em>方法，该方法接受当前状态并输出下一个动作。</p><p id="d482" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hm dt translated">代理对象的代码如下所示:</p><pre class="lb lc ld le fq mg mh mi mj aw mk dt"><span id="aff4" class="ji jj ht mh b fv ml mm l mn mo"># Agent:<br/>#   model as Model<br/>#   state as State<br/>#   exploration as Float<br/>class Agent():<br/><br/>    # needs a model to represent the rewards<br/>    def __init__(self, model, start_state, exploration):<br/>        self.model = model<br/>        self.state = start_state<br/>        self.exploration = exploration<br/><br/>    # encoding<br/>    #   0 &lt;- UP<br/>    #   1 &lt;- RIGHT<br/>    #   2 &lt;- LEFT<br/>    #   3 &lt;- DOWN<br/>    def get_action(self, action_id):<br/>        if action_id == 0:<br/>            return "U"<br/><br/>        if action_id == 1:<br/>            return "R"<br/><br/>        if action_id == 2:<br/>            return "D"<br/><br/>        return "L"<br/><br/>    def next_action(self, env):<br/>        # test against the current exploration constant<br/>        prob = np.random.random()<br/>        action_id = None<br/><br/>        if prob &lt; self.exploration:<br/>            action_id = np.random.choice(4)<br/>        else:<br/>            action_id = self.model.predict(self.state)<br/><br/>        # get the action token<br/>        action = self.get_action(action_id)<br/>        observation = env.next(action)<br/><br/>        self.state = observation[0]<br/><br/>        # return the observation<br/>        return observation<br/><br/>    def reduce_exploration(self):<br/>        self.exploration = self.exploration ** 0.99</span></pre><h2 id="1787" class="ji jj ht bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf dt translated">模型</h2><p id="f2ac" class="pw-post-body-paragraph kg kh ht ki b kj kk iu kl km kn ix ko jt kp kq kr jx ks kt ku kb kv kw kx ky hm dt translated">对于这个问题，我们将保持模型尽可能简单。我们将把Q值表示为状态/动作对的表格。例如，如果代理在位置(0，0)，我们可以在表中有相应的Q值:</p><ul class=""><li id="65f8" class="lr ls ht ki b kj lm km ln jt lt jx lu kb lv ky lw lx ly lz dt translated">向上:0.0</li><li id="e608" class="lr ls ht ki b kj ma km mb jt mc jx md kb me ky lw lx ly lz dt translated">左侧:0.0</li><li id="0c70" class="lr ls ht ki b kj ma km mb jt mc jx md kb me ky lw lx ly lz dt translated">右:0.4</li><li id="3821" class="lr ls ht ki b kj ma km mb jt mc jx md kb me ky lw lx ly lz dt translated">向下:0.3</li></ul><p id="d2bd" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hm dt translated">除了表示Q值之外，我们需要我们的模型能够根据已知的Q值和当前状态来预测最佳行动。这是通过选择使当前状态的值最大化的动作来完成的(如前几篇文章所建议的)。例如，基于上表，模型将预测向上的动作。</p><p id="f0fc" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hm dt translated">最后，我们需要模型能够自我更新。为此，我们将使用上一篇<a class="ae kz" rel="noopener" href="/coinmonks/practical-reinforcement-learning-pt-3-7dc614e850c9">文章</a>中介绍的等式:</p><pre class="lb lc ld le fq mg mh mi mj aw mk dt"><span id="8423" class="ji jj ht mh b fv ml mm l mn mo">Q(s,a) = Q(s,a) + step_size * ( (R + gamma * Qmax(s1, a1) - Q(s,a) )</span></pre><p id="b6d8" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hm dt translated">在下面的代码中，step_size映射到alpha，gamma映射到discount_factor。</p><p id="5bae" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hm dt translated">该代码中还包含一个<em class="mp">策略</em>方法，用于帮助可视化模型在训练后产生的最终策略。</p><pre class="lb lc ld le fq mg mh mi mj aw mk dt"><span id="64e7" class="ji jj ht mh b fv ml mm l mn mo"># Model<br/>class Model():<br/><br/>    def __init__(self, discount_factor, alpha):<br/>        self.discount_factor = discount_factor<br/>        self.actions_options = ("U", "R", "D", "L")<br/>        self.alpha = alpha<br/>        self.Q = {}<br/><br/>        # initialize the actions for all states to zero<br/>        for y in range(5):<br/>            for x in range(5):<br/>                state = (y, x)<br/><br/>                self.Q[state] = {}<br/><br/>                for a in self.actions_options:<br/>                    self.Q[state][a] = 0<br/><br/><br/>    def predict(self, state):<br/><br/>        actions = self.Q[state]<br/><br/>        max_key = None<br/>        max_val = float('-inf')<br/>        for k, v in actions.items():<br/>            if v &gt; max_val:<br/>                max_val = v<br/>                max_key = k<br/><br/>        return max_key<br/><br/>    def update(self, state, action, reward, state2, action2):<br/>        lastQ = self.Q[state][action]<br/>        self.Q[state][action] = self.Q[state][action] + self.alpha * (reward + self.discount_factor * self.Q[state2][action2] - self.Q[state][action])<br/><br/>        return np.abs(lastQ - self.Q[state][action])<br/><br/>    def policy(self, map):<br/>        policy = []<br/><br/>        for y in range(5):<br/>            l = []<br/><br/>            for x in range(5):<br/>                action = self.predict((y, x))<br/><br/>                if map[y][x] != " ":<br/>                    action = map[y][x]<br/><br/>                l.append(action)<br/><br/>            policy.append(l)<br/><br/>        return policy</span></pre><h2 id="c47b" class="ji jj ht bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf dt translated">q学习算法</h2><figure class="lb lc ld le fq lf fe ff paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="fe ff mq"><img src="../Images/5aa494d4cc2ae0236a4e0c884cae8722.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zDHjnA3xNU7WdqT7R7L8Sg.png"/></div></div><figcaption class="li lj fg fe ff lk ll bd b be z ek">Q-Learning</figcaption></figure><p id="7243" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hm dt translated">在下一段代码中，我们有Q学习算法本身。为简单起见，它被分成两部分。<em class="mp">插曲</em>方法是在环境中执行单个插曲，运行代理直到它到达终端状态，并在每个动作之后更新模型。</p><p id="61b8" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hm dt translated">train_agent迭代情节，直到Q值的变化足够低，或者模拟达到1000次迭代(设置这个硬上限是为了保证系统停止)。训练完成后，生成的策略输出到终端供用户检查。</p><pre class="lb lc ld le fq mg mh mi mj aw mk dt"><span id="e525" class="ji jj ht mh b fv ml mm l mn mo">def episode(agent, env):<br/><br/>    done = False<br/><br/>    state = (0, 0)<br/>    observation = agent.next_action(env)<br/>    action = observation[1]<br/><br/>    highest_delta = 0<br/><br/>    while not done:<br/>        #  state_position, action, reward, done<br/>        observation = agent.next_action(env)<br/><br/>        state2 = observation[0]<br/>        action2 = observation[1]<br/>        reward = observation[2]<br/>        done = observation[3]<br/><br/>        delta = agent.model.update(state, action, reward, state2, action2)<br/>        highest_delta = max(delta, highest_delta)<br/><br/>        state = state2<br/>        action = action2<br/><br/>        if done:<br/>            agent.model.Q[state][action] = reward<br/><br/>    return highest_delta<br/><br/>def train_agent(agent, env):<br/>    done = False<br/>    max_iterations = 1000<br/>    i = 0<br/><br/>    while not done:<br/>        change = episode(agent, env)<br/>        env.reset()<br/>        done = (change &lt; 0.005)<br/><br/>        i = i + 1<br/>        if i == max_iterations:<br/>            done = True</span><span id="c59e" class="ji jj ht mh b fv mv mm l mn mo">        agent.reduce_exploration()</span><span id="e19c" class="ji jj ht mh b fv mv mm l mn mo">    policy = agent.model.policy(env.map)<br/><br/>    for l in policy:<br/>        print l<br/><br/><br/>if __name__ == '__main__':<br/>    grid_world = Environment()<br/>    agent_model = Model(discount_factor=0.98, alpha=0.1)<br/>    agent = Agent(agent_model, (0,0), 1.0)<br/>    train_agent(agent, grid_world)</span></pre><h2 id="95f9" class="ji jj ht bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf dt translated">讨论和结果</h2><figure class="lb lc ld le fq lf fe ff paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="fe ff mw"><img src="../Images/d8938ebcc050dfc013dbc0112274b761.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MolftBN6lDaeIc09A135CA.png"/></div></div><figcaption class="li lj fg fe ff lk ll bd b be z ek">Output</figcaption></figure><p id="1a7c" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hm dt translated">以上是我们到目前为止的工作成果。我们可以看到，如果代理从位置(0，0)开始，我们的策略是一直向右移动，然后向下移动到目标。因为在我们当前的场景中，我们的代理总是从(0，0)开始，所以这是唯一相关的策略。</p><h2 id="b1f6" class="ji jj ht bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf dt translated">后续步骤</h2><p id="b097" class="pw-post-body-paragraph kg kh ht ki b kj kk iu kl km kn ix ko jt kp kq kr jx ks kt ku kb kv kw kx ky hm dt translated">本文介绍了一个简单问题的非常基本的RL解决方案。到目前为止，尽管这个系列的标题，很难说有什么实用的东西。在接下来的几篇文章中，我将开始介绍我们可以将这里的想法扩展到可能在真实机器人上工作的方法。</p><p id="9fc6" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hm dt translated">在那之前，</p><p id="5a82" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hm dt translated">分享享受！</p><blockquote class="mx"><p id="f700" class="my mz ht bd na nb nc nd ne nf ng ky ek translated"><a class="ae kz" href="https://coincodecap.com/?utm_source=coinmonks" rel="noopener ugc nofollow" target="_blank">直接在您的收件箱中获得最佳软件交易</a></p></blockquote><figure class="ni nj nk nl nm lf fe ff paragraph-image"><a href="https://coincodecap.com/?utm_source=coinmonks"><div class="fe ff nh"><img src="../Images/7c0b3dfdcbfea594cc0ae7d4f9bf6fcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/0*OJ-qb5G6i863msBB.png"/></div></a></figure></div></div>    
</body>
</html>