<html>
<head>
<title>Real-time Video Style Transfer: Fast, Accurate and Temporally Consistent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实时视频风格传输:快速、准确且时间一致</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/real-time-video-style-transfer-fast-accurate-and-temporally-consistent-863a175e06dc?source=collection_archive---------2-----------------------#2018-07-19">https://medium.com/coinmonks/real-time-video-style-transfer-fast-accurate-and-temporally-consistent-863a175e06dc?source=collection_archive---------2-----------------------#2018-07-19</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><figure class="fi fk iq ir is it"><div class="bz el l di"><div class="iu iv l"/></div></figure><p id="296e" class="pw-post-body-paragraph iw ix ht iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hm dt translated">世界各地的开发人员部署卷积神经网络，用于以其他图片的风格或简单的图像风格转移来重新组合图像。在现有方法达到足够高的处理速度后，视频风格转换也引起了研究者和开发者的兴趣。然而，由于高度的时间不一致性，图像风格转移模型通常不适用于视频，这可以在视觉上观察到连续风格化帧之间的闪烁和移动对象的不一致风格化。一些视频风格传输模型已经成功地提高了时间一致性，但是它们不能同时保证快速的处理速度和良好的感知风格质量。</p><p id="0733" class="pw-post-body-paragraph iw ix ht iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hm dt translated">为了解决这一具有挑战性的任务，最近推出了一种新颖的实时视频风格传输模型<strong class="iy hu"> ReCoNet </strong>。其作者声称，它可以生成时间连贯的风格转换视频，同时保持良好的感知风格。此外，与其他现有方法相比，ReCoNet在定量和定性方面都表现出了出色的性能。那么，现在让我们发现，这个模型的作者是如何能够同时实现高时间一致性、快速处理速度和良好的感知风格质量的！</p><figure class="jv jw jx jy fq it fe ff paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="fe ff ju"><img src="../Images/5e4102258dbabea858e5731880aff3ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iFJugPNPJd0Hd3w6Xp6cyQ.png"/></div></div><figcaption class="kf kg fg fe ff kh ki bd b be z ek"><strong class="bd kj">Figure 1.</strong> Temporal inconsistency in video style transfer: (b) results by Chen et al; © results by ReCoNet</figcaption></figure><h1 id="89f8" class="kk kl ht bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh dt translated">建议的方法</h1><p id="ff70" class="pw-post-body-paragraph iw ix ht iy b iz li jb jc jd lj jf jg jh lk jj jk jl ll jn jo jp lm jr js jt hm dt translated">实时连贯视频风格传输网络(ReCoNet)是由香港大学的一组研究人员<a class="ae ln" href="https://arxiv.org/pdf/1807.01197v1.pdf" rel="noopener ugc nofollow" target="_blank">提出的</a>，作为视频风格传输的最先进方法。这是一个前馈神经网络，以实时速度生成连贯的风格化视频。该过程通过编码器和解码器逐帧进行。VGG损失网络负责捕捉转移目标的感知风格。</p><p id="c6ee" class="pw-post-body-paragraph iw ix ht iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hm dt translated"><strong class="iy hu">他们方法的新颖之处在于在输出级时间损失中引入了亮度扭曲约束。</strong>允许捕捉输入视频中可追踪像素的亮度变化，并在有照明效果的区域增加风格化稳定性。总的来说，这个约束是抑制时间不一致性的关键。然而，作者还提出了一种特征映射级的时间损失，它惩罚连续帧中同一对象的高级特征的变化，因此，进一步增强了可追踪对象的时间一致性。</p><h1 id="2091" class="kk kl ht bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh dt translated">网络体系结构</h1><p id="e894" class="pw-post-body-paragraph iw ix ht iy b iz li jb jc jd lj jf jg jh lk jj jk jl ll jn jo jp lm jr js jt hm dt translated">现在，让我们来探索建议方法的技术细节，并更仔细地研究网络架构，如图2所示。</p><figure class="jv jw jx jy fq it fe ff paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="fe ff lo"><img src="../Images/c94401dc664f377a5006bfd0b8def98d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R__nd84NVuoMb_4HlKA-_g.png"/></div></div><figcaption class="kf kg fg fe ff kh ki bd b be z ek"><strong class="bd kj">Figure 2.</strong> Pipeline of ReCoNet</figcaption></figure><p id="af8b" class="pw-post-body-paragraph iw ix ht iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hm dt translated">ReCoNet由三个模块组成:</p><p id="8d6d" class="pw-post-body-paragraph iw ix ht iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hm dt translated">1.一个<strong class="iy hu">编码器</strong>将输入图像帧转换成具有聚合感知信息的编码特征图。编码器中有三个卷积层和四个残差块。</p><p id="ce59" class="pw-post-body-paragraph iw ix ht iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hm dt translated">2.一个<strong class="iy hu">解码器</strong>从特征地图生成风格化的图像。为了减少棋盘状伪像，解码器包括两个上采样卷积层，最后一个卷积层代替一个传统的去卷积层。</p><p id="803e" class="pw-post-body-paragraph iw ix ht iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hm dt translated">3.一个<strong class="iy hu"> VGG-16 </strong>损失网络计算感知损失。它在ImageNet数据集上进行了预训练。</p><p id="7576" class="pw-post-body-paragraph iw ix ht iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hm dt translated">此外，多级时间损失被添加到编码器的输出和解码器的输出，以减少时间不相干。</p><figure class="jv jw jx jy fq it fe ff paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="fe ff lp"><img src="../Images/0042048a8bde965c932c6acb3ac4d714.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lfhSVQQwFDI_PI6incPGkg.png"/></div></div></figure><p id="35e1" class="pw-post-body-paragraph iw ix ht iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hm dt translated">在训练阶段，实行双框架协同训练机制。这意味着对于每次迭代，网络在两次运行中为两个连续的图像帧生成特征图和风格化输出。注意，在推断阶段，网络在单次运行中仅处理一个图像帧。然而，在训练期间，使用两个帧的特征图和风格化输出来计算时间损失，并且对每个帧独立地计算感知损失并求和。两帧协同训练的最终损失函数是:</p><figure class="jv jw jx jy fq it fe ff paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="fe ff lq"><img src="../Images/2c00450dafa2abe001ae30064f947fda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DsFmMLvdnDEJU5Ph2Ud1Jg.png"/></div></div></figure><p id="1308" class="pw-post-body-paragraph iw ix ht iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hm dt translated">其中<em class="lr"> α </em>、𝛽、𝛾、𝜆 <em class="lr"> 𝑓 </em>和𝜆 <em class="lr"> 𝜊 </em>是训练过程的超参数。</p><h1 id="9089" class="kk kl ht bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh dt translated">ReCoNet生成的结果</h1><p id="5904" class="pw-post-body-paragraph iw ix ht iy b iz li jb jc jd lj jf jg jh lk jj jk jl ll jn jo jp lm jr js jt hm dt translated">图3展示了所建议的方法如何在三个连续的视频帧上传输四种不同的风格。如您所见，ReCoNet成功地再现了样式目标的颜色、笔触和纹理，并创建了视觉上连贯的视频帧。</p><figure class="jv jw jx jy fq it fe ff paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="fe ff ls"><img src="../Images/8b5d97d68bd920745e01a97f039d273e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ikUD7h0RbchHIU9PQFlvtw.png"/></div></div></figure><figure class="jv jw jx jy fq it fe ff paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="fe ff lt"><img src="../Images/ca3320851b6283353368432ba98a6a70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nGHvt69PL21j01PbiCklmw.png"/></div></div></figure><p id="e51b" class="pw-post-body-paragraph iw ix ht iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hm dt translated">接下来，研究人员对ReCoNet与其他三种方法的性能进行了定量比较。下表展示了五种不同场景下四种视频传输模型的时间误差。<em class="lr"> Ruder等人</em>的模型展示了最低的误差，但是从它的FPS参数可以看出，由于推理速度低，它不适合实时使用。<em class="lr">黄等人</em>的模型显示出比ReCoNet更低的时间误差，但让我们转向定性分析，看看该模型是否能够捕获与ReCoNet类似的笔画和微小纹理。</p><figure class="jv jw jx jy fq it fe ff paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="fe ff lu"><img src="../Images/3c859d74da37da23edf4f36716df705d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JUrff4bGda89VoKQnDvt0Q.png"/></div></div></figure><p id="01db" class="pw-post-body-paragraph iw ix ht iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hm dt translated">从图4的顶行可以明显看出，<em class="lr">黄等人</em>的模型没有学习到太多关于感知笔画和图案的知识。这可能是因为它们使用感知损失和时间损失之间的低权重比来保持时间一致性。此外，他们的模型使用损失网络中更深层<em class="lr"> relu4_2 </em>的特征图来计算内容损失，这使得捕捉边缘等低级特征更加困难。</p><figure class="jv jw jx jy fq it fe ff paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="fe ff lv"><img src="../Images/78553371206942188e45ce7d9e38184a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cdbnp-NMvEm-NovDDr2-Qw.png"/></div></div><figcaption class="kf kg fg fe ff kh ki bd b be z ek"><strong class="bd kj">Figure 4. </strong>Qualitative comparison of style transfer results against other approaches</figcaption></figure><p id="be0a" class="pw-post-body-paragraph iw ix ht iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hm dt translated">图4的底行显示<em class="lr">陈等人</em>的工作很好地保持了内容图像和风格图像的感知信息。然而，放大区域揭示了其风格化结果中明显的不一致性，正如较高的时间误差所证实的那样。</p><p id="cafa" class="pw-post-body-paragraph iw ix ht iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hm dt translated">有趣的是，这些模型还通过用户研究进行了比较。在两次比较中，分别对4个不同的视频片段应用了4种不同的风格，并要求50个人回答以下问题:</p><ul class=""><li id="c809" class="lw lx ht iy b iz ja jd je jh ly jl lz jp ma jt mb mc md me dt translated">Q1。就颜色、笔画、纹理和其他视觉模式而言，哪个模型在感知上更类似于风格图像？</li><li id="fa3d" class="lw lx ht iy b iz mf jd mg jh mh jl mi jp mj jt mb mc md me dt translated">Q2。哪个模型在时间上更一致，例如闪烁伪像更少，并且同一物体的颜色和风格一致？</li><li id="af52" class="lw lx ht iy b iz mf jd mg jh mh jl mi jp mj jt mb mc md me dt translated">Q3。总体来说哪个型号更好？</li></ul><figure class="jv jw jx jy fq it fe ff paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="fe ff mk"><img src="../Images/88ff6ee075efa59a0cfdf97399999a56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ACwKseq1q2WMFVyh1lcQLA.png"/></div></div></figure><p id="fb58" class="pw-post-body-paragraph iw ix ht iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hm dt translated">如表3所示，该用户研究的结果验证了从定性分析得出的结论:ReCoNet实现了比<em class="lr"> Chen等人</em>的模型更好的时间一致性，同时保持了类似的良好感知风格；<em class="lr">黄等</em>的模型在时间一致性上优于ReCoNet，但在知觉风格上要差得多。</p><h1 id="f79f" class="kk kl ht bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh dt translated">结果</h1><p id="bea2" class="pw-post-body-paragraph iw ix ht iy b iz li jb jc jd lj jf jg jh lk jj jk jl ll jn jo jp lm jr js jt hm dt translated">这种新颖的视频风格转换方法在以实时处理速度生成连贯的风格化视频方面表现出色，同时保持了非常好的感知风格。作者建议在输出级时间损失和特征图级时间损失中使用亮度扭曲约束，以在照明效果下获得更好的风格化稳定性以及更好的时间一致性。尽管这些约束在提高结果视频的时间一致性方面是有效的，但在时间一致性方面，ReCoNet仍然落后于一些最先进的方法。然而，考虑到其在捕捉内容图像和风格图像的感知信息方面的高处理速度和突出结果，这种方法无疑处于视频风格转换的前沿。</p><p id="ff39" class="pw-post-body-paragraph iw ix ht iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hm dt translated"><a class="ae ln" rel="noopener" href="/@katerynakoidan/">凯特琳娜·科伊丹</a></p></div></div>    
</body>
</html>