<html>
<head>
<title>Understanding Neural ALU</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解神经ALU</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/understanding-neural-alu-e7bf2ea64953?source=collection_archive---------4-----------------------#2018-08-15">https://medium.com/coinmonks/understanding-neural-alu-e7bf2ea64953?source=collection_archive---------4-----------------------#2018-08-15</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><div class=""><h2 id="d628" class="pw-subtitle-paragraph iq hs ht bd b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ek translated">神经算术逻辑单元(NALU)</h2></div><p id="d45c" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">嗨，你现在一定听说过谷歌DeepMind发表的新论文——神经算术逻辑单元。上周，我浏览了一下，从那以后，我就很想了解它的数学上的细微差别。所以在这里我试着把自己看完之后的理解写在纸上。我认为没有比告诉世界你所知道的更好的学习方法了，所以这就是。</p><p id="c64c" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">今天，正如我们所知，神经网络能够识别物体，生成数据，翻译几乎任何东西，甚至可以学习驾驶汽车，只要你有广泛的数据进行训练。但神经网络缺乏的一点是计数或学习数字函数的能力。嗯，它们在训练数据中确实工作得很好，但是当我们推断我们的数据并将其输入到相同的网络中时会发生什么呢？他们当然会失败。所以这是本文的核心主题。神经算术逻辑单元(NALU)正如他们所说，<strong class="jk hu"> <em class="ke">能够跟踪时间，对数字的图像进行算术运算，将数字语言翻译成实值标量，执行计算机代码，并对图像中的对象进行计数。</em>T3】</strong></p><p id="135b" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">为了说明神经网络中的数值外推失败，他们训练正常的神经网络来学习身份函数。训练数据由-5和5之间的数字组成，他们测试了-20和20之间的数字的学习功能。该模型是一个自动编码器，它接受标量值作为输入(例如，输入-2)，使用完全连接的层对其进行编码，然后将输入值重构为最后一个隐藏层的线性组合(例如，输出-2)。结论— <em class="ke">所有的非线性函数都无法学习表示超出训练范围的数字。该故障的严重程度直接对应于所选激活函数内的非线性程度。</em></p><p id="dcb7" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">这是普通自动编码器的样子——</p><blockquote class="kf kg kh"><p id="2427" class="ji jj ke jk b jl jm iu jn jo jp ix jq ki js jt ju kj jw jx jy kk ka kb kc kd hm dt translated"><strong class="jk hu">输入(2) →编码器(输入层+全连接层)→全连接层→解码器(全连接层+重构层)→输出(2) </strong></p></blockquote><p id="12e8" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">正如您在下面看到的，所有的非线性都给出了在-5和5的训练数据范围之外的高平均绝对误差</p><figure class="km kn ko kp fq kq fe ff paragraph-image"><div class="fe ff kl"><img src="../Images/5e10cbba397a039f9714a3ecaaa7bdc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*rosXaUfloS6tz3frCOZL0g.png"/></div><figcaption class="kt ku fg fe ff kv kw bd b be z ek">Source: Neural Arithmetic Logic Units <a class="ae kx" href="https://arxiv.org/abs/1808.00508" rel="noopener ugc nofollow" target="_blank"><strong class="bd ky"><em class="kz">arXiv:1808.00508</em></strong></a><strong class="bd ky"><em class="kz"> [cs.NE]</em></strong></figcaption></figure></div><div class="ab cl la lb hb lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hm hn ho hp hq"><h1 id="35fe" class="lh li ht bd lj lk ll lm ln lo lp lq lr iz ls ja lt jc lu jd lv jf lw jg lx ly dt translated">神经累加器和神经算术逻辑单元</h1><p id="418f" class="pw-post-body-paragraph ji jj ht jk b jl lz iu jn jo ma ix jq jr mb jt ju jv mc jx jy jz md kb kc kd hm dt translated">为了克服上述问题，他们引入了NAC和NALU。</p><p id="c745" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">第一个模型是神经累加器(NAC)，它就像普通的仿射层<strong class="jk hu"> <em class="ke"> f </em> (W <em class="ke"> x </em> + b) </strong>，不同之处在于变换或仅由-1、0和1组成的权重矩阵。也就是说，与输入向量中的行的任意缩放不同，输出是相同的加法和减法。<strong class="jk hu">T9】</strong></p><p id="e40c" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">由于<strong class="jk hu"> W </strong>中的每个元素要么是-1，要么是0，要么是1，所以在微分中存在约束。因此，通过将<strong class="jk hu"> W </strong>写成无约束值，<strong class="jk hu">W = tanh(Wˇ).σ(mˇ)</strong>(tanh和sigmoid的逐元素乘积)<strong class="jk hu">，提出了连续且可微分(能够使用梯度下降)的参数化。</strong> W hat和M hat可以是任何值，并且结果<strong class="jk hu"> W </strong>保证在-1和1之间(包括-1和1 ),并且偏向于接近-1、0和1。</p><figure class="km kn ko kp fq kq fe ff paragraph-image"><div class="fe ff me"><img src="../Images/9cd2ec971419e3e0f55abeefb71226df.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*vMYerlUvUP5gw4LDZv-aSg.png"/></div><figcaption class="kt ku fg fe ff kv kw bd b be z ek"><em class="kz">Neural Accumulator(NAC) Source: </em><a class="ae kx" href="https://arxiv.org/abs/1808.00508" rel="noopener ugc nofollow" target="_blank"><strong class="bd ky"><em class="kz">arXiv:1808.00508</em></strong></a><strong class="bd ky"><em class="kz"> [cs.NE]</em></strong></figcaption></figure><p id="6d48" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">为了学习像乘法这样更复杂的数学函数，引入了神经ALU。它使用两个NAC，一个执行如上所述的加法/减法，另一个能够执行乘法、除法幂函数，如√ x. <strong class="jk hu"> <em class="ke">与NAC一样，在从输入到输出的映射过程中，对学习重新缩放有相同的偏见。</em> </strong></p><p id="d178" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">如果你看下面的图像，有两个紫色的NAC细胞。带有<em class="ke"> matmul </em>的较小的一个使用上面的等式输出NALU加法和减法运算的结果:<strong class="jk hu"> a = Wx </strong>其中<strong class="jk hu">W = tanh(Wˇ).σ(mˇ)。</strong>第二个较大的紫色NAC单元在对数空间中操作，因此能够学习乘法和除法，将其结果存储在:<strong class="jk hu">m = expW(log(| x |+ε))</strong></p><figure class="km kn ko kp fq kq fe ff paragraph-image"><div class="fe ff mf"><img src="../Images/a4b1dfcb0b5c828c8d819b56a94eb5c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*bvnoKqd1bvhbUJOI7Jm4GA.png"/></div><figcaption class="kt ku fg fe ff kv kw bd b be z ek">NALU Source: <a class="ae kx" href="https://arxiv.org/abs/1808.00508" rel="noopener ugc nofollow" target="_blank"><strong class="bd ky"><em class="kz">arXiv:1808.00508</em></strong></a><strong class="bd ky"><em class="kz"> [cs.NE]</em></strong></figcaption></figure><p id="243f" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">最后，这两个NAC单元由一个已学习的s形门<strong class="jk hu"> g </strong>(橙色单元)进行插值。如果添加/子单元的输出值应用了权重1，则另一个NAC单元关闭，反之亦然。</p><p id="a8c4" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated"><strong class="jk hu">y = g . a+(1g)。m </strong>其中<strong class="jk hu"> g = σ(Gx) </strong></p><p id="f383" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated"><strong class="jk hu">T41】</strong></p></div><div class="ab cl la lb hb lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hm hn ho hp hq"><h1 id="066d" class="lh li ht bd lj lk ll lm ln lo lp lq lr iz ls ja lt jc lu jd lv jf lw jg lx ly dt translated"><strong class="ak"> <em class="kz">实验</em> </strong></h1><p id="8273" class="pw-post-body-paragraph ji jj ht jk b jl lz iu jn jo ma ix jq jr mb jt ju jv mc jx jy jz md kb kc kd hm dt translated">他们做了各种实验来证明他们的主张。首先是简单的函数学习任务。NAC和NALU能够静态和递归地学习加法、减法、乘法、除法、平方和平方根等函数。结果是<strong class="jk hu"> <em class="ke">虽然几个标准架构在插值的情况下成功完成了这些任务，但是没有一个在外推法上成功。然而，在内插法和外推法中，NAC成功地模拟了加法和减法，而更灵活的NALU也成功地模拟了乘法运算。</em> </strong> <br/>其次，他们对MNIST数字计数任务和MNIST数字加法任务进行了实验。标准架构在插值长度的保持序列上是成功的，但是在外推上完全失败。值得注意的是，<strong class="jk hu"><em class="ke">RNN-坦和RNN-雷鲁模型也无法学习插值到比训练期间所见更短的序列。然而，新议程联盟和NALU都外推和插值很好。</em>T9】</strong></p><p id="da57" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">他们还试验了语言到数字的翻译，以测试数字单词的表达是否是以系统的方式学习的。不出所料，NAC和NALU不仅学会了，而且聪明地推断出将语言翻译成数字。<br/>他们还训练了一个强化学习程序，目标是代理必须在5x5网格中给出的完全相同的时间实例<strong class="jk hu"> T </strong>到达目标</p><figure class="km kn ko kp fq kq fe ff paragraph-image"><div class="fe ff mg"><img src="../Images/fea8d79faf335f7cdc521b08d0153321.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*ZDDEKpkKEDZaEFh8_v-RCA.png"/></div><figcaption class="kt ku fg fe ff kv kw bd b be z ek">Learning to track time in a grid world environment Source: <a class="ae kx" href="https://arxiv.org/abs/1808.00508" rel="noopener ugc nofollow" target="_blank"><strong class="bd ky"><em class="kz">arXiv:1808.00508</em></strong></a><strong class="bd ky"><em class="kz"> [cs.NE]</em></strong></figcaption></figure><p id="eb33" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">有两种型号。简单地说，一个具有LSTM记忆，另一个具有LSTM记忆，并通过NAC返回到LSTM。<strong class="jk hu">两名特工都接受过T∞U { 5，12}发作的训练。两位特工都很快学会了掌握训练情节。然而，具有NAC的代理在T ≤ 19时在任务中表现良好，而标准LSTM代理在T &gt; 13 </strong> <em class="ke">时表现恶化。两个代理都失败了。作者<strong class="jk hu">假设，相对于NAC的其他用途，此处NAC更有限的外推(就数量级而言)是由仍在某种程度上使用LSTM编码算术的模型引起的。</strong></em></p><blockquote class="mh"><p id="be8c" class="mi mj ht bd mk ml mm mn mo mp mq kd ek translated"><a class="ae kx" href="https://coincodecap.com/?utm_source=coinmonks" rel="noopener ugc nofollow" target="_blank">直接在您的收件箱中获得最佳软件交易</a></p></blockquote><figure class="ms mt mu mv mw kq fe ff paragraph-image"><a href="https://coincodecap.com/?utm_source=coinmonks"><div class="fe ff mr"><img src="../Images/7c0b3dfdcbfea594cc0ae7d4f9bf6fcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/0*OJ-qb5G6i863msBB.png"/></div></a></figure></div></div>    
</body>
</html>