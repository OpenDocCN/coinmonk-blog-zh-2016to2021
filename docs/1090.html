<html>
<head>
<title>Tensorflow Graph</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">张量流图</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/tensorflow-graph-560409c20485?source=collection_archive---------8-----------------------#2018-07-19">https://medium.com/coinmonks/tensorflow-graph-560409c20485?source=collection_archive---------8-----------------------#2018-07-19</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><p id="40d8" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">这篇帖子讲的是tensorflow如何执行你的机器学习模型。我们将简要概述张量流图的组成部分，然后深入研究如何在单个和多个设备上执行该图。</p><p id="4c69" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">张量流图具有以下属性。每个<strong class="is hu">节点</strong>有<strong class="is hu">零个或多个输入</strong>，代表一个操作的<strong class="is hu">实例化。</strong></p><p id="8417" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">从图的边缘流出的值被称为<strong class="is hu">张量</strong>。这些张量在经过这些<strong class="is hu">节点</strong>时会经历各种变换。</p><p id="f9a5" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">张量是<strong class="is hu">任意维度数组</strong>，其中<strong class="is hu">的底层元素类型</strong>是在图形构建时推断出来的。这使得Tensorflow非常快，因为它通过这个图知道未来会发生什么操作。因此，这些知识允许各种编译时优化。</p><p id="f587" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">特殊边被称为控制依赖——没有数据流经这些边，但是它们表示控制依赖的<strong class="is hu">源节点</strong>必须在<strong class="is hu">目的节点能够执行</strong>之前完成执行</p><p id="076f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">这个属性允许客户端在关系之前执行<strong class="is hu">关系。例如，这对于控制峰值内存使用非常有帮助。</strong></p><h1 id="becf" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl dt translated">操作和内核</h1><p id="9498" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hm dt translated">一个操作定义一个计算:例子可以是</p><ol class=""><li id="b2e1" class="kr ks ht is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz dt translated">增加</li><li id="6af9" class="kr ks ht is b it la ix lb jb lc jf ld jj le jn kw kx ky kz dt translated">矩阵乘法</li></ol><p id="0686" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">一个操作可以有<strong class="is hu">属性</strong>。属性的一个用例是使操作<strong class="is hu">多态</strong>(在相同数据类型的元素之间执行操作)</p><p id="c780" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">内核被定义为:一个操作的<strong class="is hu">实现，它可以在<strong class="is hu">特定类型的设备</strong> (CPU、GPU、TPU)等上运行。</strong></p><h1 id="d905" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl dt translated">会议</h1><p id="607a" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hm dt translated">客户端通过创建一个<strong class="is hu">会话</strong>与Tensorflow系统进行交互。</p><ol class=""><li id="7a04" class="kr ks ht is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz dt translated">session接口有一个方法叫做<strong class="is hu">扩展</strong>。这允许我们用额外的<strong class="is hu">节点和边</strong>修改计算图。</li><li id="018c" class="kr ks ht is b it la ix lb jb lc jf ld jj le jn kw kx ky kz dt translated">会话接口有另一个方法<strong class="is hu">运行</strong>。</li></ol><ul class=""><li id="0191" class="kr ks ht is b it iu ix iy jb kt jf ku jj kv jn lf kx ky kz dt translated">该函数计算所有节点的<strong class="is hu">传递闭包，为了计算请求的输出，必须执行这些节点。</strong></li><li id="836a" class="kr ks ht is b it la ix lb jb lc jf ld jj le jn lf kx ky kz dt translated">然后<strong class="is hu">按照尊重其依赖性的顺序</strong>排列节点</li></ul><p id="cfd7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">通常，张量流的用途是</p><ol class=""><li id="e30b" class="kr ks ht is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz dt translated">设置一次带有图形的会话。</li><li id="733d" class="kr ks ht is b it la ix lb jb lc jf ld jj le jn kw kx ky kz dt translated">通过<em class="lg">运行</em>运行图形或不同的子图形数千次或数百万次</li></ol><p id="4480" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">注</strong>:图的传递闭包是定义图中每个节点之间可达性的矩阵。这个矩阵将用0和1填充。<strong class="is hu"> 0 </strong>定义不可达，<strong class="is hu"> 1 </strong>定义可达</p><h1 id="9b76" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl dt translated">变量</h1><p id="0040" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hm dt translated">一个变量是一个<strong class="is hu">持久张量</strong>。大多数张量在运行操作后无法存活。运行操作后，变量<strong class="is hu">继续存在。变量的用例在<strong class="is hu">中存储神经网络</strong>的参数。当图形上的<em class="lg">运行</em>被调用时，这些变量被更新。</strong></p><h1 id="b5d2" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl dt translated">设备</h1><p id="9fce" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hm dt translated"><strong class="is hu">工人操作一个或多个设备</strong>。这些设备可以是CPU内核、GPU等。它们由设备名称和设备类型来标识。设备名称的Eg可以是</p><p id="fbcf" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><code class="eh lh li lj lk b">/job:localhost/device:cpu:0</code></p><p id="1e5d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">在分布式设置中，作业名称是指设备正在执行的作业。每个设备对象有两个功能:</p><ol class=""><li id="f559" class="kr ks ht is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz dt translated"><strong class="is hu">分配/解除分配</strong>内存</li><li id="8970" class="kr ks ht is b it la ix lb jb lc jf ld jj le jn kw kx ky kz dt translated"><strong class="is hu">安排更高层请求的内核</strong>的执行。</li></ol><h1 id="e2b4" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl dt translated">张量</h1><p id="96dd" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hm dt translated"><strong class="is hu">类型的多维数组</strong>，这些张量是Tensorflow的基础数据类型。张量可以有各种类型，从:</p><ol class=""><li id="7ec0" class="kr ks ht is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz dt translated">8位至64位</li><li id="1367" class="kr ks ht is b it la ix lb jb lc jf ld jj le jn kw kx ky kz dt translated">IEEE浮点型和双精度型</li><li id="c32e" class="kr ks ht is b it la ix lb jb lc jf ld jj le jn kw kx ky kz dt translated">复数数据类型</li><li id="d0ed" class="kr ks ht is b it la ix lb jb lc jf ld jj le jn kw kx ky kz dt translated">字符串类型(任意字节数组)</li></ol><h1 id="352d" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl dt translated">执行图表:实现视角</h1><h1 id="8797" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl dt translated">概观</h1><p id="6734" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hm dt translated">客户端Tensorflow中的主实体。客户端联系主进程和一个或多个工作进程。</p><p id="82f7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">工作进程处理与GPU或CPU内核等设备的计算。</p><p id="05a4" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">张量流中有两种设置:</p><ol class=""><li id="77ee" class="kr ks ht is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz dt translated"><strong class="is hu">本地设置</strong> —客户端、主机和工人都在<strong class="is hu">同一台电脑中。</strong></li><li id="8bb5" class="kr ks ht is b it la ix lb jb lc jf ld jj le jn kw kx ky kz dt translated"><strong class="is hu">分布式设置</strong> —客户端、主机和工人都可以在<strong class="is hu">不同的设备</strong>中。在分布式设置中，我们在容器中运行这些不同的组件。这些容器通过像Kubernetes这样的集群调度系统进行调度。</li></ol><h1 id="15f1" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl dt translated">单一设备设置</h1><p id="4290" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hm dt translated">运行Tensorflow最简单的场景。</p><ol class=""><li id="a7d0" class="kr ks ht is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz dt translated">单个工作进程</li><li id="7406" class="kr ks ht is b it la ix lb jb lc jf ld jj le jn kw kx ky kz dt translated">单一设备</li></ol><p id="0454" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">节点的处理方式尊重节点之间的依赖关系。更具体地说</p><ul class=""><li id="a3e5" class="kr ks ht is b it iu ix iy jb kt jf ku jj kv jn lf kx ky kz dt translated">每个节点保持有多少依赖节点需要被处理的计数<em class="lg">。每当执行一个依赖项时，该计数就递减。</em></li><li id="6746" class="kr ks ht is b it la ix lb jb lc jf ld jj le jn lf kx ky kz dt translated">当<em class="lg">计数</em>为<strong class="is hu"> 0 </strong>时，该节点被放入就绪队列，进行后续处理。</li></ul><p id="7c78" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">请注意:<em class="lg">未指定就绪队列如何处理节点</em></p><h1 id="030d" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl dt translated">多设备设置</h1><p id="fbae" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hm dt translated">一旦我们有了多种设备。我们有两件事要担心:</p><ol class=""><li id="2093" class="kr ks ht is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz dt translated">决定将每个节点的计算放在哪个设备上</li><li id="620a" class="kr ks ht is b it la ix lb jb lc jf ld jj le jn kw kx ky kz dt translated">管理这些设备之间的通信。</li></ol><h1 id="adf6" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl dt translated">节点布局</h1><p id="e3b6" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hm dt translated">节点放置算法计算出<strong class="is hu">将什么节点给予什么设备</strong>。该算法使用一个<strong class="is hu">成本模型</strong>来做出决策。根据白皮书，节点放置算法使用<strong class="is hu">贪婪试探法</strong>，通过成本模型和其他参数来决定将节点放置在哪个设备中。这种贪婪的试探法考虑到了</p><ol class=""><li id="cfce" class="kr ks ht is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz dt translated">执行计算的<strong class="is hu">成本。</strong></li><li id="c902" class="kr ks ht is b it la ix lb jb lc jf ld jj le jn kw kx ky kz dt translated"><strong class="is hu">从其他设备向该节点</strong>传送输入的成本。</li></ol><p id="fd28" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">该计算将<strong class="is hu">最快完成的器件</strong>被选为器件。这个放置过程一直持续到节点被放置。</p><p id="c437" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">这个算法现在可能已经改变了，因为这篇论文是在2016年写的。</p><p id="d5c3" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">一旦在设备中放置了节点，就需要在这些设备之间放置通信协议。</p><h1 id="5a67" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl dt translated">设备间通信</h1><p id="b53b" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hm dt translated">Tensorflow <strong class="is hu">移除不同设备中节点</strong>之间的边，而<strong class="is hu">用发送和接收调用</strong>来替换它们。在运行时，发送和接收调用协调在设备间传输数据。</p><p id="09f7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">这种方法有以下好处:</p><ol class=""><li id="6c5a" class="kr ks ht is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz dt translated">数据仅通过接收调用<strong class="is hu">发送一次</strong>,内存仅针对单个张量分配一次。因此张量的所有用户将不需要他们单独的接收/发送呼叫。</li><li id="f4b6" class="kr ks ht is b it la ix lb jb lc jf ld jj le jn kw kx ky kz dt translated">通过以这种方式处理通信，我们让设备中不同节点的调度<strong class="is hu">分散到工人</strong>中。主设备不需要跟踪这一点，因为<em class="lg">发送</em>和<em class="lg">接收</em>调用处理不同工人和设备之间的同步。</li></ol><h1 id="8ec8" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl dt translated">分布式环境中的执行</h1><p id="b29a" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hm dt translated">分布式设置与多设备设置非常相似。因为发送和接收调用是通过<strong class="is hu"> TCP或RDMA </strong>调用来跨机器边界移动数据的。分布式设置中的执行需要容错。故障通过两种方式检测:</p><ol class=""><li id="d99e" class="kr ks ht is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz dt translated"><strong class="is hu">发送和接收呼叫之间的通信</strong>出错。</li><li id="5875" class="kr ks ht is b it la ix lb jb lc jf ld jj le jn kw kx ky kz dt translated">从主流程到每个工人流程的定期健康检查。</li></ol><p id="698c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">当检测到故障时，整个图形执行被<strong class="is hu">中止并从零开始。</strong></p><p id="a657" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">然而，tensorflow系统支持<strong class="is hu">检查点和重启后的恢复</strong>。</p><p id="5da7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">变量值通过所谓的<strong class="is hu">保存节点</strong>进行检查</p><p id="0e91" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">这些保存节点与变量相连接。这些节点可以配置为定期执行。比如每N次迭代后，或者从不超过N秒后一次。</p><p id="e236" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">类似地，这些变量也与一个<strong class="is hu">恢复节点</strong>连接，这样它们的值在重启后被恢复。</p><h1 id="f6c6" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl dt translated">在多种设备上进行训练的技术</h1><h1 id="d92f" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl dt translated">同步SGD</h1><p id="96bb" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hm dt translated">这个SGD依赖于一个跟踪模型参数的主服务器和几个执行一些计算的工作线程。这些工人然后将数据发送回主设备以更新参数。一旦master接收到来自工人的所有参数，它就会累积这些梯度，然后向每个工人发送新梯度的副本，以便工人可以处理下一批</p><h1 id="f5c3" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl dt translated">异步SGD</h1><p id="1de3" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hm dt translated">以上方法论不错，但我们可以做得更好。异步SGD仅仅意味着主设备在接收到一些参数后，执行更新并将梯度推送到所有的工作设备。它不会等到所有的工人都完成任务。</p><h1 id="552e" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl dt translated">模型并行训练</h1><p id="85a7" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hm dt translated">用于训练深层LSTMS。这种类型的训练是模型计算的不同部分在不同的计算设备上对同一批例子同时进行<strong class="is hu">。</strong></p><h1 id="00a3" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl dt translated">模型计算流水线的并发步骤</h1><p id="b90c" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hm dt translated">另一种更好地利用训练深度神经网络的常见方法是在同一设备中流水线化模型的计算。它是与ASGD完全相同的<strong class="is hu">，但不是多个设备，而是在同一个设备</strong>中执行相同的模型，以更好地利用设备能力来并行操作。</p><h1 id="ed07" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl dt translated">结论</h1><p id="e4e2" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hm dt translated">总之，Tensorflow是一个支持以下功能的系统</p><ol class=""><li id="ca72" class="kr ks ht is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz dt translated"><strong class="is hu">在多个设备上进行训练和推理</strong>，非常适合在<strong class="is hu">分布式设置中使用</strong>。</li><li id="54a8" class="kr ks ht is b it la ix lb jb lc jf ld jj le jn kw kx ky kz dt translated">通过<strong class="is hu">数据流图结构，其设计方式有利于未来的优化。</strong></li><li id="fbe3" class="kr ks ht is b it la ix lb jb lc jf ld jj le jn kw kx ky kz dt translated">通过使用<strong class="is hu">压缩技术，使设备之间的通信更加简单。</strong></li><li id="caa5" class="kr ks ht is b it la ix lb jb lc jf ld jj le jn kw kx ky kz dt translated"><strong class="is hu">布局算法</strong>特别有趣，作者说它有可能被深度学习算法取代<strong class="is hu">。</strong></li></ol></div></div>    
</body>
</html>