<html>
<head>
<title>Text classifier with Keras+TensorFlow using Recurrent Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于递归神经网络的Keras+TensorFlow文本分类器</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/text-classifier-with-keras-tensorflow-using-recurrent-neural-networks-ad63dd5fc316?source=collection_archive---------2-----------------------#2018-06-22">https://medium.com/coinmonks/text-classifier-with-keras-tensorflow-using-recurrent-neural-networks-ad63dd5fc316?source=collection_archive---------2-----------------------#2018-06-22</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><figure class="fi fk ir is it iu fe ff paragraph-image"><div class="fe ff iq"><img src="../Images/3cda9b9d7d46bb8b003ed95d2a63d768.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*PqcRRkYz6h0kUZEYXl9F9g.png"/></div></figure><p id="83f5" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt jv translated"><span class="l jw jx jy bm jz ka kb kc kd di"> R </span>通用神经网络(RNN)可以用来分析文本序列，并根据参数分配标签。例如，在<a class="ae ke" href="https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py" rel="noopener ugc nofollow" target="_blank"> Keras示例</a>中，它们被用于将IMDB电影评论分为正面或负面。</p><p id="9900" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">在这个例子中，我们将使用RNN来训练分类器，以解决与“系列”中的其他故事密切相关的问题，这些故事涉及使用LSTM(长短期记忆)来自动生成音乐歌词，从特定流派的语料库中学习“风格”:</p><div class="kf kg fm fo kh ki"><a rel="noopener follow" target="_blank" href="/@monocasero/word-level-lstm-text-generator-creating-automatic-song-lyrics-with-neural-networks-b8a1617104fb"><div class="kj ab ej"><div class="kk ab kl cl cj km"><h2 class="bd hu fv z el kn eo ep ko er et hs dt translated">单词级LSTM文本生成器。用神经网络生成自动歌词。</h2><div class="kp l"><h3 class="bd b fv z el kn eo ep ko er et ek translated">我开始用非技术性的聊天谈论这个项目，谈论我对5000个语料库所做的分析…</h3></div><div class="kq l"><p class="bd b gc z el kn eo ep ko er et ek translated">medium.com</p></div></div></div></a></div><div class="kf kg fm fo kh ki"><a rel="noopener follow" target="_blank" href="/@monocasero/update-automatic-song-lyrics-creator-with-word-embeddings-e30de94db8d1"><div class="kj ab ej"><div class="kk ab kl cl cj km"><h2 class="bd hu fv z el kn eo ep ko er et hs dt translated">更新:自动歌词创作与词嵌入</h2><div class="kp l"><h3 class="bd b fv z el kn eo ep ko er et ek translated">这是故事的延续…</h3></div><div class="kq l"><p class="bd b gc z el kn eo ep ko er et ek translated">medium.com</p></div></div></div></a></div><p id="3df3" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">然而，我也希望这可以被视为一个独立的信息。</p><p id="a66f" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">所有代码、一些文本语料库和更多文档都可以在Github正在进行的项目的页面中找到:</p><div class="kf kg fm fo kh ki"><a href="https://github.com/enriqueav/lstm_lyrics" rel="noopener  ugc nofollow" target="_blank"><div class="kj ab ej"><div class="kk ab kl cl cj km"><h2 class="bd hu fv z el kn eo ep ko er et hs dt translated">enriqueav/lstm _歌词</h2><div class="kp l"><h3 class="bd b fv z el kn eo ep ko er et ek translated">lstm _歌词- LSTM文字生成。用于从音乐流派的语料库中生成歌词。</h3></div><div class="kq l"><p class="bd b gc z el kn eo ep ko er et ek translated">github.com</p></div></div><div class="kr l"><div class="ks l kt ku kv kr kw iv ki"/></div></div></a></div></div><div class="ab cl kx ky hb kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="hm hn ho hp hq"><h1 id="5b56" class="le lf ht bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb dt translated">分类器的背景和目标</h1><p id="9e48" class="pw-post-body-paragraph ix iy ht iz b ja mc jc jd je md jg jh ji me jk jl jm mf jo jp jq mg js jt ju hm dt translated">lstm_lyrics项目的主要目标是训练一个神经网络来“学习”音乐流派的歌词风格，然后能够从中生成文本行。然而，算法总是试图添加某些“可变性”，以避免陷入某些无限的文本循环，或者在给定某个种子的情况下生成完全相同的东西。</p><p id="e045" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">不幸的是，经过大量的实验，测试了许多不同的网络架构、数据表示等，每次这种特定的“可变性”都会导致一些生成的文本几乎是随机的。</p><p id="c559" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">判断这种情况，并考虑到我已经在为我当前的工作做文本分类器，我决定创建另一个网络来对来自该流派语料库的真实文本和随机生成的行进行分类。想法是，<strong class="iz hu">在这个新网络被训练之后，它将能够预过滤由歌词生成器创建的<em class="mh">最差的</em>行。</strong>这意味着它将检测更接近随机噪声的生成线。</p><h1 id="ebbb" class="le lf ht bd lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx mm lz ma mb dt translated"><strong class="ak">训练集的创建</strong></h1><p id="3e38" class="pw-post-body-paragraph ix iy ht iz b ja mc jc jd je md jg jh ji me jk jl jm mf jo jp jq mg js jt ju hm dt translated">我已经创建了一个实用程序脚本来生成新的神经网络将使用的训练集。它可以按如下方式执行:</p><p id="02f5" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated"><code class="eh mn mo mp mq b">python3 utils/generate_classifier_set.py corpora/corpus_banda.txt banda_subset.txt random_banda.txt</code></p><p id="5df6" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">我不会详细解释这个脚本，但基本思想是它将创建两个文件，第一个文件是原始语料库的子集，忽略所有包含至少一个被忽略单词的行(根据参数MIN_WORD_FREQUENCY过滤的不太常用的单词)。</p><p id="2182" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">第二个文件将包含随机生成的文本，遵循以下规则:</p><ul class=""><li id="f8ea" class="mr ms ht iz b ja jb je jf ji mt jm mu jq mv ju mw mx my mz dt translated">使用与第一个文件相同的单词，这意味着语料库的总词汇量减去被忽略的单词(不常用的)。</li><li id="9e1b" class="mr ms ht iz b ja na je nb ji nc jm nd jq ne ju mw mx my mz dt translated">包含相同数量的行(每行是一个训练或测试示例)。</li><li id="aa0b" class="mr ms ht iz b ja na je nb ji nc jm nd jq ne ju mw mx my mz dt translated">以与原始语料库中相同的概率选择单词。</li><li id="35c4" class="mr ms ht iz b ja na je nb ji nc jm nd jq ne ju mw mx my mz dt translated">以与语料库中相同的概率选择行的长度，这意味着，如果语料库中30%的行具有5个单词，则大致相同百分比的随机生成的行将具有5个单词，等等。</li></ul><p id="4c9b" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">考虑到所有这些规则，这两个文件的大小将非常相似。举一个具体的例子:</p><pre class="nf ng nh ni fq nj mq nk nl aw nm dt"><span id="da3d" class="nn lf ht mq b fv no np l nq nr">Both files have exactly the same number of lines (126,665 in this case), and roughly the same number of total words (703,435 vs. 705,279) and characters (<em class="mh">3,515,098 vs. 3,523,796)</em>:</span><span id="cd18" class="nn lf ht mq b fv ns np l nq nr"><em class="mh">$ python3 generate_random_lines.py corpora/corpus_banda.txt </em>banda_subset.txt<em class="mh"> </em>random_banda.txt<em class="mh"><br/>$ wc </em>banda_subset.txt<em class="mh"> </em>random_banda.txt<em class="mh"><br/>  126665  703435 3515098 </em>banda_subset.txt<em class="mh"><br/>  126665  705279 3523796 </em>random_banda.txt</span></pre><p id="35c5" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">训练分类器的脚本将读取这两个文件来创建训练/测试示例。</p><h1 id="f38c" class="le lf ht bd lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx mm lz ma mb dt translated">分类器训练</h1><p id="53ce" class="pw-post-body-paragraph ix iy ht iz b ja mc jc jd je md jg jh ji me jk jl jm mf jo jp jq mg js jt ju hm dt translated">如果你已经看了这一系列的故事(现在你已经看了😊)，你已经知道歌词生成器的单个训练示例由一个句子组成，一个单词接一个单词，目标标签是单个值，由语料库中的下一个单词表示。例如:</p><pre class="nf ng nh ni fq nj mq nk nl aw nm dt"><span id="3e08" class="nn lf ht mq b fv no np l nq nr">&gt;&gt;&gt; sentences[0]</span><span id="2018" class="nn lf ht mq b fv ns np l nq nr">['put', 'a', 'gun', 'against', 'his']</span><span id="5261" class="nn lf ht mq b fv ns np l nq nr">&gt;&gt;&gt; next_words[0]</span><span id="e1ff" class="nn lf ht mq b fv ns np l nq nr">'head'</span><span id="7ac1" class="nn lf ht mq b fv ns np l nq nr">&gt;&gt;&gt; sentences[1]</span><span id="31f5" class="nn lf ht mq b fv ns np l nq nr">['a', 'gun', 'against', 'his', 'head']</span><span id="d965" class="nn lf ht mq b fv ns np l nq nr">&gt;&gt;&gt; next_words[1]</span><span id="c1fc" class="nn lf ht mq b fv ns np l nq nr">'pulled'</span></pre><p id="3e8d" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">给定输入，单个例子的网络输出将是每个可能单词的概率。因此，如果您的词汇表包含5000个不同的单词，那么输出将是一个包含5000个值的向量，每个单词的概率都为1。例如，如果网络已经训练好，如果您输入:</p><pre class="nf ng nh ni fq nj mq nk nl aw nm dt"><span id="11c2" class="nn lf ht mq b fv no np l nq nr">['put', 'a', 'gun', 'against', 'his']</span></pre><p id="06f4" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">5000个概率中最高的将是指向单词<code class="eh mn mo mp mq b">'head'</code>的那个。</p><p id="c2ba" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">二进制分类器的情况与此类似，但甚至更简单，因为输出将是0和1之间的单个浮点值，表示网络对给定句子的置信度是<strong class="iz hu">肯定的，</strong>无论这在您的上下文中意味着什么。在这种情况下，<strong class="iz hu">正</strong>表示真实文本(与随机生成相反)。</p></div><div class="ab cl kx ky hb kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="hm hn ho hp hq"><p id="cd8a" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">现在说说代码。</p><p id="b73e" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">完整的脚本<a class="ae ke" href="https://github.com/enriqueav/lstm_lyrics/blob/master/classifier_train.py" rel="noopener ugc nofollow" target="_blank">可在此处获得</a>，我们将回顾最重要的部分。</p><p id="a88a" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">首先，我们从包含正面和负面例子的文件中创建训练集。我们已经介绍了如何创建这些文件。因为不是所有的句子都有相同的长度(以单词为单位)，所以我们用<code class="eh mn mo mp mq b">pad_and_split_sentences</code>填充它们，我们还创建了标签<code class="eh mn mo mp mq b">y</code>，用正确的长度连接0和1。</p><pre class="nf ng nh ni fq nj mq nk nl aw nm dt"><span id="17a4" class="nn lf ht mq b fv no np l nq nr">good_ones = process_file(sys.argv[1])<br/>bad_ones = process_file(sys.argv[2])<br/><br/>x = pad_and_split_sentences(good_ones + bad_ones)<br/>y = [1]*len(good_ones) + [0]*len(bad_ones)</span></pre><p id="8cf4" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">然后我们创建字典，在这个过程之后，变量<code class="eh mn mo mp mq b">words</code>将是一个包含两个文件中所有不同单词的有序集合</p><pre class="nf ng nh ni fq nj mq nk nl aw nm dt"><span id="7202" class="nn lf ht mq b fv no np l nq nr">print("Reading files and getting unique words")<br/>words = set([PAD_WORD])<br/>for line in x:<br/>    words = words.union(set(line))<br/>words = sorted(words)<br/>print('Unique words:', len(words))<br/><br/>word_indices = dict((c, i) for i, c in enumerate(words))<br/>indices_word = dict((i, c) for i, c in enumerate(words))</span></pre><p id="e5d0" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">下一步是相当标准的，我们洗牌并分成90%的训练和10%的测试</p><pre class="nf ng nh ni fq nj mq nk nl aw nm dt"><span id="8a01" class="nn lf ht mq b fv no np l nq nr">sentences, labels, sentences_test, labels_test = shuffle_and_split_training_set(x, y)</span></pre><p id="8820" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">然后我们得到模型，你可以在这个故事中读到单词嵌入<a class="ae ke" rel="noopener" href="/@monocasero/update-automatic-song-lyrics-creator-with-word-embeddings-e30de94db8d1">，它基本上是一种将不同单词翻译成向量的方法(在这个例子中，有32个维度)。然后我们将这些向量传递给一个64个单位的双向长短期记忆(一种RNN单位)。</a></p><p id="c860" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">增加辍学是避免过度适应的一种<a class="ae ke" href="https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/" rel="noopener ugc nofollow" target="_blank">方式。最后，输出是1的密集层，有<strong class="iz hu"> sigmoid </strong>激活。正如我们所讨论的，分类器的输出将是一个介于0和1之间的浮点值，这由<strong class="iz hu"> sigmoid </strong>给出。</a></p><pre class="nf ng nh ni fq nj mq nk nl aw nm dt"><span id="1013" class="nn lf ht mq b fv no np l nq nr">model = Sequential()<br/>model.add(Embedding(len(words), 32))<br/>model.add(Bidirectional(LSTM(64)))<br/>model.add(Dropout(dropout))<br/>model.add(Dense(1, activation='sigmoid'))</span></pre><p id="a5fd" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">然后我们用<strong class="iz hu">binary _ cross entropy</strong>loss和<strong class="iz hu"> adam </strong> optimizer(可以改成experiment)编译模型。为了适应这个模型，我们使用了一个数据生成器，我在系列文章的第一篇中已经解释过了，基本思想是将示例+标签小批量地输入到模型中，而不是一次性地发送出去。这主要用于当您的训练集不适合内存时，或者当您想要在执行时进行<a class="ae ke" href="https://www.quora.com/What-does-the-term-data-Augmentation-mean-in-the-context-of-Machine-Learning" rel="noopener ugc nofollow" target="_blank">数据扩充</a>时。</p><pre class="nf ng nh ni fq nj mq nk nl aw nm dt"><span id="efb0" class="nn lf ht mq b fv no np l nq nr">model.compile(loss='binary_crossentropy', <br/>              optimizer="adam", <br/>              metrics=['accuracy'])<br/>print(model.summary())<br/><br/>model.fit_generator(generator(sentences, labels, BATCH_SIZE),<br/>                    steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,<br/>                    epochs=10,<br/>                    callbacks=callbacks_list,<br/>                    validation_data=generator(sentences_test, labels_test, BATCH_SIZE),<br/>                    validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)</span></pre><p id="e6a5" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">最后，我们调用一个函数来使用测试集上网络的最终权重，并获得一种混淆矩阵，从而获得<strong class="iz hu">真阳性、真阴性、假阳性、假阴性的数量</strong>。如果最后一个参数为真，它还将打印所有返回了<strong class="iz hu">假阴性</strong>和<strong class="iz hu">假阳性</strong>的例子，因此您可以直观地看到网络犯了什么样的错误。</p><pre class="nf ng nh ni fq nj mq nk nl aw nm dt"><span id="bc67" class="nn lf ht mq b fv no np l nq nr">confusion_matrix(sentences_test, labels_test, True)</span></pre><p id="1fb1" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">要开始训练，我们需要执行以下命令</p><pre class="nf ng nh ni fq nj mq nk nl aw nm dt"><span id="6a35" class="nn lf ht mq b fv no np l nq nr">$ python3 classifier_train.py &lt;positive_examples_file&gt; &lt;negative_examples_file&gt;</span></pre><h1 id="a828" class="le lf ht bd lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx mm lz ma mb dt translated">结论</h1><p id="120c" class="pw-post-body-paragraph ix iy ht iz b ja mc jc jd je md jg jh ji me jk jl jm mf jo jp jq mg js jt ju hm dt translated">对于所提出的情况(随机文本对来自语料库的歌词)，10个时期后的训练准确度在训练集中约为98% (acc: 0.9841)，在测试集中约为94.3% (val_acc: 0.9431)(验证)。</p><figure class="nf ng nh ni fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="nu nv di nw bf nx"><div class="fe ff nt"><img src="../Images/615ac46215d3a74819a3a9f280037f8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1hTjY3QuP4j19hd7O2J7gQ.png"/></div></div></figure><p id="c0b8" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">考虑到双方都有歧义，这是非常好的。运气(不好的话)的话，随便几行就会看起来很像真正的歌词。2.有些情况下，从语料库中提取的真实歌词看起来很像随机噪音，特别是在墨西哥班达或\_(ツ)_/雷鬼音乐中。</p><h1 id="8f36" class="le lf ht bd lg lh mi lj lk ll mj ln lo lp mk lr ls lt ml lv lw lx mm lz ma mb dt translated">额外的</h1><p id="678e" class="pw-post-body-paragraph ix iy ht iz b ja mc jc jd je md jg jh ji me jk jl jm mf jo jp jq mg js jt ju hm dt translated">只是作为一个实验，我试图运行相同的分类器训练，但标签被打乱，使用相同的例子，但随机改变它们的“正面”或“负面”标签。训练集会有什么变化？每个时期的测试/验证集呢？</p><figure class="nf ng nh ni fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="nu nv di nw bf nx"><div class="fe ff ny"><img src="../Images/b7d407d086ccbbe1bf4700ef61a46536.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aU9ygQGogq0PkvQAIZR6vw.png"/></div></div></figure><p id="9610" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">该网络能够在超过70%的范围内适应训练集(acc: 0.7148)，但是由于标签是随机的，因此在被标记为“正”与被标记为“负”之间没有真实的关系。我们可以验证这种拟合确实是无意义的，因为验证的准确性(val_acc)从未超过50%。不出所料，即使是“训练过的”网络也无法归纳出训练阶段从未见过的例子。</p><figure class="nf ng nh ni fq iu fe ff paragraph-image"><div class="fe ff nz"><img src="../Images/722a4e0de3661de1a33416be88e64c97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*_MBNdrSQmVvoylsbgDCIug.png"/></div></figure><blockquote class="oa"><p id="da2b" class="ob oc ht bd od oe of og oh oi oj ju ek translated">加入Coinmonks <a class="ae ke" href="https://t.me/coincodecap" rel="noopener ugc nofollow" target="_blank">电报频道</a>和<a class="ae ke" href="https://www.youtube.com/c/coinmonks/videos" rel="noopener ugc nofollow" target="_blank"> Youtube频道</a>获取每日<a class="ae ke" href="http://coincodecap.com/" rel="noopener ugc nofollow" target="_blank">加密新闻</a></p></blockquote><h2 id="15d6" class="nn lf ht bd lg ok ol om lk on oo op lo ji oq or ls jm os ot lw jq ou ov ma ow dt translated">另外，阅读</h2><ul class=""><li id="20fb" class="mr ms ht iz b ja mc je md ji ox jm oy jq oz ju mw mx my mz dt translated"><a class="ae ke" rel="noopener" href="/coinmonks/top-10-crypto-copy-trading-platforms-for-beginners-d0c37c7d698c">复制交易</a> | <a class="ae ke" rel="noopener" href="/coinmonks/crypto-tax-software-ed4b4810e338">加密税务软件</a></li><li id="723e" class="mr ms ht iz b ja na je nb ji nc jm nd jq ne ju mw mx my mz dt translated"><a class="ae ke" href="https://coincodecap.com/grid-trading" rel="noopener ugc nofollow" target="_blank">网格交易</a> | <a class="ae ke" rel="noopener" href="/coinmonks/the-best-cryptocurrency-hardware-wallets-of-2020-e28b1c124069">加密硬件钱包</a></li><li id="874f" class="mr ms ht iz b ja na je nb ji nc jm nd jq ne ju mw mx my mz dt translated"><a class="ae ke" href="http://Top 4 Telegram Channels for Crypto Traders" rel="noopener ugc nofollow" target="_blank">密码电报信号</a> | <a class="ae ke" rel="noopener" href="/coinmonks/crypto-trading-bot-c2ffce8acb2a">密码交易机器人</a></li><li id="f33b" class="mr ms ht iz b ja na je nb ji nc jm nd jq ne ju mw mx my mz dt translated"><a class="ae ke" rel="noopener" href="/coinmonks/crypto-exchange-dd2f9d6f3769">最佳加密交易所</a> | <a class="ae ke" rel="noopener" href="/coinmonks/bitcoin-exchange-in-india-7f1fe79715c9">最佳加密交易所</a></li><li id="47a8" class="mr ms ht iz b ja na je nb ji nc jm nd jq ne ju mw mx my mz dt translated">开发人员的最佳加密API</li><li id="b359" class="mr ms ht iz b ja na je nb ji nc jm nd jq ne ju mw mx my mz dt translated">最佳<a class="ae ke" rel="noopener" href="/coinmonks/top-5-crypto-lending-platforms-in-2020-that-you-need-to-know-a1b675cec3fa">密码借贷平台</a></li><li id="9487" class="mr ms ht iz b ja na je nb ji nc jm nd jq ne ju mw mx my mz dt translated">杠杆代币的终极指南</li></ul></div></div>    
</body>
</html>