<html>
<head>
<title>Activation Functions Demystified!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">激活功能去神秘化！</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/activation-functions-demystified-661d1183f5f8?source=collection_archive---------12-----------------------#2018-06-26">https://medium.com/coinmonks/activation-functions-demystified-661d1183f5f8?source=collection_archive---------12-----------------------#2018-06-26</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><figure class="fi fk ir is it iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff iq"><img src="../Images/abeb0dfeaff5e632a27110eb70c087ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gSRfACz04ybDmGoX"/></div></div><figcaption class="jb jc fg fe ff jd je bd b be z ek">Photo by <a class="ae jf" href="https://unsplash.com/@sharonmccutcheon?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Sharon McCutcheon</a> on <a class="ae jf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><blockquote class="jg"><p id="1733" class="jh ji ht bd jj jk jl jm jn jo jp jq ek translated">有了伟大的深度学习资源，就有了伟大的深度学习行话。</p></blockquote><p id="8f77" class="pw-post-body-paragraph jr js ht jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn jq hm dt translated">如果你已经研究深度学习有一段时间了，你一定会遇到很多与该领域相关的术语。对于初学者来说，这可能会让人感到不知所措。所以，如果你遇到了术语“<strong class="jt hu">激活功能</strong>”，并发现自己和上面的狗一样困惑，不要担心。我掩护你。</p><p id="bd79" class="pw-post-body-paragraph jr js ht jt b ju ko jw jx jy kp ka kb kc kq ke kf kg kr ki kj kk ks km kn jq hm dt translated">在我们开始讨论<em class="kt">激活功能</em>之前，让我们先了解一下<em class="kt">激活</em>到底是什么。</p><h2 id="1ca0" class="ku kv ht bd kw kx ky kz la lb lc ld le kc lf lg lh kg li lj lk kk ll lm ln lo dt translated">那么什么是激活呢？</h2><p id="2425" class="pw-post-body-paragraph jr js ht jt b ju lp jw jx jy lq ka kb kc lr ke kf kg ls ki kj kk lt km kn jq hm dt translated">一个<em class="kt">激活</em>只是一个数字。这就是激活的全部。我们知道，深度学习的核心是以下等式:</p><blockquote class="jg"><p id="e370" class="jh ji ht bd jj jk lu lv lw lx ly jq ek translated">y = W * x + b</p></blockquote><p id="51b5" class="pw-post-body-paragraph jr js ht jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn jq hm dt translated">一旦我们计算了这个等式，<code class="eh lz ma mb mc b">y</code>保持的值被称为<em class="kt">激活</em>。因此，计算这个方程的神经网络中的每个节点都拥有自己的值<code class="eh lz ma mb mc b">y</code>，所有这些值都被称为<em class="kt">激活</em>。这就是激活的全部内容。没有魔法。没有复杂的解释。没有隐藏的概念。<strong class="jt hu"> <em class="kt">简直就是一个数字。</em> </strong></p><h2 id="65c7" class="ku kv ht bd kw kx ky kz la lb lc ld le kc lf lg lh kg li lj lk kk ll lm ln lo dt translated">好吧，网上的家伙。我明白了。但是什么是激活函数呢？</h2><p id="d93c" class="pw-post-body-paragraph jr js ht jt b ju lp jw jx jy lq ka kb kc lr ke kf kg ls ki kj kk lt km kn jq hm dt translated">一个<em class="kt">激活函数</em>仅仅是一个应用于激活的数学函数，以便在我们的网络中引入一些<strong class="jt hu"> <em class="kt">非线性</em> </strong>。<em class="kt">迷茫？请继续阅读。</em></p><p id="1e2c" class="pw-post-body-paragraph jr js ht jt b ju ko jw jx jy kp ka kb kc kq ke kf kg kr ki kj kk ks km kn jq hm dt translated">一个神经网络基本上是由<em class="kt">多个线性</em>和<em class="kt">个非线性函数</em>组合而成。<code class="eh lz ma mb mc b">y = m * x + b</code>是一个线性函数。但是，如果我们的网络在所有层中都只有线性函数，那么网络将简单地充当单层网络。这样的网络不可能学到很多东西。</p><p id="9cde" class="pw-post-body-paragraph jr js ht jt b ju ko jw jx jy kp ka kb kc kq ke kf kg kr ki kj kk ks km kn jq hm dt translated">这就是为什么我们使用激活函数来产生每一层中每个节点的输出。这在我们的网络中引入了非线性，使网络能够学习。</p><blockquote class="jg"><p id="5cb6" class="jh ji ht bd jj jk lu lv lw lx ly jq ek translated">这种线性和非线性函数的结合使得神经网络能够逼近任何东西。</p></blockquote><h2 id="f7a5" class="ku kv ht bd kw kx md kz la lb me ld le kc mf lg lh kg mg lj lk kk mh lm ln lo dt translated">我可以使用不同的激活功能吗？</h2><p id="7114" class="pw-post-body-paragraph jr js ht jt b ju lp jw jx jy lq ka kb kc lr ke kf kg ls ki kj kk lt km kn jq hm dt translated">有许多不同类型的激活功能已经被使用了很多年。但是，在这篇文章中，我将解释目前在研究和行业中使用的4个最流行和最有用的激活函数，而不是全部。</p><p id="b4b8" class="pw-post-body-paragraph jr js ht jt b ju ko jw jx jy kp ka kb kc kq ke kf kg kr ki kj kk ks km kn jq hm dt translated">Sigmoid():Sigmoid激活函数将激活值转换为0到1之间的值。<em class="kt">对</em> <strong class="jt hu"> <em class="kt">二元分类问题</em> </strong> <em class="kt">有用，多用于此类问题</em>的最终输出层。此外，sigmoid激活会导致缓慢的梯度下降，因为高值和低值的斜率都很小。sigmoid激活在数学上表示为以下等式:</p><blockquote class="jg"><p id="3234" class="jh ji ht bd jj jk lu lv lw lx ly jq ek translated"><strong class="ak">Sigmoid(z)= 1/(1+exp(-z))</strong></p></blockquote><figure class="mj mk ml mm mn iu fe ff paragraph-image"><div class="fe ff mi"><img src="../Images/e1d9c383bb0c14c104fa316ddf1f0e23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*PaEjawWyMpwVttNjwswDnA.png"/></div><figcaption class="jb jc fg fe ff jd je bd b be z ek">Sigmoid activation (Source: Wikipedia)</figcaption></figure><p id="23a1" class="pw-post-body-paragraph jr js ht jt b ju ko jw jx jy kp ka kb kc kq ke kf kg kr ki kj kk ks km kn jq hm dt translated"><strong class="jt hu">Tanh():</strong>Tanh激活函数将激活转换为-1到+1之间的值。在大多数情况下，该激活函数优于sigmoid激活函数，因为输出是归一化的。这是一个非常流行的激活函数，用于隐藏层。数学上，它表示为:</p><blockquote class="jg"><p id="b02f" class="jh ji ht bd jj jk lu lv lw lx ly jq ek translated">tanh(z)=[exp(z)-exp(-z)]/[exp(z)+exp(-z)]</p></blockquote><figure class="mj mk ml mm mn iu fe ff paragraph-image"><div class="fe ff mo"><img src="../Images/74c5926809382afd178044ead37a4448.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*zxcix_umQ8yepvGYNhy5fA.png"/></div><figcaption class="jb jc fg fe ff jd je bd b be z ek">tanh activation (Source: Wikipedia)</figcaption></figure><p id="f84d" class="pw-post-body-paragraph jr js ht jt b ju ko jw jx jy kp ka kb kc kq ke kf kg kr ki kj kk ks km kn jq hm dt translated"><strong class="jt hu">ReLU():</strong>ReLU(整流线性单元)是一个真正<em class="kt">复杂且听起来很花哨的激活函数，但它所做的只是将负值变为零。这就是它的全部功能。这是<strong class="jt hu">最流行和最有效的激活函数，用于隐藏层。这是最常用的激活功能。并且是大多数神经网络层的默认选择。数学上，它表示为:</strong></em></p><blockquote class="jg"><p id="6430" class="jh ji ht bd jj jk lu lv lw lx ly jq ek translated">ReLU(z) = max(0，z)</p></blockquote><figure class="mj mk ml mm mn iu fe ff paragraph-image"><div class="fe ff mi"><img src="../Images/77f96eab2649cb0f487b896144442484.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*743vu_UFvZXB7JqLQyCENA.png"/></div><figcaption class="jb jc fg fe ff jd je bd b be z ek">ReLU activation (Source: Wikipedia)</figcaption></figure><p id="1593" class="pw-post-body-paragraph jr js ht jt b ju ko jw jx jy kp ka kb kc kq ke kf kg kr ki kj kk ks km kn jq hm dt translated"><strong class="jt hu">漏ReLU:</strong>ReLU的一个问题可能是负值的导数(斜率)为零。在某些情况下，我们可能不希望这样。为了解决这个问题，我们可以使用泄漏ReLU。泄漏的ReLU确保负值的斜率不为零。但大多数情况下，ReLU函数会做得很好。数学上，它表示为:</p><blockquote class="jg"><p id="355d" class="jh ji ht bd jj jk lu lv lw lx ly jq ek translated">Leaky_ReLU(z) = max(0.01*z，z)</p></blockquote><figure class="mj mk ml mm mn iu fe ff paragraph-image"><div class="fe ff mp"><img src="../Images/1263bcb99ec14a2a20f379a202b911fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*3ckkWaRuc_3ru_kiFZxyiQ.png"/></div><figcaption class="jb jc fg fe ff jd je bd b be z ek">Leaky ReLU activation; a=0.01 (Source: imgur)</figcaption></figure><p id="3bdd" class="pw-post-body-paragraph jr js ht jt b ju ko jw jx jy kp ka kb kc kq ke kf kg kr ki kj kk ks km kn jq hm dt translated"><em class="kt"> ReLU和Leaky ReLU </em>的<strong class="jt hu">优势</strong>是导数(斜率)远大于零<em class="kt">(对于z &gt; 0) </em>，因此，与<em class="kt"> sigmoid或tanh </em>相比，算法将学习得更快。</p><p id="d67f" class="pw-post-body-paragraph jr js ht jt b ju ko jw jx jy kp ka kb kc kq ke kf kg kr ki kj kk ks km kn jq hm dt translated">这些是激活函数，从研究到工业，到处都在使用。</p><h2 id="f0c3" class="ku kv ht bd kw kx ky kz la lb lc ld le kc lf lg lh kg li lj lk kk ll lm ln lo dt translated">但是，嘿，我怎么知道用哪一个呢？</h2><p id="bb81" class="pw-post-body-paragraph jr js ht jt b ju lp jw jx jy lq ka kb kc lr ke kf kg ls ki kj kk lt km kn jq hm dt translated">因此，选择激活函数的经验法则非常简单:</p><ol class=""><li id="01a7" class="mq mr ht jt b ju ko jy kp kc ms kg mt kk mu jq mv mw mx my dt translated">如果你需要在0和1之间选择一个值，比如在二进制分类中，那么使用Sigmoid激活函数。否则，对于所有其他情况，不要使用这个。</li><li id="4fe8" class="mq mr ht jt b ju mz jy na kc nb kg nc kk nd jq mv mw mx my dt translated">ReLU是<strong class="jt hu"> <em class="kt">默认</em> </strong>用于所有其他情况的选择。但是在某些情况下，你也可以使用tanh。如果您不确定使用哪一个，请尝试同时使用两种。</li></ol></div><div class="ab cl ne nf hb ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="hm hn ho hp hq"><p id="de86" class="pw-post-body-paragraph jr js ht jt b ju ko jw jx jy kp ka kb kc kq ke kf kg kr ki kj kk ks km kn jq hm dt translated">关于激活函数就是这样。他们被用来使网络非线性，你有几个选择，你可以使用哪个激活功能。我希望我已经成功地为你揭开了激活函数的神秘面纱，你现在对它们有了更好的理解。</p><p id="4f13" class="pw-post-body-paragraph jr js ht jt b ju ko jw jx jy kp ka kb kc kq ke kf kg kr ki kj kk ks km kn jq hm dt translated">有任何问题、反馈或批评吗？在 <a class="ae jf" href="https://twitter.com/iamJYash" rel="noopener ugc nofollow" target="_blank"> <strong class="jt hu"> <em class="kt">推特</em> </strong> </a> <strong class="jt hu">上留言或联系我！</strong></p></div></div>    
</body>
</html>