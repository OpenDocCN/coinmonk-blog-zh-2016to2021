<html>
<head>
<title>Machine Learning Tutorial #2: Training</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习教程#2:训练</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/machine-learning-tutorial-2-training-f6f735830838?source=collection_archive---------3-----------------------#2018-08-12">https://medium.com/coinmonks/machine-learning-tutorial-2-training-f6f735830838?source=collection_archive---------3-----------------------#2018-08-12</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><div class=""><h2 id="43cc" class="pw-subtitle-paragraph iq hs ht bd b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ek translated">主题:性能度量，交叉验证，模型选择，超参数优化，项目反映，工具</h2></div><figure class="jj jk jl jm fq jn fe ff paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="fe ff ji"><img src="../Images/688f0991ea5b97411e4be5f98ba6db22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iPgIcpnc-nzkigs6RaTZBw.png"/></div></div><figcaption class="ju jv fg fe ff jw jx bd b be z ek">Machine Learning project overview. Author: Adam Novotny</figcaption></figure><p id="68b0" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">ML教程的第二部分是第一部分<a class="ae ku" rel="noopener" href="/@adam5ny/machine-learning-tutorial-1-preprocessing-d90198e37577">预处理</a>的后续部分。所有代码都可以在这个<a class="ae ku" href="https://github.com/adam5ny/blogs/tree/master/ml-training" rel="noopener ugc nofollow" target="_blank"> Github repo </a>中获得。本系列其他教程:<a class="ae ku" rel="noopener" href="/coinmonks/machine-learning-tutorial-1-preprocessing-d90198e37577"> #1预处理</a>、#2训练(本文)、<a class="ae ku" rel="noopener" href="/@adam5ny/machine-learning-tutorial-3-evaluation-a157f90914c9"> #3评测</a>、<a class="ae ku" rel="noopener" href="/@adam5ny/machine-learning-tutorial-4-deployment-79764123e9e1"> #4预测</a></p><p id="5e8d" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">我用4个数据集结束了教程#1:训练特征、测试特征、训练目标变量和测试目标变量。在本教程#2中，将只使用培训功能和培训目标变量。测试数据将在教程#3中用于评估目的。</p><h1 id="d418" class="kv kw ht bd kx ky kz la lb lc ld le lf iz lg ja lh jc li jd lj jf lk jg ll lm dt translated">性能指标</h1><p id="d542" class="pw-post-body-paragraph jy jz ht ka b kb ln iu kd ke lo ix kg kh lp kj kk kl lq kn ko kp lr kr ks kt hm dt translated">我们专注于回归算法，因此我将考虑3个最常用的性能指标</p><ul class=""><li id="c5b4" class="ls lt ht ka b kb kc ke kf kh lu kl lv kp lw kt lx ly lz ma dt translated"><a class="ae ku" href="https://en.wikipedia.org/wiki/Mean_absolute_error" rel="noopener ugc nofollow" target="_blank">平均绝对误差</a> (MAE)</li><li id="434f" class="ls lt ht ka b kb mb ke mc kh md kl me kp mf kt lx ly lz ma dt translated"><a class="ae ku" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank">均方误差</a> (MSE)</li><li id="0392" class="ls lt ht ka b kb mb ke mc kh md kl me kp mf kt lx ly lz ma dt translated"><a class="ae ku" href="https://en.wikipedia.org/wiki/Coefficient_of_determination" rel="noopener ugc nofollow" target="_blank"> R </a></li></ul><p id="57a6" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">在实践中，可以做出特定于领域的决策来补充上面的标准度量。例如，投资者通常更关心重大的下跌错误，而不是上涨错误。因此，可能会得出过度强调与财务损失相对应的下行误差的指标。</p><h1 id="ccdc" class="kv kw ht bd kx ky kz la lb lc ld le lf iz lg ja lh jc li jd lj jf lk jg ll lm dt translated">交互效度分析</h1><p id="5bfc" class="pw-post-body-paragraph jy jz ht ka b kb ln iu kd ke lo ix kg kh lp kj kk kl lq kn ko kp lr kr ks kt hm dt translated">我将回到我在<a class="ae ku" rel="noopener" href="/@adam5ny/machine-learning-tutorial-1-preprocessing-d90198e37577">预处理</a>中提到的同一主题。由于时间序列数据的性质，标准随机K倍验证会产生前瞻性偏差，因此不应使用。为了说明这个问题，让我们假设我们将8年的数据分成8份，每份代表一年。第一个训练周期将使用折叠# 1–7进行训练，折叠#8进行测试。下一个训练周期可能使用折叠# 2–8进行训练，使用折叠#1进行测试。这当然是不可接受的，因为我们使用第2-7年的数据来预测第1年。</p><p id="ba27" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">我们的交叉验证必须尊重数据的时间顺序。我们可以使用步行验证或简单的多次训练测试分割。为了说明，我将使用3个训练测试分割。例如，假设我们有2000个样本，从最早的开始按时间戳排序。我们的3个部分如下所示:</p><figure class="jj jk jl jm fq jn fe ff paragraph-image"><div class="fe ff mg"><img src="../Images/7f6d524a4c94900226e828241bf9bdb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*cFti5rqcbFrE5p_4My4eww.png"/></div><figcaption class="ju jv fg fe ff jw jx bd b be z ek">Train-Test splits. Author: Adam Novotny</figcaption></figure><h1 id="cb97" class="kv kw ht bd kx ky kz la lb lc ld le lf iz lg ja lh jc li jd lj jf lk jg ll lm dt translated">型号选择</h1><figure class="jj jk jl jm fq jn fe ff paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="fe ff mh"><img src="../Images/7a9bce6f1e5bd3bb7a63330262e4a0b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M2clZWay68ODL2jEB-J5Dw.png"/></div></div><figcaption class="ju jv fg fe ff jw jx bd b be z ek">ML Model Selection. Author: Adam Novotny</figcaption></figure><p id="0519" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">在这一部分，我将选择要培训的模型。“受监督的”算法部分(上图中的红色部分)是相关的，因为数据集包含要素和标签(目标变量)。谈到算法选择，我喜欢遵循<a class="ae ku" href="https://en.wikipedia.org/wiki/Occam%27s_razor" rel="noopener ugc nofollow" target="_blank">奥卡姆剃刀</a>。换句话说，从表现出最快的训练时间和最好的可解释性的算法开始。然后我们可以增加复杂性。</p><p id="30b5" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">在本节中，我将探讨以下算法:</p><ul class=""><li id="2519" class="ls lt ht ka b kb kc ke kf kh lu kl lv kp lw kt lx ly lz ma dt translated">线性回归:快速学习，易于解释</li><li id="6f5a" class="ls lt ht ka b kb mb ke mc kh md kl me kp mf kt lx ly lz ma dt translated">决策树:学习速度快(需要修剪)，易于解释</li><li id="387c" class="ls lt ht ka b kb mb ke mc kh md kl me kp mf kt lx ly lz ma dt translated">神经网络:学习缓慢，难以解释</li></ul><h2 id="ca74" class="mi kw ht bd kx mj mk ml lb mm mn mo lf kh mp mq lh kl mr ms lj kp mt mu ll mv dt translated">线性回归</h2><p id="ecfa" class="pw-post-body-paragraph jy jz ht ka b kb ln iu kd ke lo ix kg kh lp kj kk kl lq kn ko kp lr kr ks kt hm dt translated">从线性回归开始有助于了解我们是否可以在进入复杂的机器学习算法之前，通过简单的统计数据来实现我们的目标。具有明确定义的特征的房价预测就是一个例子，其中线性回归通常工作良好，并且没有必要使用更复杂的算法。</p><p id="5675" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">使用sklearn训练线性回归模型很简单:</p><pre class="jj jk jl jm fq mw mx my mz aw na dt"><span id="ff1a" class="mi kw ht mx b fv nb nc l nd ne">from sklearn import linear_model<br/>model = linear_model.LinearRegression()<br/>model.fit(X_train, y_train)<br/>y_pred = model.predict(X_test)</span></pre><p id="5549" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">最初的结果没有产生任何有希望的东西，所以我采取了另一个步骤，进一步转换特性。我创建了多项式和非线性特征来解释非线性关系。例如，在2次多项式的情况下，特征[a，b ]变成[1，a，b，a，ab，b]。</p><figure class="jj jk jl jm fq jn fe ff paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="fe ff nf"><img src="../Images/8b50459a294b801d0fd8d03625ae8d80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c9-D9EJoTwKsRlJuK_WQaQ.png"/></div></div><figcaption class="ju jv fg fe ff jw jx bd b be z ek">Linear Regression results. Author: Adam Novotny</figcaption></figure><p id="8223" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">x轴代表3个交叉验证区段(第一次折叠使用1749个样本用于训练，1749个样本用于测试，第二次折叠使用3499个样本用于训练，1749个样本用于测试，最后一次折叠使用5249个样本用于训练，1749个样本用于测试)。显然，结果表明线性模型在实践中是没有用的。在这个阶段，我至少有以下选择:</p><ul class=""><li id="a280" class="ls lt ht ka b kb kc ke kf kh lu kl lv kp lw kt lx ly lz ma dt translated">岭回归:解决过度拟合(如果有的话)</li><li id="52f2" class="ls lt ht ka b kb mb ke mc kh md kl me kp mf kt lx ly lz ma dt translated">线性套索:降低模型复杂性</li></ul><p id="cd14" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">在这一点上，我不认为上述任何选项会对结果产生有意义的影响。我将继续其他算法，看看他们如何比较。</p><p id="4a43" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">然而，在继续之前，我需要设定期望值。金融界有一句话，成功的预测者只需要51%的时间是正确的。财务杠杆可以用来放大结果，所以只要有一点点正确就会产生有影响力的结果。这就设定了期望值，因为我们永远不会在这个领域找到60%正确或更好的算法。因此，我们预计R值较低。这一点需要说明，因为机器学习中的很多样本项目都是为了好看而设计的，这是我们在现实生活的价格预测中永远无法企及的。</p><h2 id="6110" class="mi kw ht bd kx mj mk ml lb mm mn mo lf kh mp mq lh kl mr ms lj kp mt mu ll mv dt translated">决策图表</h2><p id="91ab" class="pw-post-body-paragraph jy jz ht ka b kb ln iu kd ke lo ix kg kh lp kj kk kl lq kn ko kp lr kr ks kt hm dt translated">使用sklearn训练决策树回归模型同样简单:</p><pre class="jj jk jl jm fq mw mx my mz aw na dt"><span id="bed1" class="mi kw ht mx b fv nb nc l nd ne">from sklearn import tree<br/>model = tree.DecisionTreeRegressor()<br/>model.fit(X_train, y_train)<br/>y_pred = model.predict(X_test)</span></pre><p id="a75a" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">上述拟合函数的默认结果几乎总是<a class="ae ku" href="https://en.wikipedia.org/wiki/Overfitting" rel="noopener ugc nofollow" target="_blank">过度拟合</a>。决策树有一个非常有表达力的假设空间，所以在没有修剪的情况下，它们可以代表几乎任何函数。训练数据的r很容易变成完美的1.0，而测试数据的结果将是0。因此，我们需要使用scikit-learn的max_depth参数<a class="ae ku" href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor" rel="noopener ugc nofollow" target="_blank"> DecisionTreeRegressor </a>来确保该树能够很好地概括测试数据。</p><p id="d4ee" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">决策树的最大优势之一是它们的可解释性:参见许多使用标准说明性数据集的有用的<a class="ae ku" rel="noopener" href="/@rnbrown/creating-and-visualizing-decision-trees-with-python-f8e8fa394176">可视化文章</a>。</p><h2 id="e08a" class="mi kw ht bd kx mj mk ml lb mm mn mo lf kh mp mq lh kl mr ms lj kp mt mu ll mv dt translated">神经网络</h2><p id="61f6" class="pw-post-body-paragraph jy jz ht ka b kb ln iu kd ke lo ix kg kh lp kj kk kl lq kn ko kp lr kr ks kt hm dt translated">Scikit-learn使<a class="ae ku" href="http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor" rel="noopener ugc nofollow" target="_blank">简单的神经网络</a>训练就像构建决策树一样简单:</p><pre class="jj jk jl jm fq mw mx my mz aw na dt"><span id="6435" class="mi kw ht mx b fv nb nc l nd ne">from sklearn.neural_network import MLPRegressor<br/>model = MLPRegressor(hidden_layer_sizes=(200, 200), solver="lbfgs", activation="relu")<br/>model.fit(X_train, y_train)<br/>y_pred = model.predict(X_test)</span></pre><p id="7dee" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">在普通笔记本电脑上，训练一个具有2个隐藏层(每个隐藏层200个单元)和多项式特征的神经网络需要几十秒钟。为了加快下一节的培训过程，我将不再使用scikit-learn，而是将<a class="ae ku" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>用于TensorFlow后端。</p><p id="4250" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">Keras API同样简单。该项目甚至包括scikit-learn 的<a class="ae ku" href="https://keras.io/scikit-learn-api/#wrappers-for-the-scikit-learn-api" rel="noopener ugc nofollow" target="_blank">包装器，以利用scikit的研究库。</a></p><pre class="jj jk jl jm fq mw mx my mz aw na dt"><span id="d0e2" class="mi kw ht mx b fv nb nc l nd ne">from keras.models import Sequential<br/>from keras.layers import Dense<br/>model = Sequential()<br/>input_size = len(X[0])<br/>model.add(Dense(200, activation="relu", input_dim=input_size))<br/>model.add(Dense(200, activation="relu"))<br/>model.add(Dense(1, activation="linear"))<br/>model.compile(optimizer="adam", loss="mse")<br/>model.fit(X_train, y_train, epochs=25, verbose=1)<br/>y_pred = model.predict(X_test)</span></pre><h1 id="c74b" class="kv kw ht bd kx ky kz la lb lc ld le lf iz lg ja lh jc li jd lj jf lk jg ll lm dt translated">超参数优化</h1><p id="429e" class="pw-post-body-paragraph jy jz ht ka b kb ln iu kd ke lo ix kg kh lp kj kk kl lq kn ko kp lr kr ks kt hm dt translated">做超参数优化的诀窍是理解参数不应该被单独对待。许多参数相互影响，这就是为什么经常进行彻底的<a class="ae ku" href="https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search" rel="noopener ugc nofollow" target="_blank">网格搜索</a>。然而，网格搜索是，它变得非常昂贵。</p><h2 id="f412" class="mi kw ht bd kx mj mk ml lb mm mn mo lf kh mp mq lh kl mr ms lj kp mt mu ll mv dt translated">决策图表</h2><p id="33f1" class="pw-post-body-paragraph jy jz ht ka b kb ln iu kd ke lo ix kg kh lp kj kk kl lq kn ko kp lr kr ks kt hm dt translated">我们的决策树网格搜索将迭代以下输入:</p><ul class=""><li id="d438" class="ls lt ht ka b kb kc ke kf kh lu kl lv kp lw kt lx ly lz ma dt translated">拆分器:用于拆分节点的策略(最佳或随机)</li><li id="522c" class="ls lt ht ka b kb mb ke mc kh md kl me kp mf kt lx ly lz ma dt translated">树的最大深度</li><li id="a09d" class="ls lt ht ka b kb mb ke mc kh md kl me kp mf kt lx ly lz ma dt translated">每次分割的最小样本数:分割内部节点所需的最小样本数</li><li id="8b7e" class="ls lt ht ka b kb mb ke mc kh md kl me kp mf kt lx ly lz ma dt translated">最大叶节点:数量或无(允许无限数量的叶节点)</li></ul><p id="2cf1" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">说明性的网格搜索结果如下:</p><figure class="jj jk jl jm fq jn fe ff paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="fe ff ng"><img src="../Images/1f0ddc1889eaf1b2ac49396885295615.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CQHDbOr3_ZO7oWkdOYdKQw.png"/></div></div><figcaption class="ju jv fg fe ff jw jx bd b be z ek">Grid Search Decision Tree — first rows</figcaption></figure><figure class="jj jk jl jm fq jn fe ff paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="fe ff nh"><img src="../Images/330fe223334d0d4b4d705da0f74ff467.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DQHZ9reWIOMF6fq9eKA8IQ.png"/></div></div><figcaption class="ju jv fg fe ff jw jx bd b be z ek">Grid Search Decision Tree — last rows</figcaption></figure><p id="7713" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">使用最佳参数的性能:</p><figure class="jj jk jl jm fq jn fe ff paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="fe ff nf"><img src="../Images/4d15bf2ec02910533960155959846a88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2qHu4Z1DiJGmx440QCyTAA.png"/></div></div><figcaption class="ju jv fg fe ff jw jx bd b be z ek">Decision Tree results</figcaption></figure><p id="2289" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">同样，结果似乎不太乐观。它们似乎比线性回归更好(较低的MAE和MSE ),但是R仍然太低而没有用。然而，我的结论是，决策树更强的表达能力是有用的，在这个阶段我会放弃线性回归模型。</p><h2 id="3da0" class="mi kw ht bd kx mj mk ml lb mm mn mo lf kh mp mq lh kl mr ms lj kp mt mu ll mv dt translated">神经网络</h2><p id="5a71" class="pw-post-body-paragraph jy jz ht ka b kb ln iu kd ke lo ix kg kh lp kj kk kl lq kn ko kp lr kr ks kt hm dt translated">探索由Keras建立的神经网络的超参数，我们至少可以改变以下参数:</p><ul class=""><li id="7eea" class="ls lt ht ka b kb kc ke kf kh lu kl lv kp lw kt lx ly lz ma dt translated">每层中隐藏层和/或单元的数量</li><li id="b20b" class="ls lt ht ka b kb mb ke mc kh md kl me kp mf kt lx ly lz ma dt translated">模型<a class="ae ku" href="https://keras.io/optimizers/" rel="noopener ugc nofollow" target="_blank">优化器</a> (SGD、Adam等)</li><li id="102f" class="ls lt ht ka b kb mb ke mc kh md kl me kp mf kt lx ly lz ma dt translated"><a class="ae ku" href="https://keras.io/activations/" rel="noopener ugc nofollow" target="_blank">每层的激活功能</a>(relu，tanh)</li><li id="6691" class="ls lt ht ka b kb mb ke mc kh md kl me kp mf kt lx ly lz ma dt translated">批次大小:每次梯度更新的样本数</li><li id="0f13" class="ls lt ht ka b kb mb ke mc kh md kl me kp mf kt lx ly lz ma dt translated">要训练的时期:整个训练数据集的迭代次数</li></ul><p id="efd2" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">说明性的网格搜索结果如下:</p><figure class="jj jk jl jm fq jn fe ff paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="fe ff ni"><img src="../Images/df8b84362d0e499b961a5bf07deb460c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c_fhFAu5NkphQXM6Do8QXQ.png"/></div></div><figcaption class="ju jv fg fe ff jw jx bd b be z ek">Grid Search Neural Net — first rows</figcaption></figure><figure class="jj jk jl jm fq jn fe ff paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="fe ff ni"><img src="../Images/661007e245bfd27aa53787ed144b5387.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CZIYuZ9UrEdoVkWhOtfIWQ.png"/></div></div><figcaption class="ju jv fg fe ff jw jx bd b be z ek">Grid Search Neural Net — last rows</figcaption></figure><p id="2e19" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">使用最佳参数，我们获得以下性能指标:</p><figure class="jj jk jl jm fq jn fe ff paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="fe ff nf"><img src="../Images/d2d57edca90d565dd1ccfd032585b908.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*auWhs2uGbBher9adskreog.png"/></div></div><figcaption class="ju jv fg fe ff jw jx bd b be z ek">Keras MAE, MSE, R2</figcaption></figure><p id="20e2" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">神经网络和决策树的结果是相似的，这是常见的。这两种算法都有非常有表现力的假设空间，并且经常产生可比较的结果。如果我取得了可比较的结果，我倾向于使用决策树模型，因为它的训练时间更快，可解释性更强。</p><h1 id="0c68" class="kv kw ht bd kx ky kz la lb lc ld le lf iz lg ja lh jc li jd lj jf lk jg ll lm dt translated">项目反思</h1><p id="7c43" class="pw-post-body-paragraph jy jz ht ka b kb ln iu kd ke lo ix kg kh lp kj kk kl lq kn ko kp lr kr ks kt hm dt translated">在这个阶段，很明显没有模型可以用于生产。虽然决策树模型看起来表现最好，但它在测试数据上的表现仍然不可靠。在这一阶段，是时候返回去寻找额外的特征和/或数据源了。</p><p id="fd5f" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">正如我在第一篇<a class="ae ku" rel="noopener" href="/@adam5ny/machine-learning-tutorial-1-preprocessing-d90198e37577">预处理教程</a>中提到的，金融从业者可能会花几个月的时间来获取数据和构建特性。特定领域的知识至关重要，我认为金融市场至少展示了有效市场假说的弱式。这意味着未来的股票回报不能从过去的价格变动中预测。我只使用了过去的价格变动来开发上述模型，因此从业者在第一个教程中就会注意到结果并不乐观。</p><p id="d8b7" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">为了完成本教程，我将继续保存决策树模型，并在本教程的下一节中将其用于演示目的(就像它是最终的生产模型一样):</p><pre class="jj jk jl jm fq mw mx my mz aw na dt"><span id="1a38" class="mi kw ht mx b fv nb nc l nd ne">pickle.dump(model, open("dtree_model.pkl", "wb"))</span></pre><p id="fee2" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">重要提示:Python pickle库中有<a class="ae ku" href="https://www.cs.uic.edu/~s/musings/pickle/" rel="noopener ugc nofollow" target="_blank">个已知安全漏洞</a>。为了安全起见，关键的一点是永远不要取消对非您创建的数据的检查。</p><h1 id="f8d3" class="kv kw ht bd kx ky kz la lb lc ld le lf iz lg ja lh jc li jd lj jf lk jg ll lm dt translated">工具</h1><p id="384d" class="pw-post-body-paragraph jy jz ht ka b kb ln iu kd ke lo ix kg kh lp kj kk kl lq kn ko kp lr kr ks kt hm dt translated">工具是一个常见的问题，但通常并不重要，直到项目由成千上万的例子和至少数百个特性组成。我通常从scikit开始——学习，当性能成为瓶颈时就转移到其他地方。例如，<a class="ae ku" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>不仅是一个深度学习框架，还包含其他算法，如<a class="ae ku" href="https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor" rel="noopener ugc nofollow" target="_blank"> LinearRegressor </a>。如果scikit-learn执行得不够好，我们可以用TensorFlow和GPU训练上面的线性回归。</p><p id="b087" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">本系列其他教程:<a class="ae ku" rel="noopener" href="/coinmonks/machine-learning-tutorial-1-preprocessing-d90198e37577"> #1预处理</a>、#2训练(本文)、<a class="ae ku" rel="noopener" href="/@adam5ny/machine-learning-tutorial-3-evaluation-a157f90914c9"> #3评测</a>、<a class="ae ku" rel="noopener" href="/@adam5ny/machine-learning-tutorial-4-deployment-79764123e9e1"> #4预测</a></p><p id="d7ad" class="pw-post-body-paragraph jy jz ht ka b kb kc iu kd ke kf ix kg kh ki kj kk kl km kn ko kp kq kr ks kt hm dt translated">作者网址:<a class="ae ku" href="https://www.adamnovotny.com/" rel="noopener ugc nofollow" target="_blank">adamnovotny.com</a></p></div></div>    
</body>
</html>