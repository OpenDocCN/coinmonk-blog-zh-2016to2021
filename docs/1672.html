<html>
<head>
<title>Solving Curious case of MountainCar reward problem using OpenAI Gym, Keras, TensorFlow in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python中的OpenAI Gym，Keras，TensorFlow解决登山车奖励问题的奇案</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/solving-curious-case-of-mountaincar-reward-problem-using-openai-gym-keras-tensorflow-in-python-d031c471b346?source=collection_archive---------3-----------------------#2018-10-19">https://medium.com/coinmonks/solving-curious-case-of-mountaincar-reward-problem-using-openai-gym-keras-tensorflow-in-python-d031c471b346?source=collection_archive---------3-----------------------#2018-10-19</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><figure class="fi fk iq ir is it fe ff paragraph-image"><div class="ab fr cl iu"><img src="../Images/7ef4cb708af5da6517c0cb0ce32b46fa.png" data-original-src="https://miro.medium.com/v2/1*nbCSvWmyS_BUDz_WAJyKUw.gif"/></div></figure><p id="a222" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">这篇文章将帮助你使用OpenAI Gym和TensorFlow为MountainCar等回报较少的游戏编写游戏机器人。</p><p id="9f6e" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">一旦我建立了一个玩钢管舞游戏的模型，我觉得很有信心，并认为让我们为另一个游戏编写代码，并发现登山车游戏很有趣，然后我想为什么不为它编写代码。</p><p id="5a43" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">一旦我开始写作，我就意识到这不是一件容易的事情。最大的问题是它总是给出一个负的奖励，无论我采取什么随机行动都无关紧要，我最终得到了总分<strong class="iz hu"> -200 </strong>，并最终输掉了比赛。我检查了不同的文章，尝试了不同的方法，但没有找到正确的答案。</p><p id="8f4c" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">在看了这么多地方之后，我意识到与其依赖游戏给出的奖励，为什么不根据具体情况自己创造一个呢？这解决了我的问题。我想和每个人分享，这样就没有人会经历我所经历的痛苦。</p></div><div class="ab cl jv jw hb jx" role="separator"><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka"/></div><div class="hm hn ho hp hq"><p id="7169" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">不要浪费太多时间，让我们开始编码吧。如果你是第一次尝试OpenAI健身房，请在这里阅读我的<a class="ae kc" rel="noopener" href="/@ashok.tankala/build-your-first-ai-game-bot-using-openai-gym-keras-tensorflow-in-python-50a4d4296687">以前的文章</a>。</p><p id="b353" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">首先，让我们导入实现它所需的包</p><pre class="kd ke kf kg fq kh ki kj kk aw kl dt"><span id="9d26" class="km kn ht ki b fv ko kp l kq kr">import gym<br/>import random<br/>import numpy as np<br/>from keras.models     import Sequential<br/>from keras.layers     import Dense<br/>from keras.optimizers import Adam</span></pre><p id="7111" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">让我们创建环境并初始化变量</p><pre class="kd ke kf kg fq kh ki kj kk aw kl dt"><span id="ecba" class="km kn ht ki b fv ko kp l kq kr">env = gym.make('MountainCar-v0')<br/>env.reset()<br/>goal_steps = 200<br/>score_requirement = -198<br/>intial_games = 10000</span></pre><p id="a073" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">在我们开始写代码之前，让我们先了解一下我们将要进入的内容</p><pre class="kd ke kf kg fq kh ki kj kk aw kl dt"><span id="6ea4" class="km kn ht ki b fv ko kp l kq kr">def play_a_random_game_first():<br/>    for step_index in range(goal_steps):<br/>        env.render()<br/>        action = env.action_space.sample()<br/>        observation, reward, done, info = env.step(action)<br/>        print("Step {}:".format(step_index))<br/>        print("action: {}".format(action))<br/>        print("observation: {}".format(observation))<br/>        print("reward: {}".format(reward))<br/>        print("done: {}".format(done))<br/>        print("info: {}".format(info))<br/>        if done:<br/>            break<br/>    env.reset()</span></pre><p id="0cab" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">如果执行这段代码，您将得到如下输出</p><pre class="kd ke kf kg fq kh ki kj kk aw kl dt"><span id="19a0" class="km kn ht ki b fv ko kp l kq kr">Step 0:<br/>action: 0<br/>observation: [-0.55321127 -0.00078406]<br/>reward: -1.0<br/>done: False<br/>info: {}<br/>Step 1:<br/>action: 1<br/>observation: [-0.55377353 -0.00056225]<br/>reward: -1.0<br/>done: False<br/>info: {}<br/>...<br/>Step 198:<br/>action: 0<br/>observation: [-0.40182971  0.01383677]<br/>reward: -1.0<br/>done: False<br/>info: {}<br/>Step 199:<br/>action: 1<br/>observation: [-0.38888603  0.01294368]<br/>reward: -1.0<br/>done: True<br/>info: {}</span></pre><p id="af2c" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">根据文档"-1，每个时间步长，直到达到0.5的目标位置。和MountainCarContinuous v0一样，爬左边的山是没有惩罚的，到达左边的山就像一堵墙。”</p><p id="59cc" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">当您到达0.5(顶部)位置时，或者达到200次迭代时，剧集结束。我玩了10000次几次，但从来没有达到最高位置。所以在数据填充的时候，我改了一个小逻辑终于给了我解决方案。</p><p id="fed0" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">数据填充的代码是</p><figure class="kd ke kf kg fq it"><div class="bz el l di"><div class="ks kt l"/></div></figure><p id="1bce" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">关键部分在于上面的代码，让我们逐行理解，我将解释帮助我解决这个问题的调整。</p><ol class=""><li id="e12e" class="ku kv ht iz b ja jb je jf ji kw jm kx jq ky ju kz la lb lc dt translated">我们初始化了training_data和accepted_scores数组。</li><li id="db11" class="ku kv ht iz b ja ld je le ji lf jm lg jq lh ju kz la lb lc dt translated">我们需要玩多次，这样我们可以收集数据，我们可以进一步使用。因此，我们将播放10000次，以便获得大量数据。这一行表示“游戏索引在范围内(初始游戏):”</li><li id="9d77" class="ku kv ht iz b ja ld je le ji lf jm lg jq lh ju kz la lb lc dt translated">我们初始化了score、game_memory、previous_observation变量，这些变量将存储当前游戏的总得分和前一步观察(指汽车的位置和速度)以及我们为此采取的行动。</li><li id="150d" class="ku kv ht iz b ja ld je le ji lf jm lg jq lh ju kz la lb lc dt translated">对于范围内的step _ index(goal _ steps):—此代码将游戏进行200步，因为当您到达0.5(顶部)位置时，或者如果达到200次迭代，则一集结束。</li><li id="0043" class="ku kv ht iz b ja ld je le ji lf jm lg jq lh ju kz la lb lc dt translated">我们需要采取随机行动，这样我们就可以玩游戏，这可能会导致成功完成该步骤或输掉游戏。这里只允许3个动作:向左推(0)，不推(1)和向右推(2)。所以这段代码(random.randrange(0，3))用于采取随机操作之一。</li><li id="c859" class="ku kv ht iz b ja ld je le ji lf jm lg jq lh ju kz la lb lc dt translated">我们将采取那个行动/步骤。然后，我们将检查它是否是第一个行动/步骤，然后我们将存储之前的观察和我们为此采取的行动。</li><li id="4e8f" class="ku kv ht iz b ja ld je le ji lf jm lg jq lh ju kz la lb lc dt translated">然后，我们将检查观察值为[0]的汽车的位置是否大于-0.2，如果是，那么我不接受我们的游戏环境给出的奖励，而是接受1，因为-0.2的位置是山顶，这意味着我们的随机行动产生了一些丰硕的成果。</li><li id="91b4" class="ku kv ht iz b ja ld je le ji lf jm lg jq lh ju kz la lb lc dt translated">将奖励加到分数上，检查游戏是否完成，如果是，则停止游戏。</li><li id="97d6" class="ku kv ht iz b ja ld je le ji lf jm lg jq lh ju kz la lb lc dt translated">我们将检查这个游戏是否满足我们的最低要求，这意味着我们是否能够得到大于或等于-198的分数。</li><li id="3edb" class="ku kv ht iz b ja ld je le ji lf jm lg jq lh ju kz la lb lc dt translated">如果我们能够得到大于或等于-198的分数，那么我们将把这个分数添加到accept_scores中，我们进一步打印这个分数，以了解我们将多少游戏数据及其分数输入到我们的模型中。</li><li id="6d8d" class="ku kv ht iz b ja ld je le ji lf jm lg jq lh ju kz la lb lc dt translated">然后我们将对动作进行热编码，因为它的值0(向左推)、1(不推)、2(向右推)代表分类数据。</li><li id="6257" class="ku kv ht iz b ja ld je le ji lf jm lg jq lh ju kz la lb lc dt translated">然后我们会将它添加到我们的训练数据中。</li><li id="19ac" class="ku kv ht iz b ja ld je le ji lf jm lg jq lh ju kz la lb lc dt translated">我们将重置环境，以确保一切都清楚，开始玩下一个游戏。</li><li id="1c0d" class="ku kv ht iz b ja ld je le ji lf jm lg jq lh ju kz la lb lc dt translated">print(accepted_scores) —该代码用于了解我们向模型中输入了多少游戏数据及其分数。然后我们会返回训练数据。</li></ol><p id="2271" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">我们将得到一些合理的游戏分数如下</p><pre class="kd ke kf kg fq kh ki kj kk aw kl dt"><span id="0917" class="km kn ht ki b fv ko kp l kq kr">[-158.0, -172.0, -188.0, -196.0, -168.0, -182.0, -180.0, -184.0, -184.0, -184.0, -168.0, -184.0, -176.0, -182.0, -182.0, -196.0, -184.0, -194.0, -178.0, -176.0, -170.0, -190.0, -182.0, -184.0, -184.0, -188.0, -184.0, -192.0, -172.0, -186.0, -174.0, -166.0, -188.0, -186.0, -174.0, -190.0, -178.0, -170.0, -164.0, -180.0, -184.0, -172.0, -168.0, -174.0, -172.0, -174.0, -186.0]</span></pre><p id="6880" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">所以我们的数据准备好了。是时候建立我们的神经网络了。</p><pre class="kd ke kf kg fq kh ki kj kk aw kl dt"><span id="62f2" class="km kn ht ki b fv ko kp l kq kr">def build_model(input_size, output_size):<br/>    model = Sequential()<br/>    model.add(Dense(128, input_dim=input_size, activation='relu'))<br/>    model.add(Dense(52, activation='relu'))<br/>    model.add(Dense(output_size, activation='linear'))<br/>    model.compile(loss='mse', optimizer=Adam())</span><span id="9247" class="km kn ht ki b fv li kp l kq kr">return model</span></pre><p id="b77c" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">这里我们将使用序列模型。</p><pre class="kd ke kf kg fq kh ki kj kk aw kl dt"><span id="79a1" class="km kn ht ki b fv ko kp l kq kr">def train_model(training_data):<br/>    X = np.array([i[0] for i in training_data]).reshape(-1, len(training_data[0][0]))<br/>    y = np.array([i[1] for i in training_data]).reshape(-1, len(training_data[0][1]))<br/>    model = build_model(input_size=len(X[0]), output_size=len(y[0]))<br/>    <br/>    model.fit(X, y, epochs=5)<br/>    return model</span></pre><p id="cec4" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">我们有训练数据，因此，我们将创建功能和标签。</p><p id="837e" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">然后我们将开始训练</p><pre class="kd ke kf kg fq kh ki kj kk aw kl dt"><span id="3380" class="km kn ht ki b fv ko kp l kq kr">trained_model = train_model(training_data)</span></pre><p id="fdb5" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">我们将得到这样的输出</p><pre class="kd ke kf kg fq kh ki kj kk aw kl dt"><span id="10ee" class="km kn ht ki b fv ko kp l kq kr">Epoch 1/5<br/>9353/9353 [==============================] - 1s 90us/step - loss: 0.2262<br/>Epoch 2/5<br/>9353/9353 [==============================] - 1s 66us/step - loss: 0.2217<br/>Epoch 3/5<br/>9353/9353 [==============================] - 1s 65us/step - loss: 0.2209<br/>Epoch 4/5<br/>9353/9353 [==============================] - 1s 64us/step - loss: 0.2201<br/>Epoch 5/5<br/>9353/9353 [==============================] - 1s 61us/step - loss: 0.2199</span></pre><p id="e3b1" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">是时候让我们的游戏机器人为我们玩游戏了。</p><pre class="kd ke kf kg fq kh ki kj kk aw kl dt"><span id="7a86" class="km kn ht ki b fv ko kp l kq kr">scores = []<br/>choices = []<br/>for each_game in range(100):<br/>    score = 0<br/>    game_memory = []<br/>    prev_obs = []<br/>    for step_index in range(goal_steps):<br/>        env.render()<br/>        if len(prev_obs)==0:<br/>            action = random.randrange(0,2)<br/>        else:<br/>            action = np.argmax(trained_model.predict(prev_obs.reshape(-1, len(prev_obs)))[0])<br/>        <br/>        choices.append(action)<br/>        new_observation, reward, done, info = env.step(action)<br/>        prev_obs = new_observation<br/>        game_memory.append([new_observation, action])<br/>        score += reward<br/>        if done:<br/>            break</span><span id="41e7" class="km kn ht ki b fv li kp l kq kr">env.reset()<br/>    scores.append(score)</span><span id="19b6" class="km kn ht ki b fv li kp l kq kr">print(scores)<br/>print('Average Score:',sum(scores)/len(scores))<br/>print('choice 1:{}  choice 0:{} choice 2:{}'.format(choices.count(1)/len(choices),choices.count(0)/len(choices),choices.count(2)/len(choices)))</span></pre><p id="f683" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">这里你可以看到我根本没碰奖励部分。但是我们的模型知道如果它做了什么动作，它将到达山顶，所以它自动执行良好。执行这段代码后，你会得到这样的分数</p><pre class="kd ke kf kg fq kh ki kj kk aw kl dt"><span id="a255" class="km kn ht ki b fv ko kp l kq kr">[-164.0, -92.0, -162.0, -107.0, -105.0, -93.0, -97.0, -90.0, -96.0, -170.0, -99.0, -200.0, -164.0, -91.0, -200.0, -92.0, -195.0, -166.0, -104.0, -93.0, -164.0, -200.0, -200.0, -164.0, -179.0, -176.0, -122.0, -101.0, -91.0, -162.0, -99.0, -164.0, -190.0, -199.0, -101.0, -200.0, -186.0, -185.0, -170.0, -128.0, -164.0, -164.0, -166.0, -101.0, -167.0, -89.0, -105.0, -168.0, -166.0, -100.0, -100.0, -91.0, -90.0, -163.0, -165.0, -167.0, -165.0, -105.0, -88.0, -134.0, -95.0, -90.0, -166.0, -166.0, -89.0, -167.0, -162.0, -165.0, -164.0, -171.0, -163.0, -127.0, -95.0, -159.0, -89.0, -89.0, -96.0, -168.0, -96.0, -163.0, -89.0, -90.0, -183.0, -166.0, -164.0, -163.0, -171.0, -167.0, -163.0, -97.0, -171.0, -166.0, -89.0, -200.0, -162.0, -175.0, -198.0, -93.0, -200.0, -106.0]<br/>Average Score: -141.12<br/>choice 1:0.007936507936507936  choice 0:0.5136054421768708 choice 2:0.47845804988662133</span></pre><p id="0c9f" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">干得好，你的机器人做得非常好。</p><figure class="kd ke kf kg fq it fe ff paragraph-image"><div class="fe ff lj"><img src="../Images/caddff4ff6400196755e85afa68513f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/0*VsDhkvrcaTOc2bwu.gif"/></div></figure><p id="b603" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">恭喜你。！！你很好地理解了奖励机制，也理解了如果你的游戏对奖励不友好，该如何设计解决方案。</p><p id="f17b" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">你可以在这里找到Jupyter笔记本。</p></div><div class="ab cl jv jw hb jx" role="separator"><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka"/></div><div class="hm hn ho hp hq"><p id="baee" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">如果你喜欢这篇文章，请给它一些掌声来表达你的爱👏。<br/>和平。快乐编码。<br/> <a class="ae kc" href="https://blog.tanka.la/2018/10/19/solving-curious-case-of-mountaincar-reward-problem-using-openai-gym-keras-tensorflow-in-python/" rel="noopener ugc nofollow" target="_blank">这里看我的原创文章。</a></p><blockquote class="lk"><p id="d085" class="ll lm ht bd ln lo lp lq lr ls lt ju ek translated"><a class="ae kc" href="https://coincodecap.com/?utm_source=coinmonks" rel="noopener ugc nofollow" target="_blank">在您的收件箱中直接获得最佳软件交易</a></p></blockquote><figure class="lv lw lx ly lz it fe ff paragraph-image"><a href="https://coincodecap.com/?utm_source=coinmonks"><div class="fe ff lu"><img src="../Images/7c0b3dfdcbfea594cc0ae7d4f9bf6fcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/0*OJ-qb5G6i863msBB.png"/></div></a></figure></div></div>    
</body>
</html>