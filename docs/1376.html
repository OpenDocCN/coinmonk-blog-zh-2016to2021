<html>
<head>
<title>Review: VGGNet — 1st Runner-Up (Image Classification), Winner (Localization) in ILSVRC 2014</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">点评:VGGNet—ils vrc 2014亚军(图像分类)，冠军(本地化)</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11?source=collection_archive---------0-----------------------#2018-08-22">https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11?source=collection_archive---------0-----------------------#2018-08-22</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><p id="9603" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">本故事中，<strong class="is hu"> VGGNet [1] </strong>回顾。VGGNet是牛津大学的VGG ( <a class="ae jo" href="http://www.robots.ox.ac.uk/~vgg/" rel="noopener ugc nofollow" target="_blank">视觉几何组</a>)发明的，虽然VGGNet是<strong class="is hu">亚军</strong>，而不是分类任务的<strong class="is hu"> ILSVRC ( </strong> <a class="ae jo" href="http://www.image-net.org/challenges/LSVRC/" rel="noopener ugc nofollow" target="_blank"> <strong class="is hu"> ImageNet大规模视觉识别竞赛</strong> </a> <strong class="is hu"> ) 2014的冠军，相比ZF net(2013年的冠军)【2】和AlexNet(2012年的冠军)【3】有了显著的提升。而GoogLeNet是ILSVLC 2014的获奖者，我后面也会讲到。)尽管如此，<strong class="is hu"> VGGNet还是击败了GoogLeNet，在ILSVRC 2014中拿下了本地化任务。</strong></strong></p><p id="0b97" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">而且是<strong class="is hu">第一年有深度学习模型获得10%以下的错误率。</strong>最重要的是<strong class="is hu">在VGGNet之上或基于VGGNet的3×3 conv理念</strong>有许多其他模型用于其他目的或其他领域。这就是为什么我们需要了解VGGNet！这也是为什么当我写这个故事时，这是一篇2015年ICLR论文被引用超过14000次<strong class="is hu"/>。(<a class="jp jq gr" href="https://medium.com/u/aff72a0c1243?source=post_page-----d02355543a11--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="fe ff jr"><img src="../Images/a8c9d0d14978087859745aaa694be430.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*smaJBYed3MSsqDodgnJnXw.png"/></div></div><figcaption class="kd ke fg fe ff kf kg bd b be z ek"><strong class="bd kh">ILSVRC 2014 Ranking [4]</strong></figcaption></figure><p id="ccb6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">通常，人们只谈论VGG-16和VGG-19。<strong class="is hu"> </strong>我将通过烧蚀研究来谈谈<br/><strong class="is hu">VGG 11号、VGG 11号(LRN)、VGG 13号、VGG 16号(Conv1)、VGG 16号和VGG 19号</strong>。</p><p id="ce0d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">密集测试</strong>，通常被忽略，<strong class="is hu">也会被覆盖。</strong></p></div><div class="ab cl ki kj hb kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hm hn ho hp hq"><p id="513d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">ImageNet是一个数据集，包含超过1500万张带有标签的高分辨率图像，大约有22，000个类别。ILSVRC在1000个类别中的每个类别中使用大约1000个图像的ImageNet子集。总的来说，大约有130万幅训练图像、50，000幅验证图像和100，000幅测试图像。</p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div class="fe ff kp"><img src="../Images/029151dc4d81620d09178ea489cda7cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*CJF5xTINLDRJ6Ur7.jpg"/></div><figcaption class="kd ke fg fe ff kf kg bd b be z ek"><strong class="bd kh">ILSVRC</strong></figcaption></figure></div><div class="ab cl ki kj hb kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hm hn ho hp hq"><h1 id="baca" class="kq kr ht bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln dt translated">我们将涵盖的内容:</h1><ol class=""><li id="43b1" class="lo lp ht is b it lq ix lr jb ls jf lt jj lu jn lv lw lx ly dt translated"><strong class="is hu">使用3×3滤镜</strong>代替大尺寸滤镜(如11×11、7×7)</li><li id="c544" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly dt translated"><strong class="is hu"> VGG-16 </strong>和<strong class="is hu"> VGG-19 </strong>基于烧蚀研究<br/>(也包括VGG-11、VGG-11 (LRN)、VGG-13、VGG-16 (Conv1)。)</li><li id="311e" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly dt translated"><strong class="is hu">多尺度训练</strong></li><li id="5e8e" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly dt translated"><strong class="is hu">多尺度测试</strong></li><li id="2370" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly dt translated"><strong class="is hu">密集测试</strong></li><li id="deff" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly dt translated"><strong class="is hu">模型融合</strong></li><li id="bef9" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly dt translated"><strong class="is hu">VGGNet和GoogLeNet的比较</strong></li><li id="df59" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly dt translated"><strong class="is hu">本地化任务</strong></li></ol></div><div class="ab cl ki kj hb kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hm hn ho hp hq"><h1 id="33f1" class="kq kr ht bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln dt translated"><strong class="ak"> 1。3×3过滤器的使用</strong></h1><figure class="js jt ju jv fq jw fe ff paragraph-image"><div class="fe ff me"><img src="../Images/0c6ac4a1b8900aeeb3828f845746b3b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*NMC4KrYIciGdE_FaazxF7w.png"/></div><figcaption class="kd ke fg fe ff kf kg bd b be z ek"><strong class="bd kh">2 layers of 3×3 filters already covered the 5×5 area</strong></figcaption></figure><p id="7b4c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">使用2层3×3滤光片，实际上已经覆盖了上图中的5×5区域</strong>。<strong class="is hu">使用3层3×3的滤光片，实际上已经覆盖了7×7的有效面积。</strong>因此，AlexNet [3]中的11×11和ZFNet [2]中的7×7这样的大尺寸滤镜确实是不需要的。(如果有兴趣，请去我关于ZFNet[5]和AlexNet[6]的故事。)</p><p id="515f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">另一个原因是<strong class="is hu">的参数数量较少。</strong>假设每层只有1个滤波器，输入端有1层，排除偏差:</p><p id="b9df" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"> 1层11×11滤镜</strong>，参数个数= <strong class="is hu"> 11×11=121 <br/> 5层3×3滤镜</strong>，参数个数= <strong class="is hu"> 3×3×5=45 <br/>参数个数减少63% </strong></p><p id="f365" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"> 1层7×7滤镜</strong>，参数个数=<strong class="is hu">7×7 = 49<br/>3</strong>T21】3层3×3滤镜，参数个数=<strong class="is hu">3×3 = 27<br/>参数个数减少45% </strong></p><p id="4e8f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">通过使用<strong class="is hu"> 1层5×5滤波器</strong>，参数数= 5<strong class="is hu">×5 = 25<br/>T30通过使用<strong class="is hu"> 2层3×3滤波器</strong>，参数数= <strong class="is hu"> 3×3+3×3=18 <br/>参数数减少28% </strong></strong></p><p id="ab35" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">网络越大，对训练图像的需求就越大。还有消失梯度问题。但消失梯度问题在ResNet [9]中通过跳过连接得到了某种程度的解决。</p><p id="b46b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">学习的参数</strong>越少，<strong class="is hu">收敛越快</strong>越好，<strong class="is hu">减少过拟合问题</strong>。</p></div><div class="ab cl ki kj hb kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hm hn ho hp hq"><h1 id="07c9" class="kq kr ht bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln dt translated"><strong class="ak"> 2。基于消融研究的VGG-16 </strong>和VGG-19 </h1><figure class="js jt ju jv fq jw fe ff paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="fe ff mf"><img src="../Images/6abbf2bc34ea98a0d585f671983b85d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_1DEx3bHlnBApCWWQ0HgcQ.png"/></div></div><figcaption class="kd ke fg fe ff kf kg bd b be z ek"><strong class="bd kh">Different VGG Layer Structures Using Single Scale (256) Evaluation</strong></figcaption></figure><p id="6aa8" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">为了获得最佳的深度学习层结构，进行了烧蚀研究，如上图所示。</p><ol class=""><li id="8248" class="lo lp ht is b it iu ix iy jb mg jf mh jj mi jn lv lw lx ly dt translated">首先，<strong class="is hu"> VGG-11已经获得了10.4%的错误率</strong>，和ILSVRC 2013中的ZFNet差不多。VGG-11被设定为基准。</li><li id="ad1a" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly dt translated"><strong class="is hu"> VGG-11 (LRN)获得10.5%的错误率，是AlexNet建议的增加局部反应归一化(LRN) </strong>操作的一种。通过比较VGG-11和VGG-11 (LRN)，错误率没有提高，这意味着LRN是无用的。事实上，LRN在后来的深度学习网络中已经不再使用，取而代之的是批量归一化(BN)。</li><li id="2524" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly dt translated"><strong class="is hu"> VGG-13 </strong> <strong class="is hu">获得9.9%的错误率，这意味着额外的conv有助于分类精度</strong>。</li><li id="fc1e" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly dt translated"><strong class="is hu"> VGG-16 (Conv1)获得9.4%的错误率，这意味着额外的三个1×1 conv层有助于分类精度</strong>。<strong class="is hu"> 1×1 conv实际上有助于增加决策函数的非线性。</strong>在不改变输入输出维度的情况下，<strong class="is hu"> 1×1 conv在做同样高维的投影映射。</strong>这项技术在一篇名为“网络中的网络”的论文[7]中是必不可少的，在Google net[8](2014年ILSVRC的获奖者)和ResNet[9](2015年ILSVRC的获奖者)中也是如此。我将在未来更多地谈论GoogLeNet和ResNet评论故事。</li><li id="5319" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly dt translated"><strong class="is hu"> VGG-16获得8.8%的错误率，这意味着深度学习网络仍在通过增加层数来改善。</strong></li><li id="2654" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly dt translated"><strong class="is hu"> VGG-19获得9.0%的错误率，这意味着深度学习网络没有通过增加层数来改善。因此，作者停止添加图层。</strong></li></ol><p id="eea0" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">通过观察逐层叠加，我们可以观察到<strong class="is hu"> VGG-16和VGG-19开始收敛</strong>，精度提升变慢。当人们谈论VGGNet时，他们通常会提到VGG-16和VGG-19。</p></div><div class="ab cl ki kj hb kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hm hn ho hp hq"><h1 id="0369" class="kq kr ht bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln dt translated"><strong class="ak"> 3。多尺度训练</strong></h1><p id="4f30" class="pw-post-body-paragraph iq ir ht is b it lq iv iw ix lr iz ja jb mj jd je jf mk jh ji jj ml jl jm jn hm dt translated">由于对象在图像中具有不同的尺度，<strong class="is hu">如果我们只在相同的尺度下训练网络，对于具有其他尺度的对象，我们可能会错过检测或者具有错误的分类。为了解决这个问题，作者提出了多尺度训练。</strong></p><p id="e163" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">对于<strong class="is hu">单尺度训练</strong>，图像以等于256或384的较小尺寸缩放，即<strong class="is hu"> S=256或384。</strong>由于网络只接受224×224的输入图像。缩放后的图像将被裁剪为224×224。其概念如下:</p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div class="fe ff mm"><img src="../Images/9118d738a38e671f8f8d1468d34d8e68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*c8N9P2qSWMtJaygUMn7TEQ.png"/></div><figcaption class="kd ke fg fe ff kf kg bd b be z ek"><strong class="bd kh">Single-Scale Training with S=256 and S=384</strong></figcaption></figure><p id="46af" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">对于<strong class="is hu">多尺度训练</strong>，图像以较小的尺寸进行缩放，范围等于256到512，即<strong class="is hu">S =【256；512] </strong>，然后裁剪成224×224。因此，<strong class="is hu">在S </strong>、<strong class="is hu">的范围内，我们将不同的缩放对象输入到网络中进行训练。</strong></p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="fe ff mn"><img src="../Images/9f3c0c6a4c1c1d231aebb445ea82a85e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZAyXZp8nw3YPF-YVVHNAsg.png"/></div></div><figcaption class="kd ke fg fe ff kf kg bd b be z ek"><strong class="bd kh">Multi-Scale Training Results</strong></figcaption></figure><p id="53de" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">通过使用多尺度训练</strong>，我们可以想象<strong class="is hu">对于不同物体尺寸的测试图像物体更加准确。</strong></p><p id="5988" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"> VGG-13 </strong>将错误率从9.4%/9.3%降至<strong class="is hu"> 8.8%。<br/> VGG-16 </strong>将错误率从8.8%/8.7%降至<strong class="is hu"> 8.1%。<br/> VGG-19 </strong>将错误率从9.0%/8.7%降至<strong class="is hu"> 8.0%。</strong></p></div><div class="ab cl ki kj hb kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hm hn ho hp hq"><h1 id="8d04" class="kq kr ht bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln dt translated">4.<strong class="ak">多尺度测试</strong></h1><p id="8eb7" class="pw-post-body-paragraph iq ir ht is b it lq iv iw ix lr iz ja jb mj jd je jf mk jh ji jj ml jl jm jn hm dt translated">类似于多尺度训练，<strong class="is hu">多尺度测试</strong>也可以降低错误率，因为我们不知道测试图像中对象的大小。如果我们<strong class="is hu">将测试图像缩放到不同的尺寸</strong>，我们就可以<strong class="is hu">增加正确分类的几率</strong>。</p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="fe ff mo"><img src="../Images/952a666a0a38745e1f21ca1c9434f031.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hy0nncKW3GMQj6iLggIYJw.png"/></div></div><figcaption class="kd ke fg fe ff kf kg bd b be z ek"><strong class="bd kh">Multi-Scale Testing Results</strong></figcaption></figure><p id="376d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">通过使用多尺度测试而不是单尺度训练，降低了错误率。<br/>相比单尺度训练单尺度测试，</p><p id="7908" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"> VGG-13 </strong>将错误率从9.4%/9.3%降至<strong class="is hu"> 9.2%。<br/> VGG-16 </strong>将错误率从8.8%/8.7%降至<strong class="is hu"> 8.6%。<br/> VGG-19 </strong>将错误率从9.0%/8.7%降至8.7/8.6 <strong class="is hu"> %。</strong></p><p id="3c1c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">通过使用多尺度训练和测试</strong>，降低了错误率<strong class="is hu">。<br/> </strong>与只进行多尺度测试相比，</p><p id="097a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"> VGG-13 </strong>将差错率从9.2%/9.2%降至<strong class="is hu"> 8.2%，<br/> VGG-16 </strong>将差错率从8.6%/8.6%降至<strong class="is hu"> 7.5%，<br/> VGG-19 </strong>将差错率从8.7%/8.6%降至7<strong class="is hu">5%，</strong></p></div><div class="ab cl ki kj hb kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hm hn ho hp hq"><h1 id="9f62" class="kq kr ht bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln dt translated">5.密集(卷积化)测试</h1><p id="14d9" class="pw-post-body-paragraph iq ir ht is b it lq iv iw ix lr iz ja jb mj jd je jf mk jh ji jj ml jl jm jn hm dt translated">测试时，在<strong class="is hu"> AlexNet </strong>中，裁剪图像的<strong class="is hu"> 4个角和中心以及它们的<strong class="is hu">水平翻转</strong>进行测试，即<strong class="is hu"> 10次测试</strong>。并且输出概率向量被相加或平均以获得更好的结果。</strong></p><p id="b8de" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">VGGNet与培训中的不同，如下所示:</p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="fe ff mp"><img src="../Images/d90c6d97db045b18256fadc8b5353d44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4F-9zrU07yhwj6gChX_q-Q.png"/></div></div><figcaption class="kd ke fg fe ff kf kg bd b be z ek"><strong class="bd kh">VGGNet During Testing</strong></figcaption></figure><p id="9c98" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">第一个FC被替换为7×7 conv。<br/>第二个和第三个FC被替换为1×1 conv。<br/> 这样，所有的FC层都被conv层取代。</p><p id="ac3a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">测试时，在<strong class="is hu"> VGGNet </strong>中，测试图像直接通过VGGNet，得到一个类得分图。该类得分图被空间平均为固定大小的向量。</p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div class="fe ff mq"><img src="../Images/1130e46b318ba0352835b96289de84c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*-aMX3n3pTnNaY7kWxnd9Xg.png"/></div><figcaption class="kd ke fg fe ff kf kg bd b be z ek"><strong class="bd kh">Workflow of VGGNet Testing</strong></figcaption></figure><p id="91d1" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">如果我们也包括水平翻转，只有2次测试。</p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="fe ff mr"><img src="../Images/a6c69a93173098634153e0c2aa2f8400.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7iXW9gXGEQHIK_L7fT7Czw.png"/></div></div><figcaption class="kd ke fg fe ff kf kg bd b be z ek"><strong class="bd kh">Dense (VGGNet), Multi-crop (Approach by AlexNet), Dense+Multi-crop (Both)</strong></figcaption></figure><p id="b18f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">通过对密集和多作物结果进行平均，VGG-16和VGG-19的误差率降低到7.2%和7.1%。</strong></p></div><div class="ab cl ki kj hb kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hm hn ho hp hq"><h1 id="c39d" class="kq kr ht bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln dt translated"><strong class="ak"> 6。模型融合</strong></h1><figure class="js jt ju jv fq jw fe ff paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="fe ff ms"><img src="../Images/1e2e4203a7a1d10baf8d41361b0c463c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qHM5R9d54gZgqzwPrzye1A.png"/></div></div><figcaption class="kd ke fg fe ff kf kg bd b be z ek"><strong class="bd kh">Fusion All Techniques Mentioned Above</strong></figcaption></figure><p id="0fbb" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">通过结合VGG-16和VGG-19加上多尺度训练、多尺度测试、多作物和密集，错误率降低到6.8%。</p></div><div class="ab cl ki kj hb kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hm hn ho hp hq"><h1 id="0506" class="kq kr ht bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln dt translated"><strong class="ak"> 7。VGGNet和GoogLeNet的比较</strong></h1><figure class="js jt ju jv fq jw fe ff paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="fe ff mt"><img src="../Images/9ab9984ad5850ad82a40653ff1b9f403.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MHN9_mEDnD6D_IzxytnN9Q.png"/></div></div><figcaption class="kd ke fg fe ff kf kg bd b be z ek"><strong class="bd kh">Comparison Between VGGNet and GoogLeNet</strong></figcaption></figure><p id="e133" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">与<strong class="is hu">使用7网的GoogLeNet】具有6.7% </strong>的<strong class="is hu">错误率相比，<strong class="is hu">使用2网</strong>的VGGNet，加上多尺度训练、多尺度测试、多作物和密集具有6.8% </strong>的<strong class="is hu">错误率，具有竞争力。</strong></p><p id="4884" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">只有1网</strong>，<strong class="is hu"> VGGNet有7.0%的错误率</strong>，比<strong class="is hu"> GoogLeNet好，GoogLeNet有7.9%的错误率</strong>。</p><p id="217f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">然而，<strong class="is hu">在ILSVRC 2014提交时，VGGNet只有7.3%的错误率，目前获得亚军。</strong></p></div><div class="ab cl ki kj hb kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hm hn ho hp hq"><h1 id="e8c6" class="kq kr ht bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln dt translated">8.本地化任务(帖子更新于2018年9月2日)</h1><p id="cfae" class="pw-post-body-paragraph iq ir ht is b it lq iv iw ix lr iz ja jb mj jd je jf mk jh ji jj ml jl jm jn hm dt translated">对于定位任务，边界框由存储其中心坐标、宽度和高度的四维向量表示。因此，逻辑回归目标被欧几里德损失代替，这惩罚了预测的边界框参数与地面真实的偏差。</p><p id="4ea1" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">可以选择边界框预测是在所有类之间共享(<strong class="is hu">单类回归，SCR </strong>)还是特定于类(<strong class="is hu">每类回归，PCR </strong>)。在前一种情况下，最后一层是<strong class="is hu"> 4-D </strong>，而在后一种情况下是<strong class="is hu"> 4000-D </strong>(因为数据集中有1000个类)。</p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div class="fe ff mu"><img src="../Images/86e31b59cd520d7996dc9ac3ca655e1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*fD8Aa_b1CccnlHfB38UhPw.png"/></div><figcaption class="kd ke fg fe ff kf kg bd b be z ek"><strong class="bd kh">Localization Results</strong></figcaption></figure><p id="4065" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">如上图，<strong class="is hu"> PCR优于SCR。</strong>和<strong class="is hu">微调所有层比只微调第一和第二FC层更好</strong>。上述结果仅通过使用中心裁剪获得。</p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div class="fe ff mv"><img src="../Images/733160949d7624eb835de1292182e11e.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*EhyfkIhcTT6TOtkmZ82PlQ.png"/></div><figcaption class="kd ke fg fe ff kf kg bd b be z ek"><strong class="bd kh">Multi-Scale Training and Testing</strong></figcaption></figure><p id="e797" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">通过前几节刚刚描述的<strong class="is hu">多次训练和测试</strong>，前5名的<strong class="is hu">定位误差降低到25.3%。</strong></p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div class="fe ff mw"><img src="../Images/466687ae2274aa881b1fdccd917f94df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*o8aRY26Uz4scWTgzi89-4Q.png"/></div><figcaption class="kd ke fg fe ff kf kg bd b be z ek"><strong class="bd kh">Comparison with state-of-the-art results</strong></figcaption></figure><p id="4276" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">如上图所示，VGGNet甚至超过了GoogLeNet </strong>，并且<strong class="is hu">赢得了ILSVRC 2014 </strong>的本地化任务。</p><figure class="js jt ju jv fq jw fe ff paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="fe ff mx"><img src="../Images/92ebb69142edc46bf5a59dacfda88969.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QByDgPOZtXNGYHjI-qNLKQ.png"/></div></div><figcaption class="kd ke fg fe ff kf kg bd b be z ek"><strong class="bd kh">VOC 2007, 2012 and Caltech 101 and 256 Dataset Results</strong></figcaption></figure><p id="6d3a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"> VGGNet在VOC 2007、2012和Caltech 256数据集上的结果最好。</strong>在加州理工101数据集上也有<strong class="is hu">的竞争结果。</strong></p></div><div class="ab cl ki kj hb kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hm hn ho hp hq"><p id="d49c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">我将介绍GoogLeNet [8]、ResNet [9]等用于图像分类的工具。<br/>敬请关注。</p></div><div class="ab cl ki kj hb kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hm hn ho hp hq"><h1 id="bb44" class="kq kr ht bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln dt translated">参考</h1><ol class=""><li id="4636" class="lo lp ht is b it lq ix lr jb ls jf lt jj lu jn lv lw lx ly dt translated">【2015 ICLR】【VGGNet】<br/><a class="ae jo" href="https://arxiv.org/pdf/1409.1556" rel="noopener ugc nofollow" target="_blank">用于大规模图像识别的极深度卷积网络</a></li><li id="b3ec" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly dt translated">【2014 ECCV】【ZFNet】<br/><a class="ae jo" href="https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf" rel="noopener ugc nofollow" target="_blank">可视化和理解卷积网络</a></li><li id="36c4" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly dt translated">【2012 NIPS】【Alex net】<br/><a class="ae jo" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank">使用深度卷积神经网络的ImageNet分类</a></li><li id="314f" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly dt translated">ILSVRC 2014排名<br/><a class="ae jo" href="http://www.image-net.org/challenges/LSVRC/2014/results#clsloc" rel="noopener ugc nofollow" target="_blank">http://www . image-net . org/challenges/ls vrc/2014/results # cls loc</a></li><li id="6dbb" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly dt translated"><a class="ae jo" rel="noopener" href="/coinmonks/paper-review-of-zfnet-the-winner-of-ilsvlc-2013-image-classification-d1a5a0c45103">ZFNet综述—2013年度ILSVRC(图像分类)获奖者</a></li><li id="3ee9" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly dt translated"><a class="ae jo" rel="noopener" href="/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160">AlexNet，CaffeNet综述ILSVRC 2012(图像分类)获得者</a></li><li id="90b9" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly dt translated">【2014 ICLR】<br/><a class="ae jo" href="https://arxiv.org/pdf/1312.4400.pdf" rel="noopener ugc nofollow" target="_blank">网络中的网络</a></li><li id="0551" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly dt translated">【2015 CVPR】【谷歌在线】<br/> <a class="ae jo" href="https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf" rel="noopener ugc nofollow" target="_blank">回旋更深</a></li><li id="8ab6" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly dt translated">【2016 CVPR】【ResNet】<br/><a class="ae jo" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank">图像识别的深度残差学习</a></li></ol><blockquote class="my"><p id="e2ca" class="mz na ht bd nb nc nd ne nf ng nh jn ek translated">加入Coinmonks <a class="ae jo" href="https://t.me/coincodecap" rel="noopener ugc nofollow" target="_blank">电报频道</a>和<a class="ae jo" href="https://www.youtube.com/c/coinmonks/videos" rel="noopener ugc nofollow" target="_blank"> Youtube频道</a>获取每日<a class="ae jo" href="http://coincodecap.com/" rel="noopener ugc nofollow" target="_blank">加密新闻</a></p></blockquote><h2 id="15d6" class="ni kr ht bd ks nj nk nl kw nm nn no la jb np nq le jf nr ns li jj nt nu lm nv dt translated">此外，请阅读</h2><ul class=""><li id="20fb" class="lo lp ht is b it lq ix lr jb ls jf lt jj lu jn nw lw lx ly dt translated"><a class="ae jo" href="http://Top 4 Telegram Channels for Crypto Traders" rel="noopener ugc nofollow" target="_blank">密码电报信号</a> | <a class="ae jo" rel="noopener" href="/coinmonks/crypto-trading-bot-c2ffce8acb2a">密码交易机器人</a></li><li id="14e6" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn nw lw lx ly dt translated"><a class="ae jo" rel="noopener" href="/coinmonks/top-10-crypto-copy-trading-platforms-for-beginners-d0c37c7d698c">拷贝交易</a> | <a class="ae jo" rel="noopener" href="/coinmonks/crypto-tax-software-ed4b4810e338">密码税务软件</a></li><li id="723e" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn nw lw lx ly dt translated"><a class="ae jo" href="https://coincodecap.com/grid-trading" rel="noopener ugc nofollow" target="_blank">网格交易</a> | <a class="ae jo" rel="noopener" href="/coinmonks/the-best-cryptocurrency-hardware-wallets-of-2020-e28b1c124069">加密硬件钱包</a></li><li id="f33b" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn nw lw lx ly dt translated"><a class="ae jo" rel="noopener" href="/coinmonks/crypto-exchange-dd2f9d6f3769">密码交换</a> | <a class="ae jo" rel="noopener" href="/coinmonks/buy-bitcoin-in-india-feb50ddfef94">印度密码应用</a></li><li id="47a8" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn nw lw lx ly dt translated"><a class="ae jo" rel="noopener" href="/coinmonks/best-crypto-apis-for-developers-5efe3a597a9f">开发人员的最佳加密API</a></li><li id="c9bf" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn nw lw lx ly dt translated">最佳<a class="ae jo" rel="noopener" href="/coinmonks/top-5-crypto-lending-platforms-in-2020-that-you-need-to-know-a1b675cec3fa">密码借出平台</a></li><li id="9487" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn nw lw lx ly dt translated"><a class="ae jo" rel="noopener" href="/coinmonks/leveraged-token-3f5257808b22">杠杆令牌</a>终极指南</li><li id="4a2a" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn nw lw lx ly dt translated"><a class="ae jo" href="https://coincodecap.com/crypto-affiliate-programs" rel="noopener ugc nofollow" target="_blank">八大加密附属计划</a> | <a class="ae jo" href="https://coincodecap.com/etoro-vs-coinbase" rel="noopener ugc nofollow" target="_blank"> eToro vs比特币基地</a></li><li id="e0bf" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn nw lw lx ly dt translated"><a class="ae jo" href="https://coincodecap.com/binance-futures-trading" rel="noopener ugc nofollow" target="_blank">币安期货交易</a>|<a class="ae jo" href="https://coincodecap.com/mudrex-3commas-etoro" rel="noopener ugc nofollow" target="_blank">3 commas vs Mudrex vs eToro</a></li><li id="a5fa" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn nw lw lx ly dt translated"><a class="ae jo" href="https://coincodecap.com/buy-monero" rel="noopener ugc nofollow" target="_blank">如何购买Monero </a> | <a class="ae jo" href="https://coincodecap.com/idex-review" rel="noopener ugc nofollow" target="_blank"> IDEX评论</a> | <a class="ae jo" href="https://coincodecap.com/bitkan-trading-bot" rel="noopener ugc nofollow" target="_blank"> BitKan交易机器人</a></li><li id="990c" class="lo lp ht is b it lz ix ma jb mb jf mc jj md jn nw lw lx ly dt translated"><a class="ae jo" rel="noopener" href="/coinmonks/youhodler-vs-coinloan-vs-hodlnaut-b1050acde55a">尤霍德勒vs科恩洛vs霍德诺特</a> | <a class="ae jo" href="https://coincodecap.com/cryptohopper-vs-haasbot" rel="noopener ugc nofollow" target="_blank"> Cryptohopper vs哈斯博特</a></li></ul></div></div>    
</body>
</html>