<html>
<head>
<title>TensorFlow: A new paradigm for large scale ML in distributed systems</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">张量流:分布式系统中大规模ML的新范式</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/tensorflow-a-new-paradigm-for-large-scale-ml-in-distributed-systems-fc45458e0c5?source=collection_archive---------4-----------------------#2018-08-14">https://medium.com/coinmonks/tensorflow-a-new-paradigm-for-large-scale-ml-in-distributed-systems-fc45458e0c5?source=collection_archive---------4-----------------------#2018-08-14</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><figure class="fi fk ir is it iu fe ff paragraph-image"><div class="fe ff iq"><img src="../Images/2363b6a706b3d589c2d3d67bc531e524.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*iSZ5_n1F0fahjhCOrpXEHQ.jpeg"/></div></figure><h2 id="93e3" class="ix iy ht bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju dt translated">介绍</h2><p id="e107" class="pw-post-body-paragraph jv jw ht jx b jy jz ka kb kc kd ke kf ji kg kh ki jm kj kk kl jq km kn ko kp hm dt translated"><a class="ae kq" href="https://arxiv.org/pdf/1603.04467.pdf" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>是谷歌内部开发的用于机器学习的编程范式。从那以后，他们开源了它，并且有相当多的社区在使用这个新的范例。我打算在这里介绍TensorFlow的主要构件和架构。</p><p id="5102" class="pw-post-body-paragraph jv jw ht jx b jy kr ka kb kc ks ke kf ji kt kh ki jm ku kk kl jq kv kn ko kp hm dt translated">TensorFlow是为从强大的GPU到轻量级移动设备的异构环境开发的。它既可以用于推理，也可以用于在各种设备上学习。TensorFlow是谷歌开发的第二代ML基础设施——借鉴了<a class="ae kq" href="http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf" rel="noopener ugc nofollow" target="_blank">dist faith</a>的经验</p><p id="d41e" class="pw-post-body-paragraph jv jw ht jx b jy kr ka kb kc ks ke kf ji kt kh ki jm ku kk kl jq kv kn ko kp hm dt translated">对DistBelief的核心改进似乎如下。在这一点上，其中一些可能看起来有点抽象，但随着我们深入细节，就会变得清晰。</p><ol class=""><li id="1b9b" class="kw kx ht jx b jy kr kc ks ji ky jm kz jq la kp lb lc ld le dt translated">用类似数据流的范例描述计算</li><li id="e1a7" class="kw kx ht jx b jy lf kc lg ji lh jm li jq lj kp lb lc ld le dt translated">能够在高度异构的系统中训练和运行模型—从移动设备到强大的服务器群。</li><li id="8cef" class="kw kx ht jx b jy lf kc lg ji lh jm li jq lj kp lb lc ld le dt translated">以更好的方式表达并行性，允许一组节点更新共享参数</li><li id="7fcc" class="kw kx ht jx b jy lf kc lg ji lh jm li jq lj kp lb lc ld le dt translated">统一的接口，而不是大规模部署用于训练，然后小规模部署用于推理，这通常会导致系统中不希望的抽象。</li><li id="bdcc" class="kw kx ht jx b jy lf kc lg ji lh jm li jq lj kp lb lc ld le dt translated">性能改进</li></ol><h2 id="396b" class="ix iy ht bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju dt translated">张量流编程范式</h2><p id="c1fc" class="pw-post-body-paragraph jv jw ht jx b jy jz ka kb kc kd ke kf ji kg kh ki jm kj kk kl jq km kn ko kp hm dt translated">TensorFlow程序由程序员使用他们选择的前端语言编写，如python或c++。这些程序被转换成一个有向图。该图表示数据流计算，其中数据从节点经由图边流动。每个节点有多个输入和输出，它代表一个特定的操作。图中流动的大部分数据都表示为张量。张量是任意维数的类型化数组——当你处理矩阵或方程组时，这种构造非常有用。</p><p id="0506" class="pw-post-body-paragraph jv jw ht jx b jy kr ka kb kc ks ke kf ji kt kh ki jm ku kk kl jq kv kn ko kp hm dt translated">还有一些控制和循环结构可以重叠，插入到图形中以便更好地控制。让我们定义张量流系统的一些基本元素。</p><p id="ae4d" class="pw-post-body-paragraph jv jw ht jx b jy kr ka kb kc ks ke kf ji kt kh ki jm ku kk kl jq kv kn ko kp hm dt translated"><strong class="jx hu">运算:</strong>运算是一些类似矩阵乘法的运算，可以在图构造的时候推断出来。</p><p id="94ec" class="pw-post-body-paragraph jv jw ht jx b jy kr ka kb kc ks ke kf ji kt kh ki jm ku kk kl jq kv kn ko kp hm dt translated"><strong class="jx hu">内核:</strong>内核是给定操作的设备特定实现，例如特定于GPU的实现。</p><p id="e79a" class="pw-post-body-paragraph jv jw ht jx b jy kr ka kb kc ks ke kf ji kt kh ki jm ku kk kl jq kv kn ko kp hm dt translated">大多数程序创建一个会话，并使用它来运行数据流计算。Run interface可以计算图形依赖性，然后可以将节点放置在适当的设备上。想法是设置图表一次，然后通过它运行数据计算多次。还可以通过设置适当的输入和输出来运行子图。</p><p id="7e24" class="pw-post-body-paragraph jv jw ht jx b jy kr ka kb kc ks ke kf ji kt kh ki jm ku kk kl jq kv kn ko kp hm dt translated"><strong class="jx hu">变量:</strong>变量可用于存储图形多次运行的数据。大多数张量是临时的，在给定的运行过程中会被释放。模型的参数是存储在变量中的良好候选者，因为它们通常在每次运行子图之后被更新。</p><p id="3c0d" class="pw-post-body-paragraph jv jw ht jx b jy kr ka kb kc ks ke kf ji kt kh ki jm ku kk kl jq kv kn ko kp hm dt translated"><strong class="jx hu">张量:</strong>张量是任意维的数组。它们表示在系统中流动的数据，并充当图节点的输入和输出。</p><p id="5844" class="pw-post-body-paragraph jv jw ht jx b jy kr ka kb kc ks ke kf ji kt kh ki jm ku kk kl jq kv kn ko kp hm dt translated"><strong class="jx hu">设备:</strong>设备在上面执行给定的任务。使用设备类型(例如CPU与GPU)来识别设备。设备管理它们自己的存储器，并且还调度在给定设备上执行的各种任务。根据设备的类型，可以使用特殊的分配器为张量分配内存。</p><h1 id="a2d6" class="lk iy ht bd iz ll lm ln jd lo lp lq jh lr ls lt jl lu lv lw jp lx ly lz jt ma dt translated">张量流架构</h1><p id="a4e1" class="pw-post-body-paragraph jv jw ht jx b jy jz ka kb kc kd ke kf ji kg kh ki jm kj kk kl jq km kn ko kp hm dt translated">在系统组件方面，TensorFlow由主机、工作机和客户端组成，用于分布式协调和执行。上一节描述的会话接口充当与<em class="mb">主进程和</em>工作进程<em class="mb">通信的<em class="mb">客户端</em>。工人</em>仲裁对<em class="mb">设备的访问。主人</em>在提出正确的方案供工人执行的过程中扮演着重要的角色。TensorFlow可以在完全本地的环境中执行，也可以在分布式环境中执行，这三个实体可以驻留在不同的机器上。</p><h2 id="8069" class="ix iy ht bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju dt translated">在单个或多个设备上执行</h2><figure class="md me mf mg fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="fe ff mc"><img src="../Images/adda49008e1292da47362470350bee2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XY1_01vWZb5_Ot9f4d3CSQ.png"/></div></div><figcaption class="ml mm fg fe ff mn mo bd b be z ek">LHS — Single GPU device. RHS — Heterogeneous devices each working on part of the subgraph</figcaption></figure><p id="f04a" class="pw-post-body-paragraph jv jw ht jx b jy kr ka kb kc ks ke kf ji kt kh ki jm ku kk kl jq kv kn ko kp hm dt translated">在单个设备上执行是最简单的情况，只有一个工作人员和一个设备。它可以利用图中内置的图依赖关系。独立节点可以利用设备的并行性。如果给定的节点依赖于其他节点，那么该节点的执行将一直等待，直到所有的依赖都被执行完。</p><p id="c4e9" class="pw-post-body-paragraph jv jw ht jx b jy kr ka kb kc ks ke kf ji kt kh ki jm ku kk kl jq kv kn ko kp hm dt translated">在<em class="mb">多个设备</em>的情况下，这个问题看起来像是一个调度问题。哪个图节点应该落在哪个设备上？需要跨越设备边界的所有数据怎么办？TensorFlow中使用的试探法之一是将放置成本建模为以字节为单位的输入/输出张量大小和计算时间的函数。计算时间可以静态估计或基于先前的执行时间。使用这些，调度程序从输入节点开始运行模拟。它考虑给定节点的所有可行设备，然后使用贪婪启发式算法将节点放置在设备上。这考虑了字节传输的成本以及设备所需的计算时间。选择节点执行最早完成的设备。</p><h2 id="0863" class="ix iy ht bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju dt translated">生成的节点:发送、接收、保存、恢复、获取和馈送</h2><p id="5a6d" class="pw-post-body-paragraph jv jw ht jx b jy jz ka kb kc kd ke kf ji kg kh ki jm kj kk kl jq km kn ko kp hm dt translated">一旦建立了节点布局，图形的一些边将不得不穿过器件边界。这是通过在每个设备中放置合成发送和接收存根节点来实现的。(显然，系统用户不必担心这一点，TensorFlow系统可以在幕后组合这些节点。)所有的通信都由这些节点处理。这些节点将张量作为输入。</p><figure class="md me mf mg fq iu fe ff paragraph-image"><div class="fe ff mp"><img src="../Images/f48526b1975de1b9625ac522186989b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*VuBeO9254Z_cZ_lTfVMuKQ.png"/></div><figcaption class="ml mm fg fe ff mn mo bd b be z ek">Placing subgraphs on two devices, with tensors a and x crossing device boundaries using generated send and receive nodes</figcaption></figure><p id="ffd7" class="pw-post-body-paragraph jv jw ht jx b jy kr ka kb kc ks ke kf ji kt kh ki jm ku kk kl jq kv kn ko kp hm dt translated">这些以通信为中心的节点有助于抽象所有通信和进一步优化。一个这样的优化是，相同的张量获得单个下游接收节点进行通信，以避免额外的复制(在上图中，考虑设备B上的<em class="mb">张量a </em> recv节点)。发送和接收节点的另一个优点是，它们可以用于跨设备同步，并且工作人员可以相互通信，而不必成为每个调度依赖项的主节点。这些节点的另一个优点是，在内部它们可以针对TCP/RDMA进行优化。</p><p id="38b8" class="pw-post-body-paragraph jv jw ht jx b jy kr ka kb kc ks ke kf ji kt kh ki jm ku kk kl jq kv kn ko kp hm dt translated">前面我们描述了用于保存状态的变量。由于系统中可能存在故障，TensorFlow提供了一种检查点机制。这是通过使用保存和恢复节点来完成的，在构建图形时，变量会连接到这些节点。有时值可以被持久地写入文件系统。相反，Restore节点在重启后的第一次迭代中用于填充给定的变量。</p><p id="ec0c" class="pw-post-body-paragraph jv jw ht jx b jy kr ka kb kc ks ke kf ji kt kh ki jm ku kk kl jq kv kn ko kp hm dt translated">在数据繁重的环境中，执行子图通常很有用。TensorFlow通过命名图形节点和提供输入/输出端口来实现这一点。缺失的输入/输出可以通过让用户指定输入来填充。子图的输入由<em class="mb">馈送</em>节点代替，输出由<em class="mb">获取</em>节点代替。</p><h2 id="6462" class="ix iy ht bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju dt translated">分布式执行</h2><p id="dfbc" class="pw-post-body-paragraph jv jw ht jx b jy jz ka kb kc kd ke kf ji kg kh ki jm kj kk kl jq km kn ko kp hm dt translated">发送和接收节点显然有助于不同子图的分布式执行。如果其中一个节点由于某种原因失败，那么整个执行就会中止。对于故障检测，定期健康检查由主和工人一起执行。发送和接收节点还可以检测彼此之间的通信错误和响应失败。在这种情况下，他们可以再次实例化图形执行的整体中止。</p><h1 id="8e8c" class="lk iy ht bd iz ll lm ln jd lo lp lq jh lr ls lt jl lu lv lw jp lx ly lz jt ma dt translated">高级TensorFlow功能</h1><p id="c093" class="pw-post-body-paragraph jv jw ht jx b jy jz ka kb kc kd ke kf ji kg kh ki jm kj kk kl jq km kn ko kp hm dt translated">到目前为止，我们已经介绍了张量流的基本范式。这里有一些在机器学习环境中有用的高级功能。</p><h2 id="0f0a" class="ix iy ht bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju dt translated">梯度计算</h2><p id="281f" class="pw-post-body-paragraph jv jw ht jx b jy jz ka kb kc kd ke kf ji kg kh ki jm kj kk kl jq km kn ko kp hm dt translated">ML中最常见的算法之一是梯度下降。该算法背后的中心思想是对特征的(猜测-实际)误差进行偏导数，然后尝试最小化该误差。导数——给定点的斜率给出了误差最小化的方向。因此tensorflow提供了一种内置的方法来计算图形的梯度。图形数据流非常符合现实世界中使用链式法则计算导数的方式。</p><p id="5789" class="pw-post-body-paragraph jv jw ht jx b jy kr ka kb kc ks ke kf ji kt kh ki jm ku kk kl jq kv kn ko kp hm dt translated">请参见下面的示例。在此，我们尝试对<em class="mb"> z </em> w.r.t <em class="mb"> x </em>取偏导数，即dz/dx。z，x和y是一维张量。给定<em class="mb"> z </em> w.r.t <em class="mb"> x </em>的偏导数为2x，这个程序的输出为<strong class="jx hu"><em class="mb">【array([20，40，60，80]，dtype=int32】)。</em> </strong>对于y等其他张量也可以做类似的梯度计算。</p><pre class="md me mf mg fq mq mr ms mt aw mu dt"><span id="de4f" class="ix iy ht mr b fv mv mw l mx my">import tensorflow as tf<br/>x = tf.constant([10, 20, 30, 40])<br/>y = tf.constant([1, 2, 3, 4])<br/>z = x * x + y/5<br/>s = tf.Session()<br/>g = tf.gradients(z, x)<br/>print(s.run(g))</span></pre><p id="8b2a" class="pw-post-body-paragraph jv jw ht jx b jy kr ka kb kc ks ke kf ji kt kh ki jm ku kk kl jq kv kn ko kp hm dt translated">见TensorFlow生成的图结构。它在左侧创建了一个操作树来表示函数。然后，它还通过探索z对x的依赖性来创建相应的梯度树。这可以通过跟踪依赖于x的张量z来完成，然后添加前向链接并计算沿路径的梯度。</p><figure class="md me mf mg fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="fe ff mz"><img src="../Images/1ebab6f83ae643ca613c1339b05db6cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_bdBwzUKah5Qwfth6J0v_w.png"/></div></div><figcaption class="ml mm fg fe ff mn mo bd b be z ek">LHS is the graph for Z = X*X + Y/5 and RHS is the derivative computation for the partial derivative dz/dx i.e. tf.gradients(z,x). See how gradient graph traverses in the opposite direction like the chain rule of composite functions.</figcaption></figure><p id="3961" class="pw-post-body-paragraph jv jw ht jx b jy kr ka kb kc ks ke kf ji kt kh ki jm ku kk kl jq kv kn ko kp hm dt translated">这种梯度计算会占用大量内存。除了通过使用高效排序快速释放临时变量的基本启发之外，更多未来的想法围绕着将内存从GPU转移到CPU等。</p><h2 id="684a" class="ix iy ht bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju dt translated">设备限制</h2><p id="adaa" class="pw-post-body-paragraph jv jw ht jx b jy jz ka kb kc kd ke kf ji kg kh ki jm kj kk kl jq km kn ko kp hm dt translated">可以使用约束来指定节点，例如在GPU上运行此节点。另一个有用的约束是:将一个变量放在具有特定节点的同一设备上，以获得更好的数据局部性。这会影响上面的节点放置算法。节点到设备放置算法首先为图节点找到可行的设备。然后，第二遍计算出哪些节点需要协同定位，然后是该集合的交集，以及前面描述的贪婪试探法，最终确定节点在设备上的放置</p><h2 id="13e7" class="ix iy ht bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju dt translated">控制流语句</h2><p id="ca9b" class="pw-post-body-paragraph jv jw ht jx b jy jz ka kb kc kd ke kf ji kg kh ki jm kj kk kl jq km kn ko kp hm dt translated">我们之前简单提到过用控制语句来扩充图数据流。TensoreFlow提供了类似开关、中断、for循环的结构。值得注意的是，因为循环中的不同节点可以放置在不同的设备上，所以类似for-loop的结构成为分布式循环控制和终止的问题。因此，图结构包括控制节点，并且这些控制节点在每次迭代的开始和迭代的结束以及最终循环的终止期间相互通信。</p><h2 id="bc27" class="ix iy ht bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju dt translated">行列</h2><p id="5575" class="pw-post-body-paragraph jv jw ht jx b jy jz ka kb kc kd ke kf ji kg kh ki jm kj kk kl jq km kn ko kp hm dt translated">TensorFlow中的队列结构可用于异步计算。当一个图节点完成生成其输出时，它可以将该数据排队，消费者节点可以在以后准备好时将其出队。类似地，可以将一些数据预取到队列中，以便设备可以在完成之前的计算后立即开始处理这些数据。队列也可用于批量梯度计算。除了标准FIFO队列之外，混排队列对于数据的随机化也很有用——这是ML中的另一个常见需求。</p><h2 id="e227" class="ix iy ht bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju dt translated">容器</h2><p id="57ce" class="pw-post-body-paragraph jv jw ht jx b jy jz ka kb kc kd ke kf ji kg kh ki jm kj kk kl jq km kn ko kp hm dt translated">容器内部成为持久状态的后备存储，就像变量所使用的那样。容器的范围被绑定到流程的开始和结束。但是命名容器可以用于长期状态和跨设备。</p><h1 id="302d" class="lk iy ht bd iz ll lm ln jd lo lp lq jh lr ls lt jl lu lv lw jp lx ly lz jt ma dt translated">最佳化</h1><p id="5097" class="pw-post-body-paragraph jv jw ht jx b jy jz ka kb kc kd ke kf ji kg kh ki jm kj kk kl jq km kn ko kp hm dt translated">在某些模型上，Tensorflow比dist faith快大约6倍。TensoreFlow中的整体优化侧重于:</p><ol class=""><li id="841a" class="kw kx ht jx b jy kr kc ks ji ky jm kz jq la kp lb lc ld le dt translated">消除执行相同操作并具有相同数据副本的公共子图，然后使这些子图指向公共节点。</li><li id="249f" class="kw kx ht jx b jy lf kc lg ji lh jm li jq lj kp lb lc ld le dt translated">通信相关的改进集中在延迟接收节点的执行，直到它们准备好接收一些数据</li><li id="29fc" class="kw kx ht jx b jy lf kc lg ji lh jm li jq lj kp lb lc ld le dt translated">与其他异步系统类似，异步内核提供了一种在计算完成时调用回调的方法，而不是等待计算完成。</li><li id="fbe9" class="kw kx ht jx b jy lf kc lg ji lh jm li jq lj kp lb lc ld le dt translated">TensorFlow主要使用Eigen等优化库进行线性代数相关计算。</li><li id="fe8f" class="kw kx ht jx b jy lf kc lg ji lh jm li jq lj kp lb lc ld le dt translated">许多最大似然算法允许近似浮点运算。当浮点数跨设备边界传输时，TensorFlow在尾数部分使用较少的位数。</li></ol><h2 id="e870" class="ix iy ht bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju dt translated">结论</h2><p id="d1d0" class="pw-post-body-paragraph jv jw ht jx b jy jz ka kb kc kd ke kf ji kg kh ki jm kj kk kl jq km kn ko kp hm dt translated">总的来说，我发现基于数据流的方法非常有趣。图形结构和一些梯度计算和相关的见解是非常强大的IMO。总的来说，这种范式对于像ML中流行的数据丰富的应用程序来说似乎非常强大。</p><blockquote class="na"><p id="2af4" class="nb nc ht bd nd ne nf ng nh ni nj kp ek translated"><a class="ae kq" href="https://coincodecap.com/?utm_source=coinmonks" rel="noopener ugc nofollow" target="_blank">在您的收件箱中直接获得最佳软件交易</a></p></blockquote><figure class="nl nm nn no np iu fe ff paragraph-image"><a href="https://coincodecap.com/?utm_source=coinmonks"><div class="fe ff nk"><img src="../Images/7c0b3dfdcbfea594cc0ae7d4f9bf6fcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/0*OJ-qb5G6i863msBB.png"/></div></a></figure></div></div>    
</body>
</html>