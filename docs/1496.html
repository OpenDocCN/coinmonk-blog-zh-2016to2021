<html>
<head>
<title>Practical Reinforcement Learning pt. 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实用强化学习。一</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/practical-reinforcement-learning-pt-1-5d45b029f93c?source=collection_archive---------0-----------------------#2018-09-13">https://medium.com/coinmonks/practical-reinforcement-learning-pt-1-5d45b029f93c?source=collection_archive---------0-----------------------#2018-09-13</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><figure class="fi fk ir is it iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff iq"><img src="../Images/aa378a2494e4c50a4370d2d421c0ec15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GiPU7mpUlcbMpRDmRWzzhg.png"/></div></div><figcaption class="jb jc fg fe ff jd je bd b be z ek">RL in Action</figcaption></figure><h2 id="ee1e" class="jf jg ht bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc dt translated">介绍</h2><p id="f621" class="pw-post-body-paragraph kd ke ht kf b kg kh ki kj kk kl km kn jq ko kp kq ju kr ks kt jy ku kv kw kx hm dt translated">在接下来的几篇文章中，我想开始探索RL如何应用于家庭机器人应用，比如这篇博客中的MARVIN bot。我希望从直观地介绍强化学习背后的思想开始，然后朝着我认为适合实际应用的方法努力。一旦找到了，我将在MARVIN身上实现这个方案，看看它在实际机器人上的表现如何。</p><p id="abd1" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">我们开始吧！</p><h2 id="d56a" class="jf jg ht bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc dt translated">强化学习</h2><p id="5d6b" class="pw-post-body-paragraph kd ke ht kf b kg kh ki kj kk kl km kn jq ko kp kq ju kr ks kt jy ku kv kw kx hm dt translated">在进入我希望探索的方法之前，为那些可能还不熟悉强化学习(RL)的人提供一个简短的介绍是有意义的。</p><p id="9652" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">从最广泛的角度来说，RL包括训练一个主体学会根据它从环境中得到的奖励(或惩罚)来执行一个动作。训练一只狗是我见过的理解这项任务实际执行的最好例子。当一只狗被训练时(假设它使用的是现代积极训练技术，而不是基于力量的训练)，每一个正确的动作都会得到某种形式的奖励。例如，你让狗坐下，它坐下了，你给它立即的奖励。如果它不坐下，就没有奖励。</p><p id="8aa9" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">RL工作在一个相似的前提下——代理根据其环境做出的每一个正确的动作都会得到一个奖励。(如果我们愿意，我们也可以提供负面奖励)。代理人开始时不知道哪些行为会带来最好的回报，他的任务是学习最大化他所收到的回报。</p><p id="8b38" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">上图中可以看到一个具体的例子，取自Udacity的Deep RL项目。这里的目标是让手臂的抓手接触绿色圆柱体。为接触滚筒和稳定地向其移动夹具提供奖励。该代理控制机器人手臂上各种关节的速度，同时接收来自指向场景的摄像机的输入。</p><p id="74c5" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">让我们快速看一下RL是如何工作的。首先考虑离散的情况更简单。在离散情况下，代理经历以下周期:</p><ol class=""><li id="fb34" class="ld le ht kf b kg ky kk kz jq lf ju lg jy lh kx li lj lk ll dt translated">它感知或接收来自环境的某种输入。</li><li id="27ac" class="ld le ht kf b kg lm kk ln jq lo ju lp jy lq kx li lj lk ll dt translated">它在环境中执行一个动作。</li><li id="d076" class="ld le ht kf b kg lm kk ln jq lo ju lp jy lq kx li lj lk ll dt translated">它从环境中获得回报(可能是零)。</li></ol><figure class="ls lt lu lv fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff lr"><img src="../Images/19e0ea7773adfa7ced38b27c5299fd7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mSapZsfEfl_-Jbv6hpcXGg.png"/></div></div><figcaption class="jb jc fg fe ff jd je bd b be z ek">Sense, Action, Reward</figcaption></figure><blockquote class="lw lx ly"><p id="d7b8" class="kd ke lz kf b kg ky ki kj kk kz km kn ma la kp kq mb lb ks kt mc lc kv kw kx hm dt translated">为了讨论的目的，我将引用代理在“感知”阶段收集的数据，如代理所述。该信息包括代理可以访问的关于其自身配置和环境配置的任何数据。</p></blockquote><p id="c37b" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">从一个不连续的观点来看这个问题，代理人在环境中经历的整个历史可以被认为是一个(状态、动作、奖励)三元组的序列。如果我们想知道一个代理到目前为止得到的总奖励，我们只需要把它历史上所有奖励的总和加起来。</p><figure class="ls lt lu lv fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff md"><img src="../Images/ff912a30d093ac8de4f99ecd833c6afa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p7K92bzqgG4SVDij81qhVg.png"/></div></div><figcaption class="jb jc fg fe ff jd je bd b be z ek">A Simple Maze</figcaption></figure><p id="4f60" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">我们如何利用这一事实？让我们来看一个非常简单的玩具问题——上图所示的迷宫。我们无畏的迷宫解算者发现他们在菱形处有两种可能的行动选择。绿色的选择最终会让他们得到绿色的奖励，反之亦然。</p><p id="b2d9" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">如果我们事先知道回报，我们就可以计算每个选择的回报。在这种情况下，图像被标注了适当的奖励，因此我们可以看到:</p><ul class=""><li id="a8cd" class="ld le ht kf b kg ky kk kz jq lf ju lg jy lh kx me lj lk ll dt translated">返回红色:75</li><li id="ef2f" class="ld le ht kf b kg lm kk ln jq lo ju lp jy lq kx me lj lk ll dt translated">返回绿色:21</li></ul><p id="31f0" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">应该清楚的是，如果代理人想要最大化其回报，它应该选择红色选项。换个角度来看，向上移动为我们赢得75分，向下移动为我们赢得21分。我们想往上走！</p><p id="261d" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">这里做了一些关键的假设——值得注意的是，我们不仅知道环境的完整结构，而且知道每种奖励会是什么。这是一个巨大的优势，在任何实际应用中我们都不能依赖它。</p><p id="28a2" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">在现实世界中，我们必须<em class="lz">根据我们以前的经验估算</em>每一个行动的回报。让我们来看看另一张地图，看看这是如何改变现状的:</p><figure class="ls lt lu lv fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff md"><img src="../Images/82172332aa765fdef3714383a8347811.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BzInqIy0r_H1HxgWOsgGWw.png"/></div></div><figcaption class="jb jc fg fe ff jd je bd b be z ek">A Mysterious Maze!</figcaption></figure><p id="0f30" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">在我们的新场景中，代理只对两种选择有经验——当它恰好上升一次时会发生什么，当它下降时会发生什么。它不知道移动后动作的结果应该是什么。纯粹基于手头的信息，如果代理想要最大化，它需要<em class="lz">利用</em>向下方向具有最高已知分数的事实，并向绿色箭头移动。</p><p id="356f" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">然而，这揭示了一个新的问题，如果找到红点比所有可能的下一步行动更有价值呢？我们目前天真地选择我们能看到的最高分选项的策略实际上会导致低于标准分的分数。如果我们想找到最佳选择，我们将不得不进行某种<em class="lz">探索。</em></p><p id="5fd1" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">我们如何决定何时<em class="lz">探索</em>以及何时<em class="lz">利用</em>？最常见的方法是概率性地做这件事。使用这种方法，代理从随机探索每个节点的100%动作开始。随着时间的推移——基于代理查看环境的次数，或其他一些适合于任务的衡量标准——这个值逐渐降低，代理有时随机移动，其余时间利用最高的可用奖励。</p><p id="4803" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">然而，这留下了一个问题——代理“学习”的到底是什么。我们学到的是状态、奖励和行动之间的关系。在一个典型的例子中，代理从一张白纸开始——它不知道环境中可用的奖励。</p><figure class="ls lt lu lv fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff mf"><img src="../Images/8b48349d4665cdc3ffd9419f84095dbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*17y9ewY62p0kYjhMfjxVLA.png"/></div></div><figcaption class="jb jc fg fe ff jd je bd b be z ek">What Rewards?</figcaption></figure><p id="7a36" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">让我们暂时忽略<em class="lz">探索</em> / <em class="lz">开发</em>问题，假设我们的代理正在系统地搜索环境。它探索绿色圆圈上方、下方、左侧和右侧的方格，获得下图所示的奖励:</p><figure class="ls lt lu lv fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff mf"><img src="../Images/d110d69405e24b0842efa61ca2c7a6fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Aot2wnnxFbihCqNGzgdoqA.png"/></div></div><figcaption class="jb jc fg fe ff jd je bd b be z ek">Preliminary Rewards</figcaption></figure><p id="7f91" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">在一次迭代之后，代理从“绿点”状态中了解到，如果:</p><ul class=""><li id="9ff8" class="ld le ht kf b kg ky kk kz jq lf ju lg jy lh kx me lj lk ll dt translated">向上移动得到15分</li><li id="1e9a" class="ld le ht kf b kg lm kk ln jq lo ju lp jy lq kx me lj lk ll dt translated">向左移动得到11分</li><li id="636a" class="ld le ht kf b kg lm kk ln jq lo ju lp jy lq kx me lj lk ll dt translated">向下移动得到8分</li><li id="026e" class="ld le ht kf b kg lm kk ln jq lo ju lp jy lq kx me lj lk ll dt translated">向右移动得到12分</li></ul><p id="3666" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">这很简单，让我们更进一步。假设经过几次迭代后，代理探索黄色方块，得到下图所示的奖励:</p><figure class="ls lt lu lv fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff mf"><img src="../Images/ed58271fe2758238e34be10727920da4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*meAvc5os3sA-gV3IYtNn-w.png"/></div></div><figcaption class="jb jc fg fe ff jd je bd b be z ek">Updated Rewards</figcaption></figure><p id="7e2f" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">我们现在必须回答一个新问题——根据当前的信息，应该给“升职”行动分配什么奖励？我们知道，在一次“移动”之后，代理人会得到15分。第二招呢？我们可以假设代理试图最大化其当前得分，所以它会选择值6分的选项。</p><p id="293b" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">把这些加起来，我们看到在这种情况下“向上移动”行动的总奖励应该是21分。我们可以遵循类似的策略来填充网格中的其余值，始终假设代理人将最大化其总报酬。我已经提供了一个例子，这个例子在未来的一步中会更好地说明这一点:</p><figure class="ls lt lu lv fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff mf"><img src="../Images/7297deaf1b2820e89dbe39599f92db4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CoWz1RsnVA5guLcZNGG2CA.png"/></div></div><figcaption class="jb jc fg fe ff jd je bd b be z ek">New Rewards provide a change in behavior</figcaption></figure><p id="6149" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">从更新后的网格中可以看出，多一点的信息会导致代理改变其在世界中的路径。以前我们有一条“向上，向左”的路径，现在我们可以看到这条路径已经变成了“向上，向上，向右”。</p><p id="2ac7" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">在这个映射中，我们指定在每个状态下要采取的动作被称为<em class="lz">策略</em>。看待RL正在做的事情的一种方式是将其视为学习代理要遵循的策略。在这种情况下，学习包括每当发现新信息时不断更新策略。</p><p id="ec51" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">关于政策，我们能说的很少:</p><ul class=""><li id="6a6f" class="ld le ht kf b kg ky kk kz jq lf ju lg jy lh kx me lj lk ll dt translated">我们可以根据回报给他们打分，例如，给定上述政策(上，上，右)的值为33分，但政策(上，左)仅值21分。</li><li id="8f85" class="ld le ht kf b kg lm kk ln jq lo ju lp jy lq kx me lj lk ll dt translated">我们可以根据分数来比较策略—这里我们可以说策略(上，上，右)&gt;(上，左)，因为前一个策略比后一个策略给出了更高的回报。</li><li id="a4b0" class="ld le ht kf b kg lm kk ln jq lo ju lp jy lq kx me lj lk ll dt translated">我们可以为最优策略制定一个定义——它是一组(状态、动作)对，这些对产生的回报是其他策略无法比拟的。</li></ul><p id="2f83" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">我们已经接近这个系列的第一部分的结尾了，但是系统中还有一个缺陷我们还没有解决——我们应该允许这个政策走多远？</p><p id="1cc0" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">让我们回到网格中来看一个更进一步的例子——如果我们不是停留在(上，左)处，而是进一步扩展先前的“劣势”策略(上，左，上，右，右)会怎么样？这一新政策的得分将高于前一个政策，因为它能够利用比其他政策获得更多奖励的优势。现在我们可以决定所有的策略*必须*有相同的动作名称，所以如果我们从一个地方开始，我们总是精确地走27步，但这不是真实世界的代表。在现实世界中，我们可以采取多少行动没有预先定义的限制(至少在实际意义上)。</p><p id="0979" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">为了解决这个问题，我们可以引入“贴现回报”的概念。这里的想法是，我们更喜欢“接近”我们当前状态的奖励，而不是更远的奖励。下图应该有助于进一步说明这一点:</p><figure class="ls lt lu lv fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff mg"><img src="../Images/56157986834ea00798cc656fabffea62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0NGtrr8G8oNds9aHis4SKw.png"/></div></div><figcaption class="jb jc fg fe ff jd je bd b be z ek">Discounting the Reward</figcaption></figure><p id="fc9e" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">在上面的例子中，我们将代理人采取的每一步的奖励打了50%的折扣。黄色三角形表示进入网格中的给定方块所获得的奖励加上了多少折扣奖励，而白色三角形表示获得的总奖励(从策略结束时向后计算)。这里的50%值被称为<em class="lz">贴现因子。</em></p><p id="e5d2" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">这里我们可以看到，我们非常重视接近代理当前位置的奖励，而对那些需要更长时间才能达到的位置则大打折扣。事实上，在这个例子中，每个额外步骤的值会很快接近零。</p><p id="75a5" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">这个<em class="lz">折扣因子</em>通常是代理设计者为世界上的代理设置的常数。它允许我们模拟遥远国家对代理人决策的影响程度，通常在0到1之间。在这种情况下，1表示所有步骤的权重应该相等(这是我们在本文中引入折扣因子之前的做法)，0表示只有紧接的下一个步骤才对代理的决策有任何影响。</p><p id="35a7" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">我提供了另一个图像来帮助形象化折扣因子的两个极端选择对下图中的代理行为的影响。左边的绿色箭头显示了如果折扣因子设置为0，代理将通过我们的小网格世界的路径，而右边的箭头显示了如果折扣因子设置为1，代理将采取的不同路径。</p><figure class="ls lt lu lv fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff mh"><img src="../Images/1442636e8bca2f9ea38c89e482f04093.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*52yaV9AXMcpKK7f-afGFPg.png"/></div></div><figcaption class="jb jc fg fe ff jd je bd b be z ek">A change of Policy</figcaption></figure><p id="cc40" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">这对于一篇文章来说已经足够了——在我的下一篇文章中，我将扩展到目前为止所解释的内容，开始构建一个个人机器人可能感兴趣的简单RL问题。</p><p id="bfe4" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">在那之前，</p><p id="c2a8" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated">分享享受！</p><h2 id="c5b5" class="jf jg ht bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc dt translated">参考</h2><p id="d8f6" class="pw-post-body-paragraph kd ke ht kf b kg kh ki kj kk kl km kn jq ko kp kq ju kr ks kt jy ku kv kw kx hm dt translated">下面是一些很好的参考资料，帮助我学习强化学习背后的概念:</p><p id="b887" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated"><a class="ae mi" href="http://incompleteideas.net/book/bookdraft2017nov5.pdf" rel="noopener ugc nofollow" target="_blank">强化学习导论萨顿和巴尔托2017 </a></p><p id="741a" class="pw-post-body-paragraph kd ke ht kf b kg ky ki kj kk kz km kn jq la kp kq ju lb ks kt jy lc kv kw kx hm dt translated"><a class="ae mi" href="https://www.udacity.com/course/reinforcement-learning--ud600" rel="noopener ugc nofollow" target="_blank">优达城的强化学习课程</a></p><blockquote class="mj"><p id="46f8" class="mk ml ht bd mm mn mo mp mq mr ms kx ek translated"><a class="ae mi" href="https://coincodecap.com/?utm_source=coinmonks" rel="noopener ugc nofollow" target="_blank">在您的收件箱中直接获得最佳软件交易</a></p></blockquote><figure class="mu mv mw mx my iu fe ff paragraph-image"><a href="https://coincodecap.com/?utm_source=coinmonks"><div class="fe ff mt"><img src="../Images/7c0b3dfdcbfea594cc0ae7d4f9bf6fcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/0*OJ-qb5G6i863msBB.png"/></div></a></figure></div></div>    
</body>
</html>