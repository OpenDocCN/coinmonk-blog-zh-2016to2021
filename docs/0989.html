<html>
<head>
<title>Multi-Label Classification(Blog Tags Prediction)using NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用自然语言处理的多标签分类(博客标签预测)</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/multi-label-classification-blog-tags-prediction-using-nlp-b0b5ee6686fc?source=collection_archive---------0-----------------------#2018-07-10">https://medium.com/coinmonks/multi-label-classification-blog-tags-prediction-using-nlp-b0b5ee6686fc?source=collection_archive---------0-----------------------#2018-07-10</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><figure class="fi fk ir is it iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff iq"><img src="../Images/c80b7253b36fefbb7f3aea393eb99289.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8WV4pwk4PDkaimvtthv3Tg.png"/></div></div></figure><h1 id="c7d0" class="jb jc ht bd jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy dt translated">多类分类和多标签分类的区别？</h1><p id="2296" class="pw-post-body-paragraph jz ka ht kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw hm dt translated">多类分类是指在Y轴或目标变量中有多个相关的类别，但每一行数据都属于一个类别。</p><p id="063c" class="pw-post-body-paragraph jz ka ht kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv kw hm dt translated">其中在多标签分类中，多个类别与相同的数据相关联。简单地说，每一行可能有多个分类值。</p><figure class="ld le lf lg fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff lc"><img src="../Images/9eab05f43517ce58c4c87f52d2dcfc61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zLNliKf3gh0Z-ER8j_V5oQ.png"/></div></div><figcaption class="lh li fg fe ff lj lk bd b be z ek">This is a binary classification problem</figcaption></figure><p id="0c75" class="pw-post-body-paragraph jz ka ht kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv kw hm dt translated">看到上面的数据集，我们的分类值基本上是“传奇”→它有一个二进制值(真或假)</p><p id="907a" class="pw-post-body-paragraph jz ka ht kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv kw hm dt translated">现在让我们看看我们将要处理的数据:</p><figure class="ld le lf lg fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff ll"><img src="../Images/22e08fd194796edb2884b0f2eec2f60d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*THF82C5VQ7VkLKCCuKyC1Q.png"/></div></div><figcaption class="lh li fg fe ff lj lk bd b be z ek">This is a multi-label dataset</figcaption></figure><p id="e168" class="pw-post-body-paragraph jz ka ht kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv kw hm dt translated">看这个数据集，每一行都有多个相关的值。</p><h1 id="600a" class="jb jc ht bd jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy dt translated">工作流程是这样的。</h1><ol class=""><li id="86d6" class="lm ln ht kb b kc kd kg kh kk lo ko lp ks lq kw lr ls lt lu dt translated">从网上抓取数据</li><li id="07cf" class="lm ln ht kb b kc lv kg lw kk lx ko ly ks lz kw lr ls lt lu dt translated">清理和预处理</li><li id="5658" class="lm ln ht kb b kc lv kg lw kk lx ko ly ks lz kw lr ls lt lu dt translated">设想</li><li id="0e3f" class="lm ln ht kb b kc lv kg lw kk lx ko ly ks lz kw lr ls lt lu dt translated">分类</li></ol><h1 id="19a2" class="jb jc ht bd jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy dt translated">目标:</h1><p id="9304" class="pw-post-body-paragraph jz ka ht kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw hm dt translated">在这个项目中，我们将从媒体中抓取数据，识别给定的标签，并以OneHotEncoding格式制作一个数据帧，然后。将哪些博客文章归入哪些标签。</p><h1 id="d14e" class="jb jc ht bd jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy dt translated">让我们开始吧:</h1><blockquote class="ma"><p id="00a7" class="mb mc ht bd md me mf mg mh mi mj kw ek translated">好的，我正在从介质本身收集数据:</p></blockquote><blockquote class="mk ml mm"><p id="d05e" class="jz ka mn kb b kc mo ke kf kg mp ki kj mq mr km kn ms mt kq kr mu mv ku kv kw hm dt translated">从bs4导入BeautifulSoup组<br/>导入urllib3</p></blockquote><p id="afaa" class="pw-post-body-paragraph jz ka ht kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv kw hm dt translated">导入所需的库<strong class="kb hu"> <em class="mn">熊猫</em> </strong>用于数据帧，导入所需的库<strong class="kb hu"> <em class="mn"> urllib3 </em> </strong>用于连接网络和获取数据。<strong class="kb hu"> <em class="mn">美</em> </strong> <strong class="kb hu"> <em class="mn">汤</em> </strong>是一个解析HTML和XML文档的Python包。它为解析过的页面创建了一个解析树，可以用来从HTML中提取数据，这对web抓取很有用。</p><blockquote class="mk ml mm"><p id="e7a7" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated">http=urllib3。PoolManager() <br/>从熊猫导入DataFrame <br/> column=['Title '，' Body ']<br/>dfBA = data frame(columns = column)<br/>dfT = data frame(columns =[0，1，2，3，4])</p></blockquote><p id="1257" class="pw-post-body-paragraph jz ka ht kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv kw hm dt translated">为标题和正文创建空数据帧<strong class="kb hu"> column=['Title '，' Body ']dfBA = data frame(columns = column)</strong></p><p id="6624" class="pw-post-body-paragraph jz ka ht kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv kw hm dt translated">还有一个标签<strong class="kb hu"> dfT=DataFrame(columns=[0，1，2，3，4]) </strong></p><blockquote class="mk ml mm"><p id="33e6" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated"><strong class="kb hu"><em class="ht">def spider(link):</em></strong><br/>print(link)<br/>blog data = http . request(' GET '，link)<br/>soup = beautiful soup(blog data . data，' html . parser ')<br/>for soup . find _ all(' div '，{ ' class ':' post article-read more ' })中的链接:<br/> link=links.find('a ')。get(' href ')<br/>CrawlAndFrame(link)</p></blockquote><p id="73f8" class="pw-post-body-paragraph jz ka ht kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv kw hm dt translated">蜘蛛功能将进入网页，并获得网页中所有帖子的链接。</p><h2 id="53b0" class="mw jc ht bd jd mx my mz jh na nb nc jl kk nd ne jp ko nf ng jt ks nh ni jx nj dt translated">注意:由于urllib3不擅长动态抓取，它每页只能抓取7篇文章。使用硒进行动态刮痧</h2><blockquote class="mk ml mm"><p id="5d72" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated"><strong class="kb hu"><em class="ht">def CrawlAndFrame(link)</em></strong><em class="ht">:</em><br/><strong class="kb hu">try:</strong><br/>print(link)<br/>blog data = http . request(' GET '，link)<br/>soup = beautiful soup(blog data . data，' html . parser ')<br/>article = ' '<br/>tags =[]<br/>heading = soup . find(' h1 ')。text<br/>for para in soup . find _ all(' p '):<br/>p = para . text<br/>p = p . strip(' \ u ')<br/>article = article+' '+p<br/>for mtags in soup . find _ all(' a '，{ ' class ':' link u-base color—link ' }):<br/>tags . append(mtags . text)<br/># CreateDataFrame(list())<br/>someList =[heading，article，tuple(</p></blockquote><p id="d649" class="pw-post-body-paragraph jz ka ht kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv kw hm dt translated"><strong class="kb hu"><em class="mn">CrawlAndFrame()</em></strong>函数进入蜘蛛收集的每一个链接，并从那里收集带有博客中涉及的<strong class="kb hu"> <em class="mn">标签</em> </strong>的博客的所有博客<strong class="kb hu"> <em class="mn">文章</em> </strong>和<strong class="kb hu"> <em class="mn">标题</em> </strong>并传递它们<strong class="kb hu"><em class="mn">CreateDataFrame(someList)</em></strong>为head和</p><blockquote class="mk ml mm"><p id="e66f" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated"><strong class="kb hu">def CreateDataFrame(some list):</strong><br/>t = { }<br/>d = { ' Title ':[some list[0]，' Body]:[some list[1]]}<br/>for n in range(5):<br/>if len(some list[2])&gt;n:<br/>t[n]=[some list[2][n]]<br/>else:<br/>t[n]=[' 0 ']<br/>toDf</p></blockquote><p id="3100" class="pw-post-body-paragraph jz ka ht kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv kw hm dt translated"><strong class="kb hu"><em class="mn">CreateDataFrame()</em></strong>分别为(标题和正文)和(标签)创建<strong class="kb hu"> dfDA </strong>和<strong class="kb hu"> dfT </strong>的数据帧</p><figure class="ld le lf lg fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff fg"><img src="../Images/b1727b646a2797ebad3cd5124a32c547.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vgp1IiWbO3QSvieR84yH5Q.png"/></div></div><figcaption class="lh li fg fe ff lj lk bd b be z ek">dfBA</figcaption></figure><figure class="ld le lf lg fq iu fe ff paragraph-image"><div class="fe ff nk"><img src="../Images/6c2996a5a89eab3da54c05c289be892c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*V7rb8W0wRm_TbWFOiAZuzA.png"/></div><figcaption class="lh li fg fe ff lj lk bd b be z ek">dfT</figcaption></figure><p id="aebe" class="pw-post-body-paragraph jz ka ht kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv kw hm dt translated">查看<strong class="kb hu"> dfT </strong>您可能会发现这不是标记数据的最佳方式。因此，我们需要将其转换为OneHotEncoding格式(<strong class="kb hu"> <em class="mn">)，这基本上是创建一个由0和1组成的稀疏矩阵，其中1表示索引标签存在，0表示它不存在</em> </strong>)</p><p id="2057" class="pw-post-body-paragraph jz ka ht kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv kw hm dt translated">让我们开始吧:</p><blockquote class="mk ml mm"><p id="1e2a" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated">ok list =[]<br/>for cl in dfT . columns:<br/>for n in dfT[cl]:<br/>ok list . append(n)<br/>ok list = list(set(ok list))<br/>del(ok list[ok list . index(' 0 ')))<br/>newDF = data frame(columns = ok list)<br/>for x in range(dfT . count()[0]):<br/>someDict = { }<br/>for d in ok</p></blockquote><blockquote class="ma"><p id="5a60" class="mb mc ht bd md me nl nm nn no np kw ek translated">简而言之，我在这里所做的只是获取列表中所有唯一的标签，并将它们作为我的数据帧的列，如果该标签存在于行中，则放置1，如果不存在，则放置0</p></blockquote><figure class="nr ns nt nu nv iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff nq"><img src="../Images/357ee4d2435e7f297d547a05b510c0bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PTrY1rOQm7km9RwQnYGRjQ.png"/></div></div><figcaption class="lh li fg fe ff lj lk bd b be z ek">We thus got a OneHotEncoded Data frame from the tags</figcaption></figure><p id="ceed" class="pw-post-body-paragraph jz ka ht kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv kw hm dt translated">现在我们的数据已经准备好了，让我们开始预处理它</p><blockquote class="mk ml mm"><p id="a821" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated">从nltk.corpus导入stop words<br/>stop words list = stop words . words(' English ')<br/>stop wordlist . remove(' no ')<br/>stop wordlist . remove(' not ')<br/><strong class="kb hu">def remove tags(data):</strong><br/>soup = beautiful soup(data，' html . parser ')<br/>text = soup . get _ text()<br/>return text<br/>导入unicodedata</p></blockquote><blockquote class="ma"><p id="8bc4" class="mb mc ht bd md me nl nm nn no np kw ek translated">这将删除每个html标签。如果有的话。有时，即使刮擦后，一些标签仍然存在。我们正在移除它。</p></blockquote><blockquote class="mk ml mm"><p id="8b4f" class="jz ka mn kb b kc mo ke kf kg mp ki kj mq mr km kn ms mt kq kr mu mv ku kv kw hm dt translated"><strong class="kb hu">def removeAscendingChar(data):</strong><br/>data = unicode data . normalize(' NFKD '，data)。编码(' ascii '，' ignore ')。decode('utf-8 '，' ignore') <br/>返回数据</p></blockquote><blockquote class="ma"><p id="d9a3" class="mb mc ht bd md me nl nm nn no np kw ek translated">这个函数将所有带重音的字符转换成正常的英语。</p></blockquote><figure class="nr ns nt nu nv iu fe ff paragraph-image"><div class="fe ff nw"><img src="../Images/82845ced103823c526c92a3fe604d1d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*OKud8hfMHa_z0Vxa5jZ3rA.jpeg"/></div><figcaption class="lh li fg fe ff lj lk bd b be z ek">Accented characters</figcaption></figure><blockquote class="mk ml mm"><p id="11d5" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated"><strong class="kb hu">def removechardigit(text):</strong><br/>str='`1234567890-=~@#$%^&amp;*()_+[！{;&gt;:\ '&lt;。,/?" }]'<br/>for w in text:<br/>if w in str:<br/>text = text . replace(w，' ')<br/>return text<br/>from nltk . stem . wordnet import wordnet lemmatizer<br/>from nltk . tokenize import ToktokTokenizer<br/>lemma = wordnet lemmatizer()<br/>token = ToktokTokenizer()</p></blockquote><blockquote class="ma"><p id="8aa5" class="mb mc ht bd md me nl nm nn no np kw ek translated">删除所有特殊字符和数字</p></blockquote><blockquote class="mk ml mm"><p id="c7b7" class="jz ka mn kb b kc mo ke kf kg mp ki kj mq mr km kn ms mt kq kr mu mv ku kv kw hm dt translated"><strong class="kb hu">def lemitize words(text):<br/></strong>words = token . token ize(text)<br/>list lemma =[]<br/>for w in words:<br/>x = lemma . lemma tize(w，' v ')<br/>list lemma . append(x)<br/>return text<br/>def stopwodes remove(text):<br/>word list =[x . lower()。strip()for x in token . tokenize(text)]<br/>removed list =[x for x in word list if not x in stop wordList]<br/>text = ' '。join(removedList) <br/>返回文本</p></blockquote><blockquote class="ma"><p id="238c" class="mb mc ht bd md me nl nm nn no np kw ek translated">出于语法原因，文档将使用一个单词的不同形式，如<em class="nx">组织</em>、<em class="nx">组织</em>、<em class="nx">组织</em>。此外，还有具有相似含义的衍生相关词族，如<em class="nx">民主</em>、<em class="nx">民主</em>和<em class="nx">民主化</em>。在许多情况下，似乎搜索这些单词中的一个会返回包含集合中另一个单词的文档。</p><p id="4510" class="mb mc ht bd md me mf mg mh mi mj kw ek translated">词干化和词尾化的目标都是将一个词的屈折形式，有时还有派生相关形式简化为一个共同的基本形式。例如:</p><p id="e723" class="mb mc ht bd md me mf mg mh mi mj kw ek translated"><em class="nx"> am，are，is →be <br/> car，cars，cars，cars →car </em></p></blockquote><div class="nr ns nt nu nv ny"><a href="https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html" rel="noopener  ugc nofollow" target="_blank"><div class="nz ab ej"><div class="oa ab ob cl cj oc"><h2 class="bd hu fv z el od eo ep oe er et hs dt translated">词干化和词汇化</h2><div class="of l"><h3 class="bd b fv z el od eo ep oe er et ek translated">词干化和词汇化</h3></div><div class="og l"><p class="bd b gc z el od eo ep oe er et ek translated">词干和lemmatizationnlp.stanford.edu</p></div></div><div class="oh l"><div class="oi l oj ok ol oh om iz ny"/></div></div></a></div><blockquote class="mk ml mm"><p id="63b3" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated"><strong class="kb hu"> def预处理(text):</strong><br/>text = remove tags(text)<br/>text = removeCharDigit(text)<br/>text = removeAscendingChar(text)<br/>text = lemitizeWords(text)<br/>text = stopwordremove(text)<br/>return(text)</p><p id="5793" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated">导入re<br/><strong class="kb hu">def clean _ text(text):</strong><br/>text = text . lower()<br/>text = re . sub(r " what ' s "，" what is "，text)<br/>text = re . sub(r " \ s "，" text)<br/>text = re . sub(r " \ ve "，" have "，text) <br/> text = re.sub(r "不能"，"不能"，text) <br/> text = re.sub(r"n't "，"不是"，text) 【T】 text)<br/>text = re sub(r " \ ' scuse "，" excuse "，text)<br/>text = re sub(' \ W '，' '，text)<br/>text = re sub(' \ s+'，' '，text)<br/>text = text . strip(')<br/>返回文本</p></blockquote><blockquote class="ma"><p id="1557" class="mb mc ht bd md me nl nm nn no np kw ek translated">现在让我们把像<strong class="ak">我是</strong>这样的词改成<strong class="ak">我是</strong>或<strong class="ak">什么是</strong>什么是<strong class="ak">什么是</strong>，这正是clean_text正在做的事情。</p></blockquote><p id="d380" class="pw-post-body-paragraph jz ka ht kb b kc mo ke kf kg mp ki kj kk mr km kn ko mt kq kr ks mv ku kv kw hm dt translated">更多检查:</p><div class="on oo fm fo op ny"><a href="https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5" rel="noopener follow" target="_blank"><div class="nz ab ej"><div class="oa ab ob cl cj oc"><h2 class="bd hu fv z el od eo ep oe er et hs dt translated">使用Scikit-Learn进行多标签文本分类</h2><div class="of l"><h3 class="bd b fv z el od eo ep oe er et ek translated">多类分类是指具有两个以上类的分类任务；每个标签都是互斥的…</h3></div><div class="og l"><p class="bd b gc z el od eo ep oe er et ek translated">towardsdatascience.com</p></div></div><div class="oh l"><div class="oq l oj ok ol oh om iz ny"/></div></div></a></div><blockquote class="mk ml mm"><p id="77ee" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated">df['正文'] = df['正文']。map(lambda com:clean _ text(com))<br/>df[' Body ']= df[' Body ']。map(lambda com:预处理(com))</p></blockquote><h2 id="3842" class="mw jc ht bd jd mx my mz jh na nb nc jl kk nd ne jp ko nf ng jt ks nh ni jx nj dt translated">让我们想象一下:</h2><blockquote class="mk ml mm"><p id="627d" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated">total text = ' '<br/>for x in df[' Body ']:<br/>PS =预处理(x)<br/>total text = total text+" "+PS<br/>from word cloud导入word cloud<br/>WC = word cloud(background _ color = ' black '，max_font_size=50)。generate(total text)<br/>PLT . figure(figsize =(16，12)) <br/> plt.imshow(wc，interpolation= "双线性")</p></blockquote><figure class="ld le lf lg fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff iq"><img src="../Images/b571c191ccef85fb2bf5e6b9151d1bc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uRjATwZoh3uP2A42vYgebQ.png"/></div></div><figcaption class="lh li fg fe ff lj lk bd b be z ek">World cloud Representaion</figcaption></figure><h2 id="1335" class="mw jc ht bd jd mx my mz jh na nb nc jl kk nd ne jp ko nf ng jt ks nh ni jx nj dt translated">单词的频率呢？</h2><blockquote class="mk ml mm"><p id="8d1b" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated">从nltk.tokenize导入nltk<br/>toktokentizer<br/>x = nltk。FreqDist(ToktokTokenizer()。tokenize(total text))<br/>PLT . figure(figsize =(16，5)) <br/> x.plot(20)</p></blockquote><figure class="ld le lf lg fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff or"><img src="../Images/ed645c87080be5db3c3a10041fa592cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YvtoyWjsg_VusoNHN9A0kw.png"/></div></div><figcaption class="lh li fg fe ff lj lk bd b be z ek">Word frequency Plot</figcaption></figure><blockquote class="ma"><p id="6036" class="mb mc ht bd md me nl nm nn no np kw ek translated">正如你所看到的，文章中最常见的词是学习、数据、机器、人工智能等等</p></blockquote><h2 id="c48b" class="mw jc ht bd jd mx os mz jh na ot nc jl kk ou ne jp ko ov ng jt ks ow ni jx nj dt translated">分类(训练和测试模型):</h2><blockquote class="mk ml mm"><p id="7c90" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated">#使用二进制相关性<br/>从skmultillearn . problem _ transform导入二进制相关性<br/>从sklearn.naive_bayes导入GaussianNB</p><p id="c4a1" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated">#初始化二进制相关性多标签分类器<br/> #用基于高斯朴素贝叶斯分类器<br/>分类器=二进制相关性(GaussianNB())</p><p id="63d6" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated"># train <br/> classifier.fit(x，y)</p><p id="1098" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated"># predict<br/>predictions = classifier . predict(x)<br/>print(predictions . to array())<br/>print(accuracy _ score(y，predictions))</p></blockquote><p id="28d8" class="pw-post-body-paragraph jz ka ht kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv kw hm dt translated"><strong class="kb hu"> <em class="mn">什么是二元关联？？</em>T50】</strong></p><p id="d326" class="pw-post-body-paragraph jz ka ht kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv kw hm dt translated">这是最简单的技术，它基本上将每个标签视为一个单独的单个类分类问题。</p><p id="cbc5" class="pw-post-body-paragraph jz ka ht kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv kw hm dt translated">假设我们有<strong class="kb hu"> x </strong>作为自变量，y1，y2，y3作为因变量的标签。所以<strong class="kb hu"> <em class="mn">二元相关性</em> </strong>所做的是，考虑到自变量，它将每个自变量作为一个单独的类。</p><p id="d375" class="pw-post-body-paragraph jz ka ht kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv kw hm dt translated">所以它映射了</p><blockquote class="ma"><p id="4c0c" class="mb mc ht bd md me mf mg mh mi mj kw ek translated"><strong class="ak"> x →y1和x →y2和x →y3 </strong></p></blockquote><blockquote class="mk ml mm"><p id="fc5c" class="jz ka mn kb b kc mo ke kf kg mp ki kj mq mr km kn ms mt kq kr mu mv ku kv kw hm dt translated">#使用来自skmultillearn . problem _ transform的分类器链<br/>导入来自sklearn.naive _ bayes的分类器链<br/>导入高斯链<br/>导入来自sklearn.tree的决策树分类器</p><p id="b6bc" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated">#初始化分类器链多标签分类器<br/> #使用基于高斯朴素贝叶斯的分类器如果您愿意可以使用任何其他分类器<br/>#分类器=分类器链(GaussianNB()) <br/>分类器=分类器链(DecisionTreeClassifier()) <br/> #训练<br/>分类器. fit(x，y)</p><p id="f47b" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated"># predict <br/>预测= classifier.predict(x)</p><p id="2e32" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated">准确度_得分(y，预测值)</p></blockquote><h2 id="9d36" class="mw jc ht bd jd mx my mz jh na nb nc jl kk nd ne jp ko nf ng jt ks nh ni jx nj dt translated">什么是分类器链？？</h2><p id="f9a7" class="pw-post-body-paragraph jz ka ht kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw hm dt translated">在这种情况下，仅在输入数据上训练第一个分类器，然后在输入空间和链中所有先前的分类器上训练每个下一个分类器。</p><p id="c71c" class="pw-post-body-paragraph jz ka ht kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv kw hm dt translated">假设我们有<strong class="kb hu"> x </strong>作为自变量，y1，y2，y3作为因变量的标签。所以我们基本上有3个分类子集</p><blockquote class="ma"><p id="2451" class="mb mc ht bd md me mf mg mh mi mj kw ek translated">x →y1和x →y1，y2和x →y1，y2，y3</p></blockquote><blockquote class="mk ml mm"><p id="a117" class="jz ka mn kb b kc mo ke kf kg mp ki kj mq mr km kn ms mt kq kr mu mv ku kv kw hm dt translated">#使用来自skmultillearn . problem _ transform的Label Powerset <br/>导入来自sklearn.naive_bayes的LabelPowerset <br/>导入高斯</p><p id="1e33" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated">#初始化Label Powerset多标签分类器<br/> #用高斯朴素贝叶斯基分类器<br/>分类器= Label Powerset(GaussianNB())<br/>#或<br/>#分类器= classifier chain(decision tree classifier())<br/># train<br/>分类器. fit(x，y)</p><p id="7d89" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated"># predict <br/>预测= classifier.predict(x)</p><p id="f7f0" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated">准确度_得分(y，预测值)</p></blockquote><h2 id="68b1" class="mw jc ht bd jd mx my mz jh na nb nc jl kk nd ne jp ko nf ng jt ks nh ni jx nj dt translated">什么是标签Powerset？？</h2><p id="1147" class="pw-post-body-paragraph jz ka ht kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw hm dt translated">在这种情况下，我们将问题转化为多类问题，一个多类分类器在训练数据中找到的所有唯一标签组合上进行训练。</p><blockquote class="mk ml mm"><p id="8c65" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated">从skmultilearn.adapt导入MLkNN</p><p id="1150" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated">分类器= MLkNN(k=20)</p><p id="07ef" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated">classifier.fit(x，y)</p><p id="142b" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated"># predict<br/>predictions = classifier . predict(x)<br/>print(predictions . toarray())<br/>print(y)<br/>accuracy _ score(y，predictions)<br/>from sk learn . metrics导入f1_score <br/> print(f1_score(y，predictions，average='micro ')</p></blockquote><h2 id="f3b6" class="mw jc ht bd jd mx my mz jh na nb nc jl kk nd ne jp ko nf ng jt ks nh ni jx nj dt translated">MLkNN:</h2><p id="4310" class="pw-post-body-paragraph jz ka ht kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw hm dt translated">适配算法，顾名思义，适配算法直接进行多标签分类，而不是将问题转化为不同的问题子集。</p><p id="651f" class="pw-post-body-paragraph jz ka ht kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv kw hm dt translated">例如，多标签版本的kNN由MLkNN表示。</p><h2 id="3b85" class="mw jc ht bd jd mx my mz jh na nb nc jl kk nd ne jp ko nf ng jt ks nh ni jx nj dt translated">最后但同样重要的是，我们有一个分类器</h2><p id="5143" class="pw-post-body-paragraph jz ka ht kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw hm dt translated">也称为一对一，这种策略包括为每个类安装一个分类器。对于每个分类器，该类与所有其他类相匹配。除了其计算效率之外</p><blockquote class="mk ml mm"><p id="82a1" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated">从sklearn.naive_bayes导入多项式inb<br/>从sklearn.multiclass导入OneVsRestClassifier <br/>从sklearn.metrics导入accuracy_score</p><p id="ff34" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated">clf = OneVsRestClassifier(MultinomialNB())</p><p id="0055" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated">clf.fit(x，y)</p><p id="9f6b" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated">pred=clf.predict(x)</p><p id="9da3" class="jz ka mn kb b kc kx ke kf kg ky ki kj mq kz km kn ms la kq kr mu lb ku kv kw hm dt translated">准确度_得分(y，预测)</p></blockquote><h1 id="4a13" class="jb jc ht bd jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy dt translated">在这里下载完整的代码</h1><div class="on oo fm fo op ny"><a href="https://github.com/neelindresh/NeelBlog/blob/master/%5BProject%20Complete%20Tag%20Prediction%20%5DtagLabelprediction.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="nz ab ej"><div class="oa ab ob cl cj oc"><h2 class="bd hu fv z el od eo ep oe er et hs dt translated">neelindresh/NeelBlog</h2><div class="of l"><h3 class="bd b fv z el od eo ep oe er et ek translated">包含代码和csv从我的博客。通过在…上创建帐户，为neelindresh/NeelBlog的发展做出贡献</h3></div><div class="og l"><p class="bd b gc z el od eo ep oe er et ek translated">github.com</p></div></div><div class="oh l"><div class="ox l oj ok ol oh om iz ny"/></div></div></a></div></div><div class="ab cl oy oz hb pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="hm hn ho hp hq"><div class="ld le lf lg fq ny"><a href="https://www.analyticsvidhya.com/blog/2017/08/introduction-to-multi-label-classification/" rel="noopener  ugc nofollow" target="_blank"><div class="nz ab ej"><div class="oa ab ob cl cj oc"><h2 class="bd hu fv z el od eo ep oe er et hs dt translated">解决多标签分类问题(包括案例研究)</h2><div class="of l"><h3 class="bd b fv z el od eo ep oe er et ek translated">由于某种原因，回归和分类问题最终在机器中占据了大部分注意力…</h3></div><div class="og l"><p class="bd b gc z el od eo ep oe er et ek translated">www.analyticsvidhya.com</p></div></div><div class="oh l"><div class="pf l oj ok ol oh om iz ny"/></div></div></a></div><div class="on oo fm fo op ny"><a href="http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html" rel="noopener  ugc nofollow" target="_blank"><div class="nz ab ej"><div class="oa ab ob cl cj oc"><h2 class="bd hu fv z el od eo ep oe er et hs dt translated">sk learn . multi class . onevsrestclassifier-sci kit-learn 0 . 19 . 1文档</h2><div class="of l"><h3 class="bd b fv z el od eo ep oe er et ek translated">也称为一对一，这种策略包括为每个类安装一个分类器。对于每个分类器，类是…</h3></div><div class="og l"><p class="bd b gc z el od eo ep oe er et ek translated">scikit-learn.org</p></div></div><div class="oh l"><div class="pg l oj ok ol oh om iz ny"/></div></div></a></div><div class="on oo fm fo op ny"><a href="http://scikit.ml/api/classify.html" rel="noopener  ugc nofollow" target="_blank"><div class="nz ab ej"><div class="oa ab ob cl cj oc"><h2 class="bd hu fv z el od eo ep oe er et hs dt translated">选择多标签分类器- scikit-multilearn 0.0.5文档</h2><div class="of l"><h3 class="bd b fv z el od eo ep oe er et ek translated">算法自适应方法基于适用于多标签分类的单标签分类方法</h3></div><div class="og l"><p class="bd b gc z el od eo ep oe er et ek translated">scikit.ml</p></div></div></div></a></div></div><div class="ab cl oy oz hb pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="hm hn ho hp hq"><p id="c67c" class="pw-post-body-paragraph jz ka ht kb b kc kx ke kf kg ky ki kj kk kz km kn ko la kq kr ks lb ku kv kw hm dt translated">更多信息请关注我的博客:</p><div class="on oo fm fo op ny"><a href="https://dataneel.wordpress.com" rel="noopener  ugc nofollow" target="_blank"><div class="nz ab ej"><div class="oa ab ob cl cj oc"><h2 class="bd hu fv z el od eo ep oe er et hs dt translated">面向所有人的数据科学</h2><div class="of l"><h3 class="bd b fv z el od eo ep oe er et ek translated">这篇文章是关于支持向量回归的。从事机器学习或数据科学的人非常熟悉…</h3></div><div class="og l"><p class="bd b gc z el od eo ep oe er et ek translated">dataneel.wordpress.com</p></div></div><div class="oh l"><div class="ph l oj ok ol oh om iz ny"/></div></div></a></div><blockquote class="ma"><p id="3288" class="mb mc ht bd md me nl nm nn no np kw ek translated">加入Coinmonks <a class="ae pi" href="https://t.me/coincodecap" rel="noopener ugc nofollow" target="_blank">电报频道</a>和<a class="ae pi" href="https://www.youtube.com/c/coinmonks/videos" rel="noopener ugc nofollow" target="_blank"> Youtube频道</a>获取每日<a class="ae pi" href="http://coincodecap.com/" rel="noopener ugc nofollow" target="_blank">加密新闻</a></p></blockquote><h2 id="15d6" class="mw jc ht bd jd mx os mz jh na ot nc jl kk ou ne jp ko ov ng jt ks ow ni jx nj dt translated">另外，阅读</h2><ul class=""><li id="20fb" class="lm ln ht kb b kc kd kg kh kk lo ko lp ks lq kw pj ls lt lu dt translated"><a class="ae pi" rel="noopener" href="/coinmonks/top-10-crypto-copy-trading-platforms-for-beginners-d0c37c7d698c">复制交易</a> | <a class="ae pi" rel="noopener" href="/coinmonks/crypto-tax-software-ed4b4810e338">加密税务软件</a></li><li id="723e" class="lm ln ht kb b kc lv kg lw kk lx ko ly ks lz kw pj ls lt lu dt translated"><a class="ae pi" href="https://coincodecap.com/grid-trading" rel="noopener ugc nofollow" target="_blank">网格交易</a> | <a class="ae pi" rel="noopener" href="/coinmonks/the-best-cryptocurrency-hardware-wallets-of-2020-e28b1c124069">加密硬件钱包</a></li><li id="f33b" class="lm ln ht kb b kc lv kg lw kk lx ko ly ks lz kw pj ls lt lu dt translated"><a class="ae pi" rel="noopener" href="/coinmonks/crypto-exchange-dd2f9d6f3769">最佳加密交易所</a> | <a class="ae pi" rel="noopener" href="/coinmonks/bitcoin-exchange-in-india-7f1fe79715c9">印度最佳加密交易所</a></li><li id="47a8" class="lm ln ht kb b kc lv kg lw kk lx ko ly ks lz kw pj ls lt lu dt translated"><a class="ae pi" rel="noopener" href="/coinmonks/best-crypto-apis-for-developers-5efe3a597a9f">开发人员的最佳加密API</a></li><li id="f1dd" class="lm ln ht kb b kc lv kg lw kk lx ko ly ks lz kw pj ls lt lu dt translated"><a class="ae pi" href="http://Top 4 Telegram Channels for Crypto Traders" rel="noopener ugc nofollow" target="_blank">密码电报信号</a> | <a class="ae pi" rel="noopener" href="/coinmonks/crypto-trading-bot-c2ffce8acb2a">密码交易机器人</a></li><li id="b359" class="lm ln ht kb b kc lv kg lw kk lx ko ly ks lz kw pj ls lt lu dt translated">最佳<a class="ae pi" rel="noopener" href="/coinmonks/top-5-crypto-lending-platforms-in-2020-that-you-need-to-know-a1b675cec3fa">密码借贷平台</a></li><li id="9487" class="lm ln ht kb b kc lv kg lw kk lx ko ly ks lz kw pj ls lt lu dt translated">杠杆代币的终极指南</li><li id="95d1" class="lm ln ht kb b kc lv kg lw kk lx ko ly ks lz kw pj ls lt lu dt translated"><a class="ae pi" href="https://coincodecap.com/best-vpns-for-crypto-trading" rel="noopener ugc nofollow" target="_blank">加密交易的最佳VPN</a></li><li id="918f" class="lm ln ht kb b kc lv kg lw kk lx ko ly ks lz kw pj ls lt lu dt translated"><a class="ae pi" href="https://coincodecap.com/huobi-crypto-trading-signals" rel="noopener ugc nofollow" target="_blank">用于Huobi的加密交易信号</a> | <a class="ae pi" rel="noopener" href="/coinmonks/hitbtc-review-c5143c5d53c2"> HitBTC审查</a></li><li id="58f1" class="lm ln ht kb b kc lv kg lw kk lx ko ly ks lz kw pj ls lt lu dt translated"><a class="ae pi" href="https://coincodecap.com/traderwagon-review" rel="noopener ugc nofollow" target="_blank">贸易战回顾</a> | <a class="ae pi" href="https://coincodecap.com/kraken-vs-gemini-vs-bityard" rel="noopener ugc nofollow" target="_blank">北海巨妖对双子座对比特场</a></li><li id="ad10" class="lm ln ht kb b kc lv kg lw kk lx ko ly ks lz kw pj ls lt lu dt translated"><a class="ae pi" href="https://coincodecap.com/ftx-futures-trading" rel="noopener ugc nofollow" target="_blank">如何在FTX交易所交易期货</a></li><li id="71f4" class="lm ln ht kb b kc lv kg lw kk lx ko ly ks lz kw pj ls lt lu dt translated"><a class="ae pi" href="https://coincodecap.com/okex-kucoin" rel="noopener ugc nofollow" target="_blank"> OKEx vs KuCoin </a> | <a class="ae pi" href="https://coincodecap.com/celsius-alternatives" rel="noopener ugc nofollow" target="_blank">摄氏度替代品</a> | <a class="ae pi" href="https://coincodecap.com/buy-vechain" rel="noopener ugc nofollow" target="_blank">如何购买VeChain </a></li><li id="e6a8" class="lm ln ht kb b kc lv kg lw kk lx ko ly ks lz kw pj ls lt lu dt translated"><a class="ae pi" href="https://coincodecap.com/3commas-vs-pionex-vs-cryptohopper" rel="noopener ugc nofollow" target="_blank">3 commas vs Pionex vs Cryptohopper</a></li><li id="0539" class="lm ln ht kb b kc lv kg lw kk lx ko ly ks lz kw pj ls lt lu dt translated"><a class="ae pi" href="https://coincodecap.com/cornix-trading-bot" rel="noopener ugc nofollow" target="_blank">如何使用康沃尔交易机器人</a></li><li id="7ed0" class="lm ln ht kb b kc lv kg lw kk lx ko ly ks lz kw pj ls lt lu dt translated"><a class="ae pi" href="https://coincodecap.com/bitget-review" rel="noopener ugc nofollow" target="_blank">Bitget Review</a>|<a class="ae pi" href="https://coincodecap.com/gemini-vs-blockfi" rel="noopener ugc nofollow" target="_blank">Gemini vs BlockFi</a>cmd |<a class="ae pi" href="https://coincodecap.com/okex-futures-trading" rel="noopener ugc nofollow" target="_blank">OKEx期货交易</a></li><li id="60b3" class="lm ln ht kb b kc lv kg lw kk lx ko ly ks lz kw pj ls lt lu dt translated"><a class="ae pi" href="https://coincodecap.com/buy-crypto-with-credit-card" rel="noopener ugc nofollow" target="_blank">购买信用卡加密的10大最佳地点</a></li></ul></div></div>    
</body>
</html>