<html>
<head>
<title>Linear Regression Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归第一部分</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/linear-regression-bf5141ce9ac8?source=collection_archive---------1-----------------------#2018-06-09">https://medium.com/coinmonks/linear-regression-bf5141ce9ac8?source=collection_archive---------1-----------------------#2018-06-09</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><div class=""><h2 id="70ec" class="pw-subtitle-paragraph iq hs ht bd b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ek translated">线性回归是最简单的监督学习类型。</h2></div><p id="b43e" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">回归的目标是探索输入特征与目标值之间的关系，并为给定的未知数据提供连续的输出值。</p><figure class="kf kg kh ki fq kj fe ff paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="fe ff ke"><img src="../Images/73d97fadecdc8ccfeab8f1986fd6fa12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HJnsZhHTY0hVJH7qA1FiDg.png"/></div></div><figcaption class="kq kr fg fe ff ks kt bd b be z ek">This is a General data Flow diagram of a linear regression model</figcaption></figure><p id="4460" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">在线性回归中，我们用一个线性方程来研究输入和目标之间的关系。对于只有一个特征的简单线性回归模型，等式变为:</p><blockquote class="ku kv kw"><p id="b3e8" class="ji jj kx jk b jl jm iu jn jo jp ix jq ky js jt ju kz jw jx jy la ka kb kc kd hm dt translated">Y=W1*X+b</p></blockquote><ul class=""><li id="e802" class="lb lc ht jk b jl jm jo jp jr ld jv le jz lf kd lg lh li lj dt translated">y =预测值/目标值</li><li id="9e62" class="lb lc ht jk b jl lk jo ll jr lm jv ln jz lo kd lg lh li lj dt translated">x =输入</li><li id="8a18" class="lb lc ht jk b jl lk jo ll jr lm jv ln jz lo kd lg lh li lj dt translated">W1 =坡度/坡度/重量</li><li id="54f7" class="lb lc ht jk b jl lk jo ll jr lm jv ln jz lo kd lg lh li lj dt translated">b =偏差</li></ul><p id="be2d" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">该方程与直线方程相同(Y=MX+c)</p><p id="2f65" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated"><strong class="jk hu"> <em class="kx">问题来了这个W1和b是什么？</em>T3】</strong></p><p id="0869" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated"><strong class="jk hu"> <em class="kx"> — — &gt;现在让我们假设它们是调整直线以获得最佳拟合的参数。通过调整W1和b，我们得到的算法得到了最优的结果。</em> </strong></p><h2 id="a615" class="lp lq ht bd lr ls lt lu lv lw lx ly lz jr ma mb mc jv md me mf jz mg mh mi mj dt translated">多元回归:</h2><p id="6b25" class="pw-post-body-paragraph ji jj ht jk b jl mk iu jn jo ml ix jq jr mm jt ju jv mn jx jy jz mo kb kc kd hm dt translated">现在我们有了一组输入特征X={x1，x2，x3，…，xn}以及与之相关联的权重W={w1，w2，w3，…wn}。因此，等式变为:</p><pre class="kf kg kh ki fq mp mq mr ms aw mt dt"><span id="78f1" class="lp lq ht mq b fv mu mv l mw mx">Y=(x1*w1+x2*w2+x3*w3+....+xn*wn)</span></pre><p id="dcae" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">或者</p><figure class="kf kg kh ki fq kj fe ff paragraph-image"><div class="fe ff my"><img src="../Images/2ef9bec3eb1e2b3f4198e192c7ff29df.png" data-original-src="https://miro.medium.com/v2/resize:fit:272/1*_PGElzAvrnI0rpF8XXm0Pw.gif"/></div><figcaption class="kq kr fg fe ff ks kt bd b be z ek">Multiple regression Equation</figcaption></figure><p id="87f9" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">考虑到偏见</p><blockquote class="ku kv kw"><p id="b59b" class="ji jj kx jk b jl jm iu jn jo jp ix jq ky js jt ju kz jw jx jy la ka kb kc kd hm dt translated">Y=(x1*w1+x2*w2+x3*w3+…。+xn*wn)+b</p></blockquote><p id="ef00" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">或者</p><figure class="kf kg kh ki fq kj fe ff paragraph-image"><div class="fe ff mz"><img src="../Images/2a2d815d3df3481b156c4a8695aa6c1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/1*3t2biuuLxtmkDfWceg3LAA.gif"/></div><figcaption class="kq kr fg fe ff ks kt bd b be z ek">Multiple regression Equation with bias</figcaption></figure><p id="78c4" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated"><strong class="jk hu"> <em class="kx">现在让我们回到权重:</em> </strong></p><p id="b1b7" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">我们如何确定权重和偏差？</p><p id="1d15" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">= &gt;通过MSE(均方误差)测量权重，并对其进行调整以获得最佳线性线。</p><p id="f9f5" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">MSE =平均值((预测值-y的第I个值的实际值) )</p><figure class="kf kg kh ki fq kj fe ff paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="fe ff na"><img src="../Images/6ca8c428d45cb407c47d5441c7da12c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fWAwKQNwWPvIZFAzKkh93g.png"/></div></div><figcaption class="kq kr fg fe ff ks kt bd b be z ek"><strong class="bd nb">Image1</strong> from(<a class="ae nc" href="https://cdn-images-1.medium.com/max/1200/0*FjKhbw6Va8O8bCkF.png" rel="noopener">https://cdn-images-1.medium.com/max/1200/0*FjKhbw6Va8O8bCkF.png</a>)</figcaption></figure><p id="aee8" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">让我们从图1来理解这个概念——‘T12’红线是我们的线性回归线或者我们的<strong class="jk hu">预测值(y<em class="kx">’</em>)。</strong>和“<strong class="jk hu">蓝色</strong>点是我们给定的数据或<strong class="jk hu">实际值</strong>。从蓝点(实际值)到红线(预测值)的距离的平方平均值必须最小，以获得最佳拟合回归线。</p><p id="8dea" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">因此可以表示为</p><figure class="kf kg kh ki fq kj fe ff paragraph-image"><div class="fe ff nd"><img src="../Images/db39764ffa2b9ce5932913331e16af7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:254/1*rSVZvVdbdWUPyGgHhivJjA.gif"/></div><figcaption class="kq kr fg fe ff ks kt bd b be z ek">MSE where y’ is predicted value yi is actual value</figcaption></figure><p id="f30d" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">为了获得最佳结果，我们需要最小化MSE</p><p id="9ffa" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">因此，为了最小化该误差或MSE，我们使用梯度下降来在MSE或误差率计算之后找到权重。梯度下降可等同于:</p><figure class="kf kg kh ki fq kj fe ff paragraph-image"><div class="fe ff ne"><img src="../Images/2fe2be637b84efd33d40cb6054cbe631.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/1*nKQGKZPUs_AJXCrrclP1vw.gif"/></div><figcaption class="kq kr fg fe ff ks kt bd b be z ek">Gradient Descent</figcaption></figure><blockquote class="nf"><p id="8939" class="ng nh ht bd ni nj nk nl nm nn no kd ek translated">现在，在我们得到梯度下降后，我们需要每次更新权重，直到我们得到最佳拟合值</p></blockquote><blockquote class="ku kv kw"><p id="4d18" class="ji jj kx jk b jl np iu jn jo nq ix jq ky nr jt ju kz ns jx jy la nt kb kc kd hm dt translated">新权重=旧权重+(学习率*梯度下降)</p></blockquote><figure class="kf kg kh ki fq kj fe ff paragraph-image"><div class="fe ff nu"><img src="../Images/9cb187f22e2846c1f725fed5407cfa1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:304/1*goGDbqTEkI9SHnZbBxazcg.gif"/></div><figcaption class="kq kr fg fe ff ks kt bd b be z ek">new Weight=old Weight+(Learning Rate *Gradient Descent)</figcaption></figure><blockquote class="ku kv kw"><p id="c7c1" class="ji jj kx jk b jl jm iu jn jo jp ix jq ky js jt ju kz jw jx jy la ka kb kc kd hm dt translated">alpha或学习率是0-1之间的固定值，此时我们不需要知道太多。在下一篇文章中，我将解释多元回归是如何工作的。</p></blockquote><h1 id="cbda" class="nv lq ht bd lr nw nx ny lv nz oa ob lz iz oc ja mc jc od jd mf jf oe jg mi of dt translated">现在让我们看看编码部分:</h1><p id="865e" class="pw-post-body-paragraph ji jj ht jk b jl mk iu jn jo ml ix jq jr mm jt ju jv mn jx jy jz mo kb kc kd hm dt translated">我们需要什么</p><ul class=""><li id="db20" class="lb lc ht jk b jl jm jo jp jr ld jv le jz lf kd lg lh li lj dt translated">python 3.6+ <a class="ae nc" href="https://www.python.org/downloads/" rel="noopener ugc nofollow" target="_blank">下载Python </a></li><li id="1b9e" class="lb lc ht jk b jl lk jo ll jr lm jv ln jz lo kd lg lh li lj dt translated">熊猫图书馆(pip3安装熊猫)</li><li id="d90e" class="lb lc ht jk b jl lk jo ll jr lm jv ln jz lo kd lg lh li lj dt translated">matplotlib库(pip3安装matplotlib)</li><li id="259d" class="lb lc ht jk b jl lk jo ll jr lm jv ln jz lo kd lg lh li lj dt translated">scikitlearn库(pip3安装sklearn)</li><li id="4ae2" class="lb lc ht jk b jl lk jo ll jr lm jv ln jz lo kd lg lh li lj dt translated">CSV文件:<a class="ae nc" href="https://github.com/neelindresh/NeelBlog/blob/master/HousePrice.csv" rel="noopener ugc nofollow" target="_blank">https://github . com/neelindresh/Neel blog/blob/master/house price . CSV</a></li></ul><pre class="kf kg kh ki fq mp mq mr ms aw mt dt"><span id="a8ed" class="lp lq ht mq b fv mu mv l mw mx">Code: for simple regression</span><span id="7440" class="lp lq ht mq b fv og mv l mw mx">import pandas<br/>#load csv file<br/>df=pandas.read_csv('./DataSet/HousePrice.csv')<br/>df=df[['Price (Older)', 'Price (New)']]</span><span id="921c" class="lp lq ht mq b fv og mv l mw mx">#Define feature list (X) target(Y)<br/>X=df[['Price (Older)']]<br/>Y=df[['Price (New)']]</span><span id="591e" class="lp lq ht mq b fv og mv l mw mx">#load predefined linearRegression model<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.linear_model import LinearRegression<br/>xTrain,xTest,yTrain,yTest=train_test_split(X,Y)<br/>Lreg=LinearRegression().fit(xTrain,yTrain)</span><span id="0320" class="lp lq ht mq b fv og mv l mw mx">#   formula=(W1*x+b)<br/>print('Coef(W1):',Lreg.coef_)<br/>print('Intercept(W0/b):',Lreg.intercept_)<br/>W1=Lreg.coef_<br/>b=Lreg.intercept_</span><span id="ad80" class="lp lq ht mq b fv og mv l mw mx">#ploting the same<br/>import matplotlib.pyplot as plt<br/>plt.scatter(X,Y)<br/>plt.plot(X,W1*X+b,'r-')<br/>plt.show()<br/>#You can predict a value using<br/>#print(Lreg.predict(someValue))</span></pre><blockquote class="ku kv kw"><p id="09cb" class="ji jj kx jk b jl jm iu jn jo jp ix jq ky js jt ju kz jw jx jy la ka kb kc kd hm dt translated"><em class="ht">完整的代码和csv文件可以在:</em> <a class="ae nc" href="https://github.com/neelindresh/NeelBlog" rel="noopener ugc nofollow" target="_blank"> <em class="ht">从github </em> </a>下载</p></blockquote><figure class="kf kg kh ki fq kj fe ff paragraph-image"><div class="fe ff oh"><img src="../Images/9ce5bfeac64eced222e69ec18cbba651.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*sFs2I2fOaGf-LSQY-Xj8iw.png"/></div><figcaption class="kq kr fg fe ff ks kt bd b be z ek">Output</figcaption></figure><p id="e76f" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">输出图表</p><h1 id="b581" class="nv lq ht bd lr nw nx ny lv nz oa ob lz iz oc ja mc jc od jd mf jf oe jg mi of dt translated">描述:</h1><p id="7f94" class="pw-post-body-paragraph ji jj ht jk b jl mk iu jn jo ml ix jq jr mm jt ju jv mn jx jy jz mo kb kc kd hm dt translated">CSV文件有许多列。但是我只用了其中的两个来展示简单回归是如何工作的。“价格(旧)”与“价格(新)”，其中“旧”是x坐标，“新”是y坐标。</p><p id="28fc" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated"><strong class="jk hu"> <em class="kx">加载CSV文件</em> </strong></p><blockquote class="ku kv kw"><p id="c011" class="ji jj kx jk b jl jm iu jn jo jp ix jq ky js jt ju kz jw jx jy la ka kb kc kd hm dt translated"><em class="ht"> df=pandas.read_csv('。/DataSet/HousePrice.csv') </em></p></blockquote><p id="52d4" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated"><strong class="jk hu"> <em class="kx">从数据框中切分‘价格(旧)’‘价格(新)’列:</em> </strong></p><blockquote class="ku kv kw"><p id="aed5" class="ji jj kx jk b jl jm iu jn jo jp ix jq ky js jt ju kz jw jx jy la ka kb kc kd hm dt translated"><em class="ht"> df=df[['价格(旧)'，'价格(新)']] </em></p></blockquote><p id="eded" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated"><strong class="jk hu"> <em class="kx">定义特征列表(X)目标(Y) </em> </strong></p><blockquote class="ku kv kw"><p id="af2c" class="ji jj kx jk b jl jm iu jn jo jp ix jq ky js jt ju kz jw jx jy la ka kb kc kd hm dt translated"><em class="ht"> X=df[['价格(旧)']] <br/> Y=df[['价格(新)']] </em></p></blockquote><p id="6d51" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated"><strong class="jk hu"><em class="kx">trainttestsplit将数据集划分为75%训练25%测试数据</em> </strong></p><blockquote class="ku kv kw"><p id="7b25" class="ji jj kx jk b jl jm iu jn jo jp ix jq ky js jt ju kz jw jx jy la ka kb kc kd hm dt translated">xTrain，xTest，yTrain，yTest=train_test_split(X，Y)</p></blockquote><p id="e8b8" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated"><strong class="jk hu"> <em class="kx"> LinearRegression()。fit(X，Y)- &gt;将X值和Y值分别放入给定函数</em> </strong></p><blockquote class="ku kv kw"><p id="20d6" class="ji jj kx jk b jl jm iu jn jo jp ix jq ky js jt ju kz jw jx jy la ka kb kc kd hm dt translated">Lreg =线性回归()。fit(xTrain，yTrain)</p></blockquote><p id="9951" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated"><strong class="jk hu"> <em class="kx">给出最佳拟合的W1和b(最终权重和最终偏差)</em> </strong></p><blockquote class="ku kv kw"><p id="d52e" class="ji jj kx jk b jl jm iu jn jo jp ix jq ky js jt ju kz jw jx jy la ka kb kc kd hm dt translated">w1 = lreg . coef _<br/>b = lreg . intercept _</p></blockquote><p id="f6af" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated"><strong class="jk hu"> <em class="kx">使用matPlotLib </em> </strong>绘图</p><blockquote class="ku kv kw"><p id="d696" class="ji jj kx jk b jl jm iu jn jo jp ix jq ky js jt ju kz jw jx jy la ka kb kc kd hm dt translated">#绘制相同的<br/>导入matplotlib.pyplot作为plt <br/> plt.scatter(X，Y) <br/> plt.plot(X，W1*X+b，' r-') <br/> plt.show()</p></blockquote><p id="0f62" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated"><strong class="jk hu"> <em class="kx">你可以使用</em> </strong>来预测a值</p><blockquote class="ku kv kw"><p id="13c8" class="ji jj kx jk b jl jm iu jn jo jp ix jq ky js jt ju kz jw jx jy la ka kb kc kd hm dt translated">print(Lreg.predict(someValue))</p></blockquote></div><div class="ab cl oi oj hb ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="hm hn ho hp hq"><blockquote class="nf"><p id="5cf9" class="ng nh ht bd ni nj op oq or os ot kd ek translated">在youtube上关注我:</p></blockquote><div class="ou ov ow ox oy oz"><a href="https://www.youtube.com/channel/UCTJE1mGfe5qgO5OfWE6surg?view_as=subscriber" rel="noopener  ugc nofollow" target="_blank"><div class="pa ab ej"><div class="pb ab pc cl cj pd"><h2 class="bd hu fv z el pe eo ep pf er et hs dt translated">尼尔·巴塔查里亚</h2><div class="pg l"><h3 class="bd b fv z el pe eo ep pf er et ek translated">编程爱情</h3></div><div class="ph l"><p class="bd b gc z el pe eo ep pf er et ek translated">www.youtube.com</p></div></div><div class="pi l"><div class="pj l pk pl pm pi pn ko oz"/></div></div></a></div><p id="f344" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hm dt translated">关注我的博客:</p><div class="po pp fm fo pq oz"><a href="https://dataneel.wordpress.com/" rel="noopener  ugc nofollow" target="_blank"><div class="pa ab ej"><div class="pb ab pc cl cj pd"><h2 class="bd hu fv z el pe eo ep pf er et hs dt translated">面向所有人的数据科学</h2><div class="pg l"><h3 class="bd b fv z el pe eo ep pf er et ek translated">线性回归是最简单的监督学习类型。回归分析的目的是探索…</h3></div><div class="ph l"><p class="bd b gc z el pe eo ep pf er et ek translated">dataneel.wordpress.com</p></div></div><div class="pi l"><div class="pr l pk pl pm pi pn ko oz"/></div></div></a></div></div></div>    
</body>
</html>