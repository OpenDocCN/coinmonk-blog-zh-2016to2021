<html>
<head>
<title>Create a Neural Network in PyTorch — And Make Your Life Simpler</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在PyTorch中创建一个神经网络——让你的生活更简单</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/create-a-neural-network-in-pytorch-and-make-your-life-simpler-ec5367895199?source=collection_archive---------0-----------------------#2018-06-27">https://medium.com/coinmonks/create-a-neural-network-in-pytorch-and-make-your-life-simpler-ec5367895199?source=collection_archive---------0-----------------------#2018-06-27</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><figure class="fi fk ir is it iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff iq"><img src="../Images/4f350a37242b188202c5d8f4973774ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mU_kN2-DcMoXjokM"/></div></div><figcaption class="jb jc fg fe ff jd je bd b be z ek">“Black bike sits against dark brick wall” by <a class="ae jf" href="https://unsplash.com/@dsoodmand?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Darius Soodmand</a> on <a class="ae jf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><blockquote class="jg"><p id="af5b" class="jh ji ht bd jj jk jl jm jn jo jp jq ek translated">人生苦短，不能让神经网络变得复杂！</p></blockquote><p id="9f5b" class="pw-post-body-paragraph jr js ht jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn jq hm dt ko translated"><span class="l kp kq kr bm ks kt ku kv kw di"> E </span>亚瑟王！实话实说吧。有许多图书馆可供我们用来满足深度学习的渴望。我也尝试过一些。刚开始的时候，我尝试了tensorflow。然后我试了试Keras。然后我尝试了PyTorch。如果我对自己和所有的神——旧的<a class="ae jf" href="https://images.google.com/imgres?imgurl=http%3A%2F%2F3.bp.blogspot.com%2F-wKYqGJyJXxc%2FVajqaq3Kk6I%2FAAAAAAAABVs%2FmQC1DTvLzVI%2Fs920%2FBanner%252BAGOT%252B2.jpg&amp;imgrefurl=http%3A%2F%2Foldgodsandthenew.blogspot.com%2F2015%2F07%2Fblog-post.html&amp;docid=UFkdfMrASAvO7M&amp;tbnid=uE71uDhpRyczuM%3A&amp;vet=1&amp;w=920&amp;h=364&amp;source=sh%2Fx%2Fim" rel="noopener ugc nofollow" target="_blank"> <em class="kx">和新的</em></a>——完全诚实的话，我最喜欢PyTorch。</p><p id="9922" class="pw-post-body-paragraph jr js ht jt b ju ky jw jx jy kz ka kb kc la ke kf kg lb ki kj kk lc km kn jq hm dt translated">我想我喜欢PyTorch胜过Tensorflow的原因是PyTorch非常<em class="kx">python化</em>。它使用python的风格和功能，这使得理解和使用起来更加容易和简单。尤其对我来说，因为我主要用Python编码。Tensorflow的问题在于，它需要你学习很多Tensorflow特有的术语。但这不是一个关于PyTorch和Tensorflow之间15世纪战争的帖子。这篇文章是关于如何在PyTorch中创建一个简单的神经网络。</p><blockquote class="jg"><p id="c54c" class="jh ji ht bd jj jk ld le lf lg lh jq ek translated">我要教你如何生活。嗯……不完全是。但我会教你来这里的目的。</p></blockquote><p id="8e2a" class="pw-post-body-paragraph jr js ht jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn jq hm dt translated"><strong class="jt hu">更新:</strong> <em class="kx">你可以在这里获得包含</em><strong class="jt hu"><em class="kx"/></strong><a class="ae jf" href="https://github.com/J-Yash/Creating-a-simple-NN-in-pytorch" rel="noopener ugc nofollow" target="_blank"><strong class="jt hu"><em class="kx">的Jupyter笔记本完整代码</em> </strong> </a> <em class="kx">。</em></p><p id="73e0" class="pw-post-body-paragraph jr js ht jt b ju ky jw jx jy kz ka kb kc la ke kf kg lb ki kj kk lc km kn jq hm dt translated">我们将创建一个具有一个隐藏层和一个输出单元的简单神经网络。我们将在隐藏层使用<a class="ae jf" rel="noopener" href="/alchemicai/activation-functions-demystified-661d1183f5f8"> ReLU激活，在输出层使用</a>sigmoid激活。</p><p id="2149" class="pw-post-body-paragraph jr js ht jt b ju ky jw jx jy kz ka kb kc la ke kf kg lb ki kj kk lc km kn jq hm dt translated">首先，我们需要导入PyTorch库。</p><pre class="li lj lk ll fq lm ln lo lp aw lq dt"><span id="cf37" class="lr ls ht ln b fv lt lu l lv lw">import torch<br/>import torch.nn as nn</span></pre><p id="9199" class="pw-post-body-paragraph jr js ht jt b ju ky jw jx jy kz ka kb kc la ke kf kg lb ki kj kk lc km kn jq hm dt translated">然后，我们定义所有层的大小和批量大小</p><pre class="li lj lk ll fq lm ln lo lp aw lq dt"><span id="98f3" class="lr ls ht ln b fv lt lu l lv lw">n_in, n_h, n_out, batch_size = 10, 5, 1, 10</span></pre><p id="6c90" class="pw-post-body-paragraph jr js ht jt b ju ky jw jx jy kz ka kb kc la ke kf kg lb ki kj kk lc km kn jq hm dt translated">现在，我们创建一些虚拟输入数据<code class="eh lx ly lz ln b">x</code>和一些虚拟目标数据<code class="eh lx ly lz ln b">y</code>。我们使用PyTorch张量来存储这些数据。PyTorch张量可以像NumPy数组一样使用和操作，但是还有一个额外的好处，就是PyTorch张量可以在GPU上运行。但是在本教程中，我们将简单地在CPU上运行它们。虽然，把它们转移到GPU上是相当简单的。</p><pre class="li lj lk ll fq lm ln lo lp aw lq dt"><span id="279b" class="lr ls ht ln b fv lt lu l lv lw">x = torch.randn(batch_size, n_in)<br/>y = torch.tensor([[1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0]])</span></pre><p id="2343" class="pw-post-body-paragraph jr js ht jt b ju ky jw jx jy kz ka kb kc la ke kf kg lb ki kj kk lc km kn jq hm dt translated">现在，对于主菜，我们将在一行代码中定义我们的模型。</p><pre class="li lj lk ll fq lm ln lo lp aw lq dt"><span id="72ab" class="lr ls ht ln b fv lt lu l lv lw">model = nn.Sequential(nn.Linear(n_in, n_h),<br/>                     nn.ReLU(),<br/>                     nn.Linear(n_h, n_out),<br/>                     nn.Sigmoid())</span></pre><p id="8bba" class="pw-post-body-paragraph jr js ht jt b ju ky jw jx jy kz ka kb kc la ke kf kg lb ki kj kk lc km kn jq hm dt translated">这就创建了一个类似于input-&gt; linear-&gt; relu-&gt; linear-&gt; sigmoid的模型。还有另一种方法来定义我们的模型，它用于定义更复杂和定制的模型。这是通过在类中定义我们的模型来完成的。你可以在这里  了解一下<a class="ae jf" href="https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-custom-nn-modules" rel="noopener ugc nofollow" target="_blank"> <strong class="jt hu"> <em class="kx">。</em></strong></a></p><p id="b7a2" class="pw-post-body-paragraph jr js ht jt b ju ky jw jx jy kz ka kb kc la ke kf kg lb ki kj kk lc km kn jq hm dt translated">现在，是时候构建我们的损失函数了。我们将使用均方误差损失。</p><pre class="li lj lk ll fq lm ln lo lp aw lq dt"><span id="5d5e" class="lr ls ht ln b fv lt lu l lv lw">criterion = torch.nn.MSELoss()</span></pre><p id="306d" class="pw-post-body-paragraph jr js ht jt b ju ky jw jx jy kz ka kb kc la ke kf kg lb ki kj kk lc km kn jq hm dt translated">另外，不要忘记定义我们的优化器。我们在这个例子中使用随机梯度下降，学习率为0.01。返回模型参数(权重和偏差)的迭代器。</p><pre class="li lj lk ll fq lm ln lo lp aw lq dt"><span id="bb2a" class="lr ls ht ln b fv lt lu l lv lw">optimizer = torch.optim.SGD(model.parameters(), lr=0.01)</span></pre><p id="7730" class="pw-post-body-paragraph jr js ht jt b ju ky jw jx jy kz ka kb kc la ke kf kg lb ki kj kk lc km kn jq hm dt translated">现在，对于甜点，我们运行我们的梯度下降50个时代。这按顺序进行正向传播、损耗计算、反向传播和参数更新。</p><pre class="li lj lk ll fq lm ln lo lp aw lq dt"><span id="1c12" class="lr ls ht ln b fv lt lu l lv lw">for epoch in range(50):<br/>    # Forward Propagation<br/>    y_pred = model(x)</span><span id="6ead" class="lr ls ht ln b fv ma lu l lv lw">    # Compute and print loss<br/>    loss = criterion(y_pred, y)<br/>    print('epoch: ', epoch,' loss: ', loss.item())</span><span id="665b" class="lr ls ht ln b fv ma lu l lv lw">    # Zero the gradients<br/>    optimizer.zero_grad()<br/>    <br/>    # perform a backward pass (backpropagation)<br/>    loss.backward()<br/>    <br/>    # Update the parameters<br/>    optimizer.step()</span></pre><p id="2387" class="pw-post-body-paragraph jr js ht jt b ju ky jw jx jy kz ka kb kc la ke kf kg lb ki kj kk lc km kn jq hm dt translated"><code class="eh lx ly lz ln b">y_pred</code>从我们模型的正向传递中获取预测值。我们将它和目标值<code class="eh lx ly lz ln b">y</code>一起传递给<code class="eh lx ly lz ln b">criterion</code>，由其计算损失。然后，<code class="eh lx ly lz ln b">optimizer.zero_grad()</code>将所有的渐变归零。我们需要这样做，以便以前的梯度不会继续积累。然后，<code class="eh lx ly lz ln b">loss.backward()</code>是使用PyTorch亲笔签名功能的主要PyTorch魔法。Autograd根据它动态创建的计算图自动计算所有梯度和所有参数。基本上，这是梯度下降的反向传递(反向传播)。最后，我们调用<code class="eh lx ly lz ln b">optimizer.step()</code>，它使用新的梯度对所有参数进行一次更新。</p><p id="8cf0" class="pw-post-body-paragraph jr js ht jt b ju ky jw jx jy kz ka kb kc la ke kf kg lb ki kj kk lc km kn jq hm dt translated">仅此而已。我们已经在PyTorch中成功地训练了一个简单的两层神经网络，而且我们真的不需要通过大量的随机术语来完成它。PyTorch保持它的甜蜜和简单，就像每个人都喜欢的那样。</p></div><div class="ab cl mb mc hb md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="hm hn ho hp hq"><p id="78a0" class="pw-post-body-paragraph jr js ht jt b ju ky jw jx jy kz ka kb kc la ke kf kg lb ki kj kk lc km kn jq hm dt translated">如果您想了解更多关于PyTorch的知识，并想更深入地了解它，可以看看PyTorch的官方文档和教程。他们确实写得很好。你可以在这里找到它们<a class="ae jf" href="https://pytorch.org/tutorials/" rel="noopener ugc nofollow" target="_blank"><strong class="jt hu"><em class="kx"/></strong></a>。</p><p id="9dc2" class="pw-post-body-paragraph jr js ht jt b ju ky jw jx jy kz ka kb kc la ke kf kg lb ki kj kk lc km kn jq hm dt translated">我希望您觉得这篇文章很有用，并且比以前对PyTorch有了更好的了解。</p><blockquote class="jg"><p id="f58d" class="jh ji ht bd jj jk ld le lf lg lh jq ek translated">“害怕它。逃离它。错误仍然存在—灭霸”</p></blockquote><p id="14dd" class="pw-post-body-paragraph jr js ht jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn jq hm dt translated"><em class="kx">如果你想了解更多关于深度学习的知识，那么</em> <strong class="jt hu"> <em class="kx">跟我来</em> </strong> <em class="kx">。我会在不断学习的同时，不断发布有用的深度学习教程和文章。</em></p><p id="0aea" class="pw-post-body-paragraph jr js ht jt b ju ky jw jx jy kz ka kb kc la ke kf kg lb ki kj kk lc km kn jq hm dt translated"><strong class="jt hu"> <em class="kx">有任何问题、反馈或批评吗？在</em> </strong> <a class="ae jf" href="http://twitter.com/iamjyash" rel="noopener ugc nofollow" target="_blank"> <strong class="jt hu"> <em class="kx">上留言评论或者联系我</em> </strong> </a> <strong class="jt hu"> <em class="kx">。</em>T13】</strong></p><blockquote class="jg"><p id="3d5d" class="jh ji ht bd jj jk ld le lf lg lh jq ek translated">加入Coinmonks <a class="ae jf" href="https://t.me/coincodecap" rel="noopener ugc nofollow" target="_blank">电报频道</a>和<a class="ae jf" href="https://www.youtube.com/c/coinmonks/videos" rel="noopener ugc nofollow" target="_blank"> Youtube频道</a>获取每日<a class="ae jf" href="http://coincodecap.com/" rel="noopener ugc nofollow" target="_blank">加密新闻</a></p></blockquote><h2 id="15d6" class="lr ls ht bd mi mj mk ml mm mn mo mp mq kc mr ms mt kg mu mv mw kk mx my mz na dt translated">另外，阅读</h2><ul class=""><li id="20fb" class="nb nc ht jt b ju nd jy ne kc nf kg ng kk nh jq ni nj nk nl dt translated"><a class="ae jf" rel="noopener" href="/coinmonks/top-10-crypto-copy-trading-platforms-for-beginners-d0c37c7d698c">复制交易</a> | <a class="ae jf" rel="noopener" href="/coinmonks/crypto-tax-software-ed4b4810e338">加密税务软件</a></li><li id="723e" class="nb nc ht jt b ju nm jy nn kc no kg np kk nq jq ni nj nk nl dt translated"><a class="ae jf" href="https://coincodecap.com/grid-trading" rel="noopener ugc nofollow" target="_blank">网格交易</a> | <a class="ae jf" rel="noopener" href="/coinmonks/the-best-cryptocurrency-hardware-wallets-of-2020-e28b1c124069">加密硬件钱包</a></li><li id="874f" class="nb nc ht jt b ju nm jy nn kc no kg np kk nq jq ni nj nk nl dt translated"><a class="ae jf" href="http://Top 4 Telegram Channels for Crypto Traders" rel="noopener ugc nofollow" target="_blank">密码电报信号</a> | <a class="ae jf" rel="noopener" href="/coinmonks/crypto-trading-bot-c2ffce8acb2a">密码交易机器人</a></li><li id="4139" class="nb nc ht jt b ju nm jy nn kc no kg np kk nq jq ni nj nk nl dt translated"><a class="ae jf" href="https://coincodecap.com/trading-signal" rel="noopener ugc nofollow" target="_blank">有哪些交易信号？</a> | <a class="ae jf" href="https://coincodecap.com/bitstamp-coinbase" rel="noopener ugc nofollow" target="_blank">比特斯坦普vs比特币基地</a></li><li id="20ec" class="nb nc ht jt b ju nm jy nn kc no kg np kk nq jq ni nj nk nl dt translated"><a class="ae jf" href="https://coincodecap.com/profitfarmers-review" rel="noopener ugc nofollow" target="_blank"> ProfitFarmers回顾</a> | <a class="ae jf" href="https://coincodecap.com/cornix-trading-bot" rel="noopener ugc nofollow" target="_blank">如何使用Cornix Trading Bot </a></li><li id="9801" class="nb nc ht jt b ju nm jy nn kc no kg np kk nq jq ni nj nk nl dt translated"><a class="ae jf" href="https://coincodecap.com/buy-domain-on-unstoppable-domains" rel="noopener ugc nofollow" target="_blank">如何在势不可挡的域名上购买域名？</a></li><li id="8e13" class="nb nc ht jt b ju nm jy nn kc no kg np kk nq jq ni nj nk nl dt translated"><a class="ae jf" href="https://coincodecap.com/crypto-tax-india" rel="noopener ugc nofollow" target="_blank">印度的秘密税</a> | <a class="ae jf" href="https://coincodecap.com/altfins-review" rel="noopener ugc nofollow" target="_blank"> altFINS审查</a> | <a class="ae jf" rel="noopener" href="/coinmonks/prokey-review-26611173c13c"> Prokey审查</a></li><li id="f33b" class="nb nc ht jt b ju nm jy nn kc no kg np kk nq jq ni nj nk nl dt translated"><a class="ae jf" rel="noopener" href="/coinmonks/crypto-exchange-dd2f9d6f3769">最佳加密交易所</a> | <a class="ae jf" rel="noopener" href="/coinmonks/bitcoin-exchange-in-india-7f1fe79715c9">印度最佳加密交易所</a></li><li id="47a8" class="nb nc ht jt b ju nm jy nn kc no kg np kk nq jq ni nj nk nl dt translated"><a class="ae jf" rel="noopener" href="/coinmonks/best-crypto-apis-for-developers-5efe3a597a9f">面向开发者的最佳加密API</a></li><li id="b359" class="nb nc ht jt b ju nm jy nn kc no kg np kk nq jq ni nj nk nl dt translated">最佳<a class="ae jf" rel="noopener" href="/coinmonks/top-5-crypto-lending-platforms-in-2020-that-you-need-to-know-a1b675cec3fa">密码借贷平台</a></li><li id="9487" class="nb nc ht jt b ju nm jy nn kc no kg np kk nq jq ni nj nk nl dt translated"><a class="ae jf" rel="noopener" href="/coinmonks/leveraged-token-3f5257808b22">杠杆代币终极指南</a></li></ul></div></div>    
</body>
</html>