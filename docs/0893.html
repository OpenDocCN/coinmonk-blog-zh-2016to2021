<html>
<head>
<title>What is Entropy and why Information gain matter in Decision Trees?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是熵，为什么信息增益在决策树中很重要？</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/what-is-entropy-and-why-information-gain-is-matter-4e85d46d2f01?source=collection_archive---------0-----------------------#2018-06-29">https://medium.com/coinmonks/what-is-entropy-and-why-information-gain-is-matter-4e85d46d2f01?source=collection_archive---------0-----------------------#2018-06-29</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><figure class="fi fk ir is it iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff iq"><img src="../Images/6a8c994f1a13dece1fece1d7478d92ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EFioClRp6FqXF_F1gznq4w.png"/></div></div></figure><blockquote class="jb jc jd"><p id="ca50" class="je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">根据<a class="ae kd" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" rel="noopener ugc nofollow" target="_blank">维基百科</a>，<strong class="jh hu">熵</strong>指的是无序或不确定性。</p><p id="d48f" class="je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated"><strong class="jh hu">定义</strong> : <strong class="jh hu">熵</strong>是一堆例子中<strong class="jh hu">杂质</strong>、<strong class="jh hu">无序</strong>或<strong class="jh hu">不确定性</strong>的度量。</p></blockquote><h2 id="4400" class="ke kf ht bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb dt translated">熵的基本功能是什么？</h2><p id="cc6c" class="pw-post-body-paragraph je jf ht jh b ji lc jk jl jm ld jo jp kp le js jt kt lf jw jx kx lg ka kb kc hm dt translated"><em class="jg">熵控制着决策树如何决定</em> <strong class="jh hu"> <em class="jg">拆分</em> </strong> <em class="jg">数据。它实际上影响了一个</em> <strong class="jh hu"> <em class="jg">决策树</em> </strong> <em class="jg">如何绘制其边界。</em></p><p id="c502" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated"><strong class="jh hu"> <em class="jg">熵的方程式:</em> </strong></p><figure class="li lj lk ll fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff lh"><img src="../Images/60dd83023039bb3a1d144ba0bbaa8c80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S6zcbdAzUvIOKBaWBKp9MA.png"/></div></div><figcaption class="lm ln fg fe ff lo lp bd b be z ek">Equation of Entropy</figcaption></figure></div><div class="ab cl lq lr hb ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hm hn ho hp hq"><h1 id="cc60" class="lx kf ht bd kg ly lz ma kk mb mc md ko me mf mg ks mh mi mj kw mk ml mm la mn dt translated">什么是信息增益，为什么它在决策树中很重要？</h1><blockquote class="jb jc jd"><p id="463a" class="je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated"><strong class="jh hu">定义:</strong> <strong class="jh hu">信息增益(IG) </strong>衡量一个特征给我们多少关于类的“信息”。</p></blockquote><h2 id="d3d7" class="ke kf ht bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb dt translated"><strong class="ak"> <em class="mo">为什么要紧？</em>T47】</strong></h2><ul class=""><li id="8cd6" class="mp mq ht jh b ji lc jm ld kp mr kt ms kx mt kc mu mv mw mx dt translated"><strong class="jh hu">信息增益</strong>是<strong class="jh hu">决策树算法</strong>用来构建决策树的主键。</li><li id="d4ca" class="mp mq ht jh b ji my jm mz kp na kt nb kx nc kc mu mv mw mx dt translated"><strong class="jh hu">决策树</strong>算法将总是试图最大化<strong class="jh hu">信息增益</strong>。</li><li id="c013" class="mp mq ht jh b ji my jm mz kp na kt nb kx nc kc mu mv mw mx dt translated">具有最高<strong class="jh hu">信息增益</strong>的<strong class="jh hu">属性</strong>将首先被测试/分割。</li></ul><p id="f8a2" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated"><strong class="jh hu"> <em class="jg">信息增益的方程式:</em> </strong></p><figure class="li lj lk ll fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff iq"><img src="../Images/60b0228c33f43539b1578103afc93e52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bVGWGETTor7bSnhr7sXEVw.png"/></div></div><figcaption class="lm ln fg fe ff lo lp bd b be z ek">Equation of Information gain</figcaption></figure></div><div class="ab cl lq lr hb ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hm hn ho hp hq"><h2 id="36f7" class="ke kf ht bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb dt translated">为了理解熵和信息增益，让我们用一些特征和标签画一个简单的表格。</h2><figure class="li lj lk ll fq iu"><div class="bz el l di"><div class="nd ne l"/></div><figcaption class="lm ln fg fe ff lo lp bd b be z ek">This example taken from <code class="eh nf ng nh ni b">Udacity </code>(Introduction to Machine Learning) course</figcaption></figure><p id="5631" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">在这个<code class="eh nf ng nh ni b">table</code>里，</p><ul class=""><li id="11ac" class="mp mq ht jh b ji jj jm jn kp nj kt nk kx nl kc mu mv mw mx dt translated"><code class="eh nf ng nh ni b">Grade</code>、<code class="eh nf ng nh ni b">Bumpiness </code>和<code class="eh nf ng nh ni b">Speed Limit</code>是特征，<code class="eh nf ng nh ni b"><strong class="jh hu">Speed </strong></code>是标签。</li><li id="cccc" class="mp mq ht jh b ji my jm mz kp na kt nb kx nc kc mu mv mw mx dt translated">总共四次观察。</li></ul></div><div class="ab cl lq lr hb ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hm hn ho hp hq"><p id="ed72" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated"><strong class="jh hu"> <em class="jg">首先，让我们一起来看看</em> </strong> <code class="eh nf ng nh ni b"><strong class="jh hu">Grade </strong></code> <strong class="jh hu"> <em class="jg">的功能</em> </strong></p><p id="003b" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">在<code class="eh nf ng nh ni b">Grade</code>列中有四个值，对应于这些值有四个标签。</p><p id="c738" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">让我们将所有标签视为父标签<code class="eh nf ng nh ni b">node</code>。</p><pre class="li lj lk ll fq nm ni nn no aw np dt"><span id="33a1" class="ke kf ht ni b fv nq nr l ns nt">SSFF =&gt; parent node</span></pre><blockquote class="jb jc jd"><p id="d678" class="je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated"><strong class="jh hu">那么，这个父节点的熵是多少呢？</strong></p></blockquote><p id="8214" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">让我们找出答案，</p><p id="736f" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">首先，我们需要找出出现在父节点中的示例的<em class="jg">部分。父节点中存在两种类型<em class="jg">(慢速和快速)</em>的实例，父节点总共包含4个实例。</em></p><pre class="li lj lk ll fq nm ni nn no aw np dt"><span id="cb38" class="ke kf ht ni b fv nq nr l ns nt">1. P(slow) =&gt; fraction of slow examples in parent node<br/>2. P(fast) =&gt; fraction of fast examples in parent node</span></pre><p id="422d" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">让我们来看看<code class="eh nf ng nh ni b">P(slow)</code>，</p><blockquote class="jb jc jd"><p id="16b0" class="je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">p(慢速)=父节点中慢速示例的数量/示例总数</p></blockquote><figure class="li lj lk ll fq iu fe ff paragraph-image"><div class="fe ff nu"><img src="../Images/6a49a9c7633624a3f3d65b6704ba160f.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/1*0GTDce7-b91KRBX3Y4vRZw.gif"/></div><figcaption class="lm ln fg fe ff lo lp bd b be z ek">fraction of P(slow) examples</figcaption></figure><p id="c7bb" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">类似地，快速示例<code class="eh nf ng nh ni b">P(fast)</code>的分数将是，</p><figure class="li lj lk ll fq iu fe ff paragraph-image"><div class="fe ff nv"><img src="../Images/7d27587548e0f46439acbbe8cae502d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/1*3MFS_JRVnUCmcjQ5WDEZMQ.gif"/></div><figcaption class="lm ln fg fe ff lo lp bd b be z ek">fraction of P(fast) examples</figcaption></figure><p id="0f20" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">所以，父节点的<strong class="jh hu">熵</strong>:</p><figure class="li lj lk ll fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff nw"><img src="../Images/012f9db9d4a86667f5d987d1179920e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*TwnG1LE42PCEgeFatN_c9w.gif"/></div></div><figcaption class="lm ln fg fe ff lo lp bd b be z ek">entropy of parent node</figcaption></figure><pre class="li lj lk ll fq nm ni nn no aw np dt"><span id="59cf" class="ke kf ht ni b fv nq nr l ns nt">Entropy(parent) = - {0.5 log2(0.5) + 0.5 log2(0.5)}<br/>                = - {-0.5 + (-0.5)}<br/>                = 1  </span></pre><p id="45bd" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">所以父节点的<em class="jg">熵</em>就是<code class="eh nf ng nh ni b">1</code>。</p></div><div class="ab cl lq lr hb ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hm hn ho hp hq"><blockquote class="jb jc jd"><p id="e129" class="je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">现在，让我们探索一下<strong class="jh hu">决策树算法</strong>如何基于<strong class="jh hu">信息增益</strong>构建<strong class="jh hu">决策树</strong></p></blockquote><p id="799b" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">首先让我们检查父节点是否被<code class="eh nf ng nh ni b">Grade</code>分割。</p><p id="b391" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">如果来自<code class="eh nf ng nh ni b">Grade</code>特征的<strong class="jh hu"> <em class="jg">信息增益</em> </strong>大于所有其他特征，则父节点可以被<code class="eh nf ng nh ni b">Grade</code>分割。</p><p id="306c" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">为了找出  <code class="eh nf ng nh ni b">Grade</code>特征的<strong class="jh hu"> <em class="jg">信息增益，我们需要用<code class="eh nf ng nh ni b">Grade</code>特征虚拟分裂父节点。</em></strong></p><figure class="li lj lk ll fq iu fe ff paragraph-image"><div class="fe ff nx"><img src="../Images/b0c65b4dcd6a5d562b264377a68a6e8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*kJEMr-vxkWKI7Mi3Bzz8Jg.png"/></div><figcaption class="lm ln fg fe ff lo lp bd b be z ek">Virtually split by Grade</figcaption></figure><p id="ad77" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">现在，我们需要找出这两个子节点的熵。</p><p id="4c89" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">右侧子节点<code class="eh nf ng nh ni b">(F)</code>的<strong class="jh hu"> <em class="jg">熵</em> </strong>为<code class="eh nf ng nh ni b">0</code>，<em class="jg">是因为该节点中的所有实例都属于同一个类</em>。</p><p id="1f55" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">让我们找出左侧节点<code class="eh nf ng nh ni b">SSF</code>的<strong class="jh hu"> <em class="jg">熵</em> </strong>:</p><p id="80dc" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">在该节点<code class="eh nf ng nh ni b">SSF</code>中存在两种类型的示例，因此我们需要为该节点分别找出慢速示例<strong class="jh hu"> <em class="jg">和快速示例</em> </strong>的分数。</p><pre class="li lj lk ll fq nm ni nn no aw np dt"><span id="e847" class="ke kf ht ni b fv nq nr l ns nt">P(slow) = 2/3 = 0.667<br/>P(fast) = 1/3 = 0.334</span></pre><p id="5a75" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">所以，</p><pre class="li lj lk ll fq nm ni nn no aw np dt"><span id="58c8" class="ke kf ht ni b fv nq nr l ns nt">Entropy(SSF)<strong class="ni hu"> = </strong>- {0.667 log2(0.667) + 0.334 log2(0.334)}<br/>             = - {-0.38 + (-0.52)}<br/>             = 0.9</span></pre><p id="3105" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">我们也可以通过使用<code class="eh nf ng nh ni b"><a class="ae kd" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html" rel="noopener ugc nofollow" target="_blank">scipy</a> </code>库找出<em class="jg">熵</em>。</p><figure class="li lj lk ll fq iu"><div class="bz el l di"><div class="nd ne l"/></div></figure><p id="b5d2" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">现在我们需要用加权平均找出<code class="eh nf ng nh ni b">Entropy(children)</code>。</p><pre class="li lj lk ll fq nm ni nn no aw np dt"><span id="1605" class="ke kf ht ni b fv nq nr l ns nt">Total number of examples in parent node: 4<br/>  "      "    "     "     "   left child node: 3<br/>  "      "    "     "     "   right child node: 1</span></pre><p id="12ee" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated"><em class="jg">熵的公式(子带加权平均值)。:</em></p><pre class="li lj lk ll fq nm ni nn no aw np dt"><span id="79e0" class="ke kf ht ni b fv nq nr l ns nt"><strong class="ni hu">[Weighted avg]Entropy(children)</strong> = <br/>(no. of examples in left child node) / (total no. of examples in parent node) * (entropy of left node) <br/>+ <br/>(no. of examples in right child node)/ (total no. of examples in parent node) * (entropy of right node)</span></pre><figure class="li lj lk ll fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff ny"><img src="../Images/bd74d0e3d3f8c54de59c088db22604e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/1*LRK32qYjJDwSbNCMA5UpcA.gif"/></div></div></figure><p id="78e8" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">带加权平均值的熵(子代)。is = <strong class="jh hu"> 0.675 </strong></p><p id="5f37" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">所以，</p><figure class="li lj lk ll fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff iq"><img src="../Images/60b0228c33f43539b1578103afc93e52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bVGWGETTor7bSnhr7sXEVw.png"/></div></div><figcaption class="lm ln fg fe ff lo lp bd b be z ek">Equation of Information gain</figcaption></figure><pre class="li lj lk ll fq nm ni nn no aw np dt"><span id="bc30" class="ke kf ht ni b fv nq nr l ns nt">Information gain(Grade) = 1 - 0.675<br/>                        = 0.325</span></pre><p id="1ea3" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated"><strong class="jh hu"> <em class="jg">信息增益</em> </strong>来自<code class="eh nf ng nh ni b">Grade</code>特征<code class="eh nf ng nh ni b">0.325</code>。</p><p id="5261" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated"><strong class="jh hu">决策树算法</strong>选择信息增益最高的<em class="jg">来分裂/构建</em>一棵<strong class="jh hu">决策树。</strong>所以我们需要检查所有的特征以便分割树。</p><blockquote class="jb jc jd"><p id="cfa1" class="je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">来自<code class="eh nf ng nh ni b">Bumpiness</code>的信息增益</p></blockquote><figure class="li lj lk ll fq iu fe ff paragraph-image"><div class="fe ff nx"><img src="../Images/bf7d913e860421d1912331e734cb7218.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*G4EDUnPTH_ovGl8jLfhjmQ.png"/></div><figcaption class="lm ln fg fe ff lo lp bd b be z ek">virtually split by Bumpyness</figcaption></figure><p id="2e1f" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">左右子节点的<strong class="jh hu"> <em class="jg">熵</em> </strong>相同，因为它们包含相同的类。</p><p id="e619" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated"><strong class="jh hu"> <em class="jg">熵(颠簸)</em> </strong>和<strong class="jh hu"> <em class="jg">熵(平滑)</em> </strong>都等于<code class="eh nf ng nh ni b">1</code>。</p><figure class="li lj lk ll fq iu"><div class="bz el l di"><div class="nd ne l"/></div></figure><p id="cf32" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">所以，<strong class="jh hu"> <em class="jg">【熵(子)】</em> </strong>用加权平均。对于<code class="eh nf ng nh ni b">Bumpiness</code>:</p><pre class="li lj lk ll fq nm ni nn no aw np dt"><span id="3043" class="ke kf ht ni b fv nq nr l ns nt">[weighted avg.]entropy(children) = 2/4 * 1 + 2/4 * 1<br/>                                 = 1</span></pre><p id="60d1" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">因此，</p><pre class="li lj lk ll fq nm ni nn no aw np dt"><span id="d5c4" class="ke kf ht ni b fv nq nr l ns nt">Information gain(Bumpiness) = 1 - 1<br/>                            = 0</span></pre><p id="7cca" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">到现在我们还得到了<strong class="jh hu"> <em class="jg">的信息增益</em> </strong>:</p><pre class="li lj lk ll fq nm ni nn no aw np dt"><span id="f778" class="ke kf ht ni b fv nq nr l ns nt">IG(Grade) =&gt; 0.325<br/>IG(Bumpiness) =&gt; 0</span></pre></div><div class="ab cl lq lr hb ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hm hn ho hp hq"><blockquote class="jb jc jd"><p id="8a7c" class="je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">从<code class="eh nf ng nh ni b"><em class="ht">SpeedLimit</em></code>获得的信息</p></blockquote><figure class="li lj lk ll fq iu fe ff paragraph-image"><div class="fe ff nx"><img src="../Images/db465e0ea3c05b3d4398b2e75a18f5bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*lJm7Z6iqmxh86H-uTIDfZA.png"/></div><figcaption class="lm ln fg fe ff lo lp bd b be z ek">virtually split by SpeedLimit</figcaption></figure><p id="b938" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">左侧子节点的<strong class="jh hu"> <em class="jg">熵</em> </strong>将为<code class="eh nf ng nh ni b">0</code>、<em class="jg">，因为该节点中的所有实例属于同一个类。</em></p><p id="4d8d" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">同理，右侧节点的<strong class="jh hu"> <em class="jg">熵</em> </strong>为<code class="eh nf ng nh ni b">0</code>。</p><figure class="li lj lk ll fq iu"><div class="bz el l di"><div class="nd ne l"/></div></figure><p id="0335" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">于是，<strong class="jh hu"> <em class="jg">【熵子】</em> </strong>用加权平均。对于<code class="eh nf ng nh ni b">SpeedLimit</code>:</p><pre class="li lj lk ll fq nm ni nn no aw np dt"><span id="1c24" class="ke kf ht ni b fv nq nr l ns nt">[weighted avg.] entropy(children) = 2/4 *0 + 2/4 *0<br/>                                  = 0</span></pre><p id="0378" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">于是，<strong class="jh hu"> <em class="jg">信息从<code class="eh nf ng nh ni b">SpeedLimit</code>获得</em> </strong>:</p><pre class="li lj lk ll fq nm ni nn no aw np dt"><span id="a36a" class="ke kf ht ni b fv nq nr l ns nt">Information gain(SpeedLimit) = 1 - 0<br/>                             = 1</span></pre><h2 id="6ce5" class="ke kf ht bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb dt translated">从所有特性中获得的最终信息:</h2><pre class="li lj lk ll fq nm ni nn no aw np dt"><span id="8efb" class="ke kf ht ni b fv nq nr l ns nt">IG(Grade) =&gt; 0.325<br/>IG(Bumpiness) =&gt; 0<br/>IG(SpeedLimit) =&gt; 1</span></pre><blockquote class="jb jc jd"><p id="fb30" class="je jf jg jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">我们知道，<strong class="jh hu">决策树算法</strong>基于具有最高<strong class="jh hu">信息增益</strong>的特征构建<strong class="jh hu">决策树</strong></p></blockquote><p id="5874" class="pw-post-body-paragraph je jf ht jh b ji jj jk jl jm jn jo jp kp jr js jt kt jv jw jx kx jz ka kb kc hm dt translated">所以，这里我们可以看到<code class="eh nf ng nh ni b">SpeedLimit</code>拥有最高的<strong class="jh hu"> <em class="jg">信息增益</em> </strong> <em class="jg">。</em>因此，该<a class="ae kd" href="https://gist.github.com/78526Nasir/111e6405b7ac0d34823839df42e2fc67" rel="noopener ugc nofollow" target="_blank">数据集</a>的最终<strong class="jh hu">决策树</strong>将如下所示:</p><figure class="li lj lk ll fq iu fe ff paragraph-image"><div class="fe ff nx"><img src="../Images/210987522b26030fed58667788af6c40.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*a-lr_OhUpiIpIwhA7YoWAA.png"/></div><figcaption class="lm ln fg fe ff lo lp bd b be z ek">Final Decision Tree</figcaption></figure><blockquote class="nz"><p id="096d" class="oa ob ht bd oc od oe of og oh oi kc ek translated">加入Coinmonks <a class="ae kd" href="https://t.me/coincodecap" rel="noopener ugc nofollow" target="_blank">电报频道</a>和<a class="ae kd" href="https://www.youtube.com/c/coinmonks/videos" rel="noopener ugc nofollow" target="_blank"> Youtube频道</a>获取每日<a class="ae kd" href="http://coincodecap.com/" rel="noopener ugc nofollow" target="_blank">加密新闻</a></p></blockquote><h2 id="15d6" class="ke kf ht bd kg kh oj kj kk kl ok kn ko kp ol kr ks kt om kv kw kx on kz la lb dt translated">另外，阅读</h2><ul class=""><li id="20fb" class="mp mq ht jh b ji lc jm ld kp mr kt ms kx mt kc mu mv mw mx dt translated"><a class="ae kd" href="http://Top 4 Telegram Channels for Crypto Traders" rel="noopener ugc nofollow" target="_blank">密码电报信号</a> | <a class="ae kd" rel="noopener" href="/coinmonks/crypto-trading-bot-c2ffce8acb2a">密码交易机器人</a></li><li id="14e6" class="mp mq ht jh b ji my jm mz kp na kt nb kx nc kc mu mv mw mx dt translated"><a class="ae kd" rel="noopener" href="/coinmonks/top-10-crypto-copy-trading-platforms-for-beginners-d0c37c7d698c">复制交易</a> | <a class="ae kd" rel="noopener" href="/coinmonks/crypto-tax-software-ed4b4810e338">加密税务软件</a></li><li id="723e" class="mp mq ht jh b ji my jm mz kp na kt nb kx nc kc mu mv mw mx dt translated"><a class="ae kd" href="https://coincodecap.com/grid-trading" rel="noopener ugc nofollow" target="_blank">网格交易</a> | <a class="ae kd" rel="noopener" href="/coinmonks/the-best-cryptocurrency-hardware-wallets-of-2020-e28b1c124069">加密硬件钱包</a></li><li id="f33b" class="mp mq ht jh b ji my jm mz kp na kt nb kx nc kc mu mv mw mx dt translated"><a class="ae kd" rel="noopener" href="/coinmonks/crypto-exchange-dd2f9d6f3769">加密交换</a> | <a class="ae kd" rel="noopener" href="/coinmonks/buy-bitcoin-in-india-feb50ddfef94">印度的加密应用</a></li></ul></div></div>    
</body>
</html>