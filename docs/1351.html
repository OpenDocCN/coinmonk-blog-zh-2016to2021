<html>
<head>
<title>Review: ZFNet — Winner of ILSVRC 2013 (Image Classification)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾:ZFNet——2013年国际影像分类奖得主</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/paper-review-of-zfnet-the-winner-of-ilsvlc-2013-image-classification-d1a5a0c45103?source=collection_archive---------0-----------------------#2018-08-19">https://medium.com/coinmonks/paper-review-of-zfnet-the-winner-of-ilsvlc-2013-image-classification-d1a5a0c45103?source=collection_archive---------0-----------------------#2018-08-19</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><p id="74d8" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">在这个故事中，ZFNet [1]被回顾。ZFNet是ILSVRC ( </strong> <a class="ae jo" href="http://www.image-net.org/challenges/LSVRC/" rel="noopener ugc nofollow" target="_blank"> <strong class="is hu"> ImageNet大规模视觉识别竞赛</strong> </a> <strong class="is hu"> ) 2013的一种获胜者，这是一个图像分类竞赛，与ILSVRC 2012的获胜者AlexNet [2]相比有显著的改进。</strong></p><figure class="jq jr js jt fq ju fe ff paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="fe ff jp"><img src="../Images/1202d4b18ff4440bbff3dcb6b24978b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Z6Pt1EPwN_h_2SwiWKVow.png"/></div></div><figcaption class="kb kc fg fe ff kd ke bd b be z ek"><strong class="bd kf">ILSVRC Ranking</strong></figcaption></figure><p id="8cd9" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">有些人/文章认为ZFNet不是冠军，这个结论可能来自于ILSVRC的排名，如上所示。然而，Clarifai是由ZFNet的作者泽勒创立的公司。此外，根据<a class="ae jo" href="https://arxiv.org/abs/1409.0575" rel="noopener ugc nofollow" target="_blank"> ImageNet大规模视觉识别挑战</a>，它提到:</p><blockquote class="kg kh ki"><p id="b6a8" class="iq ir kj is b it iu iv iw ix iy iz ja kk jc jd je kl jg jh ji km jk jl jm jn hm dt translated">“共有24支队伍参加了ILSVRC2013比赛，而前三年总共只有21支队伍。继2012年基于深度学习的方法获得成功之后，2013年的绝大多数参赛作品在提交材料中使用了深度卷积神经网络。<strong class="is hu">分类任务的获胜者是Clarifai </strong>，将几个大型深度卷积网络平均在一起。<strong class="is hu">使用(泽勒和弗格斯，2013年)</strong>、…”的可视化技术选择网络架构</p></blockquote><p id="375c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">上文中引用的参考文献(泽勒和弗格斯，2013年)是ZFNet。由此，正式宣布ZFNet胜出！</p><p id="7850" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">这是一篇<strong class="is hu"> 2014年ECCV </strong>的论文，在我写这篇文章的时候有超过<strong class="is hu"> 4000次引用</strong>。这是一篇重要的论文，它教导我们在深层可视化CNN内核。(<a class="kn ko gr" href="https://medium.com/u/aff72a0c1243?source=post_page-----d1a5a0c45103--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p><p id="513d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">ImageNet是一个数据集，包含超过1500万张带标签的高分辨率图像，约有22，000个类别。 ILSVRC使用ImageNet的一个子集，每个子集包含1000个类别中的大约1000幅图像。总的来说，大约有130万幅训练图像、50，000幅验证图像和100，000幅测试图像。</p><figure class="jq jr js jt fq ju fe ff paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="fe ff kp"><img src="../Images/6e34a2c18953d5bf4ea635d84f908adb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*32oD6_8r16fmzNmr.PNG"/></div></div><figcaption class="kb kc fg fe ff kd ke bd b be z ek"><strong class="bd kf">15 millions of images</strong></figcaption></figure></div><div class="ab cl kq kr hb ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hm hn ho hp hq"><h1 id="1e95" class="kx ky ht bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu dt translated">关于排名的一些事实</h1><figure class="jq jr js jt fq ju fe ff paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="fe ff lv"><img src="../Images/a74425b80273f00eeb6e0d1f85cef276.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7aUsUjRnKWkstksDNLSvZg.png"/></div></div><figcaption class="kb kc fg fe ff kd ke bd b be z ek"><strong class="bd kf">ILSVRC2013 Ranking [3]</strong></figcaption></figure><p id="7136" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">2013年，ZFNet是由罗布·弗格斯博士和他的博士生马修·泽勒博士在NYU发明的。(LeNet的发明者Yann LeCun教授也来自NYU。因此，他们也感谢LeCun教授在论文致谢部分的讨论。)这就是为什么它被称为ZFNet，基于他们的姓氏，泽勒和弗格斯，2014年ECCV的论文称为“<strong class="is hu">可视化和理解卷积网络</strong>”[1]。严格来说，ZFNet实际上并不是ILSVLC 2013的赢家。相反，Clarifai是当时新成立的公司，是ILSVLC 2013图像分类的获胜者。泽勒也是Clarifai的创始人兼首席执行官。</p><p id="5816" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">如上图所示，<strong class="is hu"> ZFNet与ILSVRC 2012中的获胜者AlexNet [2]相比，图像分类错误率有了显著的提高。</strong>clari fai相对于ZFNet只有很小的改进。(有关排名的更多详细信息，请访问[3]。)尽管如此，当我们在谈论ILSVLC 2013获胜者的深度学习网络时，我们通常谈论的是ZFNet [1]。</p></div><div class="ab cl kq kr hb ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hm hn ho hp hq"><h1 id="0eab" class="kx ky ht bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu dt translated">我们将涵盖的内容</h1><p id="8627" class="pw-post-body-paragraph iq ir ht is b it lw iv iw ix lx iz ja jb ly jd je jf lz jh ji jj ma jl jm jn hm dt translated">卷积网络如何以及为什么能够表现得如此之好一直是个谜。很多时候，我们只能靠直觉解释或者经验实验来推理。在这个故事中，我将介绍ZFNet如何可视化卷积网络。通过可视化卷积网络，ZFNet通过微调2012年发明的AlexNet，成为ILSVLC 2013在图像分类方面的赢家。因此，涵盖的部分包括:</p><ol class=""><li id="6a8e" class="mb mc ht is b it iu ix iy jb md jf me jj mf jn mg mh mi mj dt translated"><strong class="is hu">解除可视化技术</strong></li><li id="33fc" class="mb mc ht is b it mk ix ml jb mm jf mn jj mo jn mg mh mi mj dt translated"><strong class="is hu">各层可视化</strong></li><li id="5906" class="mb mc ht is b it mk ix ml jb mm jf mn jj mo jn mg mh mi mj dt translated"><strong class="is hu">基于可视化结果对AlexNet的修改</strong></li><li id="d2a8" class="mb mc ht is b it mk ix ml jb mm jf mn jj mo jn mg mh mi mj dt translated"><strong class="is hu">实验结果</strong></li><li id="e291" class="mb mc ht is b it mk ix ml jb mm jf mn jj mo jn mg mh mi mj dt translated"><strong class="is hu">结论</strong></li></ol></div><div class="ab cl kq kr hb ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hm hn ho hp hq"><h1 id="9dfe" class="kx ky ht bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu dt translated"><strong class="ak"> 1。可视化的Deconvnet技术</strong></h1><figure class="jq jr js jt fq ju fe ff paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="fe ff mp"><img src="../Images/2c336e3821cb45ca69e80b073b0118f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aph2aB6IcCuMft1-MLqtqQ.png"/></div></div><figcaption class="kb kc fg fe ff kd ke bd b be z ek">The Process to Deconv a Deep Layer</figcaption></figure><p id="4e36" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">我们应该知道，深度学习框架的一个标准步骤是要有一系列的<strong class="is hu"> Conv &gt;整流(激活函数)&gt;池化</strong>。为了可视化深层特征，我们需要一组decovnet技术来反转上述动作，以便我们可以在像素域中可视化该特征。</p><h2 id="df11" class="mq ky ht bd kz mr ms mt ld mu mv mw lh jb mx my ll jf mz na lp jj nb nc lt nd dt translated">1.1.取消排队</h2><figure class="jq jr js jt fq ju fe ff paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="fe ff ne"><img src="../Images/ec8fbe7c170460def7ff5535f7f1e1ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KyfQTpv1hYDg8ABXNt0FVg.jpeg"/></div></div><figcaption class="kb kc fg fe ff kd ke bd b be z ek"><strong class="bd kf">Unpooling</strong></figcaption></figure><p id="f39f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">最大汇集运算是不可逆的，但是我们可以通过记录每个汇集区域内最大值的位置来获得近似的逆运算，如上图所示。</p><h2 id="fabe" class="mq ky ht bd kz mr ms mt ld mu mv mw lh jb mx my ll jf mz na lp jj nb nc lt nd dt translated">1.2.<strong class="ak">整流(激活功能)</strong></h2><p id="1936" class="pw-post-body-paragraph iq ir ht is b it lw iv iw ix lx iz ja jb ly jd je jf lz jh ji jj ma jl jm jn hm dt translated">因为ReLU被用作激活函数，而ReLU是保持所有值为正值，同时使负值变为零。在逆向操作中，我们只需要再次执行ReLU即可。</p><h2 id="2c23" class="mq ky ht bd kz mr ms mt ld mu mv mw lh jb mx my ll jf mz na lp jj nb nc lt nd dt translated"><strong class="ak"> 1.3。德孔夫</strong></h2><figure class="jq jr js jt fq ju fe ff paragraph-image"><div class="fe ff nf"><img src="../Images/427778a21c19fcfb9d3a3483cd906014.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/0*uxsQQN6UtlxksaDX"/></div><figcaption class="kb kc fg fe ff kd ke bd b be z ek"><strong class="bd kf">Conv (Blue is input, cyan is output)</strong></figcaption></figure><figure class="jq jr js jt fq ju fe ff paragraph-image"><div class="fe ff ng"><img src="../Images/9f2ac9c9c84c00a9a5209522bf97e6c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/0*CJYLcAXhmOepbMmh"/></div><figcaption class="kb kc fg fe ff kd ke bd b be z ek"><strong class="bd kf">Deconv (Blue is input, cyan is output)</strong></figcaption></figure><p id="c3ee" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">做解卷积运算，确实是conv的一个转置版本。</p></div><div class="ab cl kq kr hb ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hm hn ho hp hq"><h1 id="ff4a" class="kx ky ht bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu dt translated"><strong class="ak"> 2。各层可视化</strong></h1><figure class="jq jr js jt fq ju fe ff paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="fe ff nh"><img src="../Images/35a5ca8f4b0c34c58b61f9cf2aba7002.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WbyE9tqJt8Kd0vqNX9MeVQ.png"/></div></div><figcaption class="kb kc fg fe ff kd ke bd b be z ek"><strong class="bd kf">Layer 1 and Layer 2</strong></figcaption></figure><p id="5904" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">通过使用deconv技术，在随机选择的特征图中显示了每个图层的前9个激活模式。并且<strong class="is hu">在层1和层2 </strong>中观察到两个问题。</p><p id="4510" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"> (i)第1层的滤波器是极高和极低频率信息的混合</strong>，几乎不覆盖中频。如果没有中频，就会产生连锁效应，深度特征只能从极高和极低频率的信息中学习。</p><p id="6976" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"> (ii)第2层显示由第1层卷积中使用的大步幅4引起的混叠伪像</strong>。<strong class="is hu">采样频率过低时会出现混叠。</strong></p><figure class="jq jr js jt fq ju fe ff paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="fe ff ni"><img src="../Images/e8174621992b3fb45c26f02b0a113841.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hpm0NDbqDDTYHHY_7OOPfQ.png"/></div></div><figcaption class="kb kc fg fe ff kd ke bd b be z ek"><strong class="bd kf">Layer 3</strong></figcaption></figure></div><div class="ab cl kq kr hb ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hm hn ho hp hq"><p id="a0cb" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">让我们再观察3层。</p><p id="ac9e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">第三层开始学习一些通用模式，</strong>比如网格模式，和文本模式。</p><figure class="jq jr js jt fq ju fe ff paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="fe ff nj"><img src="../Images/6956684a363c00e410c55ded10b2ab1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*69ty1ZX7OoScp7oXhqbs_A.png"/></div></div><figcaption class="kb kc fg fe ff kd ke bd b be z ek"><strong class="bd kf">Layer 4 and Layer 5</strong></figcaption></figure><p id="397b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">第4层显示出显著的变化，并且更具有类别特异性</strong>，例如狗的脸和鸟的腿。</p><p id="2723" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">第5层显示具有显著姿态变化的整个对象</strong>，例如键盘和狗。</p></div><div class="ab cl kq kr hb ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hm hn ho hp hq"><h1 id="5dc1" class="kx ky ht bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu dt translated"><strong class="ak"> 3。基于可视化结果对AlexNet的修改</strong></h1><figure class="jq jr js jt fq ju fe ff paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="fe ff nk"><img src="../Images/53cbfb81fe5e6af3e5822f55ec42851f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bFjBVvUL2Po_p2mKzC4iYQ.png"/></div></div><figcaption class="kb kc fg fe ff kd ke bd b be z ek"><strong class="bd kf">ZFNet</strong></figcaption></figure><p id="51e4" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">为了便于比较，ZFNet被重绘为与AlexNet相同的样式。为了解决在第1层和第2层观察到的两个问题，ZFNet做了两处修改。(要阅读AlexNet评论，请访问[4]。)</p><h2 id="c95b" class="mq ky ht bd kz mr ms mt ld mu mv mw lh jb mx my ll jf mz na lp jj nb nc lt nd dt translated"><strong class="ak"> (i)将第一层过滤器尺寸从11x11减小到7x7。</strong></h2><h2 id="da34" class="mq ky ht bd kz mr ms mt ld mu mv mw lh jb mx my ll jf mz na lp jj nb nc lt nd dt translated"><strong class="ak"> (ii)使卷积的第1层步幅为2，而不是4。</strong></h2><figure class="jq jr js jt fq ju fe ff paragraph-image"><div class="fe ff nl"><img src="../Images/b6f9cdd0b50f43c27f391b0f2b52fbba.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*I_sBto7QJkpSNMbfpJuf2A.png"/></div><figcaption class="kb kc fg fe ff kd ke bd b be z ek"><strong class="bd kf">Layer 1: (a) More mid-frequencies in ZFNet, (b) Extremely low and high frequencies in AlexNet</strong></figcaption></figure><figure class="jq jr js jt fq ju fe ff paragraph-image"><div class="fe ff nm"><img src="../Images/233070c35a9ad54d094039c58b335d22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*YTd-watnbNQA-vArfqS_AQ.png"/></div><figcaption class="kb kc fg fe ff kd ke bd b be z ek"><strong class="bd kf">Layer 2: (c) Aliasing artifacts in AlexNet and (d) much cleaner features in ZFNet</strong></figcaption></figure></div><div class="ab cl kq kr hb ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hm hn ho hp hq"><h1 id="e686" class="kx ky ht bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu dt translated"><strong class="ak"> 4。实验结果</strong></h1><h2 id="abf3" class="mq ky ht bd kz mr ms mt ld mu mv mw lh jb mx my ll jf mz na lp jj nb nc lt nd dt translated">4.1.基于烧蚀研究的改进ZFNet</h2><figure class="jq jr js jt fq ju fe ff paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="fe ff nn"><img src="../Images/8c4d12bba352033e702844d0101ceb03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nZXKnAJlA176XqzX-A8K5w.png"/></div></div><figcaption class="kb kc fg fe ff kd ke bd b be z ek"><strong class="bd kf">Ablation Study</strong></figcaption></figure><figure class="jq jr js jt fq ju fe ff paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="fe ff nk"><img src="../Images/9cb30ad354f82aeff2ef29603abb25a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5fGruNEnHM0PfKMmvmu7WQ.png"/></div></div><figcaption class="kb kc fg fe ff kd ke bd b be z ek"><strong class="bd kf">The Modified ZFNet based on Ablation Study</strong></figcaption></figure><p id="e635" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">也有关于去除或调整层消融研究。<strong class="is hu">改进的ZFNet在top-5验证错误上可以获得16.0%。</strong></p></div><div class="ab cl kq kr hb ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hm hn ho hp hq"><h2 id="1333" class="mq ky ht bd kz mr ms mt ld mu mv mw lh jb mx my ll jf mz na lp jj nb nc lt nd dt translated">4.2.与最先进方法的比较</h2><figure class="jq jr js jt fq ju fe ff paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="fe ff no"><img src="../Images/a08aaead82fa8482e1dd0c729e5bdcc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XB9pLAicEbIRQXuy_JLAaQ.png"/></div></div><figcaption class="kb kc fg fe ff kd ke bd b be z ek"><strong class="bd kf">Error Rate (%)</strong></figcaption></figure><p id="ce00" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">通过使用AlexNet，top-5验证错误率为18.1%。</strong></p><p id="8f55" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">使用ZFNet，前5名验证错误率为16.5% </strong>。我们可以得出结论，基于可视化的修改是必要的。</p><p id="ee7d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">通过使用来自(a)的5个ZFNet和来自(b)的1个修改的ZFNet，top-5验证错误率为14.7%。这又是一种已经在LeNet和AlexNet中使用的boosting技术。(请访问[5]和[4]了解更多关于升压技术的信息。)</strong></p></div><div class="ab cl kq kr hb ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hm hn ho hp hq"><h2 id="5283" class="mq ky ht bd kz mr ms mt ld mu mv mw lh jb mx my ll jf mz na lp jj nb nc lt nd dt translated">4.3.还测试了其他相对较小的数据集</h2><figure class="jq jr js jt fq ju fe ff paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="fe ff np"><img src="../Images/c85f247464718d7e3958eed7c020824a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6TilRPLTrhniCNxj_qw7Dg.png"/></div></div><figcaption class="kb kc fg fe ff kd ke bd b be z ek"><strong class="bd kf">Caltech 101 (83.8 to 86.5 mean accuracy)</strong></figcaption></figure><figure class="jq jr js jt fq ju fe ff paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="fe ff nq"><img src="../Images/05174e1f7d91842c13ebee0b9f5ffe28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LC2_EZzVC1BcU-_TVPih-Q.png"/></div></div><figcaption class="kb kc fg fe ff kd ke bd b be z ek"><strong class="bd kf">Caltech 256 (65.7 to 74.2 mean accuracy)</strong></figcaption></figure><figure class="jq jr js jt fq ju fe ff paragraph-image"><div class="fe ff nr"><img src="../Images/3153a26d518aa6ca038a5d756aa540be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*u_Mbpx69Z8nh-7RT_LOxvw.png"/></div><figcaption class="kb kc fg fe ff kd ke bd b be z ek"><strong class="bd kf">PASCAL 2012 (79.0 mean accuracy)</strong></figcaption></figure><p id="56eb" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">从上面的表中，我们可以看到，在没有使用ImageNet图像对ZFNet进行预训练的情况下，即从头开始训练ZFNet，准确性较低。<strong class="is hu">在预训练的ZFNet之上进行训练(微调)，准确率高得多。这意味着经过训练的过滤器被推广到不同的图像，而不仅仅是ImageNet的图像。</strong></p><p id="2b28" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">特别是对于加州理工学院101和加州理工学院256数据集，ZFNet有压倒性的结果。</p><p id="c5b3" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">对于PASCAL 2012，PASCAL图像可以包含多个对象，与ImageNet中的图像相比，与自然有很大不同。因此，精度稍低，但仍可与最先进的方法相媲美。</p></div><div class="ab cl kq kr hb ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hm hn ho hp hq"><h1 id="11ff" class="kx ky ht bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu dt translated">5.结论</h1><p id="4aeb" class="pw-post-body-paragraph iq ir ht is b it lw iv iw ix lx iz ja jb ly jd je jf lz jh ji jj ma jl jm jn hm dt translated">虽然以前只能观察到浅层特征，但本文提供了一种在像素域观察深层特征的有趣方法。</p><p id="0ed6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">通过逐层可视化卷积网络，ZFNet调整了AlexNet的滤波器大小或步幅等层超参数，并成功降低了错误率。</p></div><div class="ab cl kq kr hb ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hm hn ho hp hq"><p id="eaa8" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">为了更好地了解深度学习，了解最新的方法非常重要。我会写更多的故事。</p><p id="721f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">敬请关注！！！</p></div><div class="ab cl kq kr hb ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hm hn ho hp hq"><h1 id="7416" class="kx ky ht bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu dt translated">参考</h1><ol class=""><li id="92c1" class="mb mc ht is b it lw ix lx jb ns jf nt jj nu jn mg mh mi mj dt translated">【2014 ECCV】【ZFNet】<br/><a class="ae jo" href="https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf" rel="noopener ugc nofollow" target="_blank">可视化和理解卷积网络</a></li><li id="36c4" class="mb mc ht is b it mk ix ml jb mm jf mn jj mo jn mg mh mi mj dt translated">【2012 NIPS】【Alex net】<br/><a class="ae jo" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank">使用深度卷积神经网络的ImageNet分类</a></li><li id="957b" class="mb mc ht is b it mk ix ml jb mm jf mn jj mo jn mg mh mi mj dt translated">ILSVRC 2013排名<a class="ae jo" href="http://www.image-net.org/challenges/LSVRC/2013/results.php" rel="noopener ugc nofollow" target="_blank"><br/></a><a class="ae jo" href="http://www.image-net.org/challenges/LSVRC/2013/results.php#cls" rel="noopener ugc nofollow" target="_blank">http://www . image-net . org/challenges/ls vrc/2013/results . PHP # cls</a></li><li id="ee19" class="mb mc ht is b it mk ix ml jb mm jf mn jj mo jn mg mh mi mj dt translated"><a class="ae jo" rel="noopener" href="/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160">回顾AlexNet，CaffeNet——2012年ILSVRC(图像分类)获奖者</a></li><li id="90b9" class="mb mc ht is b it mk ix ml jb mm jf mn jj mo jn mg mh mi mj dt translated"><a class="ae jo" rel="noopener" href="/@sh.tsang/paper-brief-review-of-lenet-1-lenet-4-lenet-5-boosted-lenet-4-image-classification-1f5f809dbf17">LeNet-1、LeNet-4、LeNet-5、Boosted LeNet-4的回顾(图像分类)</a></li></ol><blockquote class="nv"><p id="ac60" class="nw nx ht bd ny nz oa ob oc od oe jn ek translated">加入Coinmonks <a class="ae jo" href="https://t.me/coincodecap" rel="noopener ugc nofollow" target="_blank">电报频道</a>和<a class="ae jo" href="https://www.youtube.com/c/coinmonks/videos" rel="noopener ugc nofollow" target="_blank"> Youtube频道</a>获取每日<a class="ae jo" href="http://coincodecap.com/" rel="noopener ugc nofollow" target="_blank">加密新闻</a></p></blockquote><h2 id="15d6" class="mq ky ht bd kz mr of mt ld mu og mw lh jb oh my ll jf oi na lp jj oj nc lt nd dt translated">另外，阅读</h2><ul class=""><li id="20fb" class="mb mc ht is b it lw ix lx jb ns jf nt jj nu jn ok mh mi mj dt translated"><a class="ae jo" rel="noopener" href="/coinmonks/top-10-crypto-copy-trading-platforms-for-beginners-d0c37c7d698c">复制交易</a> | <a class="ae jo" rel="noopener" href="/coinmonks/crypto-tax-software-ed4b4810e338">加密税务软件</a></li><li id="723e" class="mb mc ht is b it mk ix ml jb mm jf mn jj mo jn ok mh mi mj dt translated"><a class="ae jo" href="https://coincodecap.com/grid-trading" rel="noopener ugc nofollow" target="_blank">网格交易</a> | <a class="ae jo" rel="noopener" href="/coinmonks/the-best-cryptocurrency-hardware-wallets-of-2020-e28b1c124069">加密硬件钱包</a></li><li id="874f" class="mb mc ht is b it mk ix ml jb mm jf mn jj mo jn ok mh mi mj dt translated"><a class="ae jo" href="http://Top 4 Telegram Channels for Crypto Traders" rel="noopener ugc nofollow" target="_blank">密码电报信号</a> | <a class="ae jo" rel="noopener" href="/coinmonks/crypto-trading-bot-c2ffce8acb2a">密码交易机器人</a></li><li id="f33b" class="mb mc ht is b it mk ix ml jb mm jf mn jj mo jn ok mh mi mj dt translated"><a class="ae jo" rel="noopener" href="/coinmonks/crypto-exchange-dd2f9d6f3769">最佳加密交易所</a> | <a class="ae jo" rel="noopener" href="/coinmonks/bitcoin-exchange-in-india-7f1fe79715c9">印度最佳加密交易所</a></li><li id="6065" class="mb mc ht is b it mk ix ml jb mm jf mn jj mo jn ok mh mi mj dt translated"><a class="ae jo" href="https://coincodecap.com/binance-vs-bitstamp" rel="noopener ugc nofollow" target="_blank">币安vs Bitstamp </a> | <a class="ae jo" href="https://coincodecap.com/bitpanda-coinbase-coinsbit" rel="noopener ugc nofollow" target="_blank"> Bitpanda vs比特币基地vs Coinsbit </a></li><li id="5711" class="mb mc ht is b it mk ix ml jb mm jf mn jj mo jn ok mh mi mj dt translated"><a class="ae jo" href="https://coincodecap.com/buy-ripple-india" rel="noopener ugc nofollow" target="_blank">如何购买Ripple (XRP) </a> | <a class="ae jo" href="https://coincodecap.com/crypto-exchange-africa" rel="noopener ugc nofollow" target="_blank">非洲最好的加密交易所</a></li><li id="861b" class="mb mc ht is b it mk ix ml jb mm jf mn jj mo jn ok mh mi mj dt translated"><a class="ae jo" href="https://coincodecap.com/crypto-exchange-africa" rel="noopener ugc nofollow" target="_blank">非洲最佳加密交易所</a> | <a class="ae jo" href="https://coincodecap.com/hoo-exchange-review" rel="noopener ugc nofollow" target="_blank">胡交易所评论</a></li><li id="452c" class="mb mc ht is b it mk ix ml jb mm jf mn jj mo jn ok mh mi mj dt translated"><a class="ae jo" href="https://coincodecap.com/etoro-robinhood" rel="noopener ugc nofollow" target="_blank"> eToro vs罗宾汉</a>|<a class="ae jo" href="https://coincodecap.com/bybit-bityard-moonxbt" rel="noopener ugc nofollow" target="_blank">MoonXBT vs by bit vs Bityard</a></li><li id="47a8" class="mb mc ht is b it mk ix ml jb mm jf mn jj mo jn ok mh mi mj dt translated">开发人员的最佳加密API</li><li id="b359" class="mb mc ht is b it mk ix ml jb mm jf mn jj mo jn ok mh mi mj dt translated">最佳<a class="ae jo" rel="noopener" href="/coinmonks/top-5-crypto-lending-platforms-in-2020-that-you-need-to-know-a1b675cec3fa">密码借贷平台</a></li><li id="3c98" class="mb mc ht is b it mk ix ml jb mm jf mn jj mo jn ok mh mi mj dt translated"><a class="ae jo" rel="noopener" href="/coinmonks/free-crypto-signals-48b25e61a8da">免费加密信号</a> | <a class="ae jo" rel="noopener" href="/coinmonks/crypto-trading-bot-c2ffce8acb2a">加密交易机器人</a></li><li id="9487" class="mb mc ht is b it mk ix ml jb mm jf mn jj mo jn ok mh mi mj dt translated"><a class="ae jo" rel="noopener" href="/coinmonks/leveraged-token-3f5257808b22">杠杆代币</a>终极指南</li></ul></div></div>    
</body>
</html>