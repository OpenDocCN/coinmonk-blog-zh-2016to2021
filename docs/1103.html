<html>
<head>
<title>Backpropagation concept explained in 5 levels of difficulty</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">反向传播概念在5个难度级别中解释</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/backpropagation-concept-explained-in-5-levels-of-difficulty-8b220a939db5?source=collection_archive---------2-----------------------#2018-07-22">https://medium.com/coinmonks/backpropagation-concept-explained-in-5-levels-of-difficulty-8b220a939db5?source=collection_archive---------2-----------------------#2018-07-22</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><p id="b220" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">纯数学意义上的神经网络中的反向传播是什么？</strong></p><h1 id="e75a" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl dt translated"><strong class="ak">小家伙:</strong></h1><p id="8976" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hm dt translated">反向传播被计算机用来从错误中学习，更好地做一件特定的事情。因此，使用这种计算机可以继续猜测，并且越来越擅长猜测，就像人类在某一特定任务中所做的那样。</p><p id="108e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">然后，这些计算机可以将它们非常擅长的许多小任务组合起来，组成一个可以做更大事情的系统，比如驾驶汽车。</p><figure class="ks kt ku kv fq kw fe ff paragraph-image"><div class="fe ff kr"><img src="../Images/1ee0f5161f45e1c4cce72e2e3db8937d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/1*aOrcD5JOP_WuokXvhMVegA.gif"/></div><figcaption class="kz la fg fe ff lb lc bd b be z ek">Tesla self driving car</figcaption></figure><h1 id="ab47" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl dt translated"><strong class="ak">高中生:</strong></h1><p id="6cb7" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hm dt translated">人工智能可以通过向计算机提供大量数据以及人类提供的正确解决方案(称为标记数据)并训练线性分类器、神经网络等模型来获得，这些模型可以推广到它以前从未见过的数据。反向传播是计算机用来找出猜测和正确解之间的误差的技术，提供了对该数据的正确解。</p><p id="b054" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">反向传播在历史上被用来以快速的方式在数据结构中遍历树。我们让计算机猜测一个值，并使用微积分通过偏导数计算误差。然后，我们修正这个错误以获得更好的猜测，并再次反向传播以找到更好的调整错误。这种反复猜测的方法显示了反向传播的递归性。人工智能模型的训练需要很长的时间和大量的计算，它需要一切可以加快它的速度。</p><p id="833a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">一些人还认为反向传播是一种糟糕的方法，应该被解决方案所取代，这些解决方案涉及对偏导数进行积分，以一次性直接获得最终误差，而不是花费大量计算时间的步骤，或者一些其他算法。用别的东西代替反向传播仍然是一个研究领域。</p><p id="cb6d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">这个完整的算法被称为梯度下降。梯度下降(或类似算法)的一部分，你推断错误(通常用微积分)和纠正它被称为反向传播。</p><figure class="ks kt ku kv fq kw fe ff paragraph-image"><div class="fe ff ld"><img src="../Images/a3287a59f2d8819c272019651e181d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*GUrilHTazmuGffxCefSqcg.gif"/></div><figcaption class="kz la fg fe ff lb lc bd b be z ek">Gradient descent animation by Andrew Ng</figcaption></figure><h1 id="8168" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl dt translated"><strong class="ak">毕业:</strong></h1><p id="7527" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hm dt translated">因此，计算机科学中的反向传播是一种算法方式，通过这种方式，我们将某些计算的结果递归地发送回父节点。</p><p id="0e1d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Backpropagation</a></p><p id="691b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">在机器学习中，反向传播向神经网络发送反馈。</p><p id="a867" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">因此，任何训练步骤都包括计算梯度(微积分中的微分)，然后进行反向传播(对梯度进行积分，以返回权重应该变化的方式)。</p><p id="6af6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">理解微积分的简单案例研究:苹果vs橘子</strong></p><p id="7177" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">举个简单的例子，我们训练一个简单的直线分类器，它绘制`<strong class="is hu">y = MX+c `</strong>。分类器的目标是找出正确的m和c值，给定水果的半径(x轴)和rgb值(y轴)，对苹果和橘子进行二进制分类。<strong class="is hu">所以分类器必须在x-y平面上画一条直线，把它分成两部分，一部分是苹果，另一部分是橘子。</strong></p><p id="ae92" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">梯度将为d/dx(mx+c) =&gt; m，这是dy/dx的偏导数，在本例中为直线的斜率。</p><p id="a0b5" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">然后，我们在必须优化的某个函数上计算<strong class="is hu">损失</strong>，比如RMS prop(正确值与预测值的均方根)，其中，我们通过将实点(如(x1，y1)、(x2，y2)……(xn，yn)代入等式，将数据应用于我们的预测梯度值。</p><p id="5fe5" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">损失=RMS道具(guess_m，guess_c)(数据，正确答案)。</p><p id="c087" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">之所以称之为<strong class="is hu">损失</strong>，是因为<strong class="is hu">给了我们猜测和正确答案</strong>之间的错误。损失使用RMS支柱的优化函数上的演算来计算。</p><p id="c945" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">这将给我们一个“增量”，通过它我们当前的m和c值必须改变。</p><p id="3411" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">反向传播步骤是当我们计算“增量”并使用它来更新m和c值时。</p><figure class="ks kt ku kv fq kw fe ff paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="fe ff lf"><img src="../Images/be04bd97dda027b2e72b6a07ece5d6e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*61ph8G7TSnTzuaK8nXEIkw.png"/></div></div><figcaption class="kz la fg fe ff lb lc bd b be z ek">So by minimizing the loss , we get a better model as shown in the animation below</figcaption></figure><figure class="ks kt ku kv fq kw fe ff paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="fe ff lf"><img src="../Images/0baa1d2cb4ac1cf6f13fae21af2e9aea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*4f2v541e_S_Le0H2GyeLfg.gif"/></div></div></figure><p id="0102" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">现在在更复杂的场景说神经网络</strong></p><p id="11dd" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">神经网络中的每个神经元/感知器都由一个权重组成，该权重表示它在训练中累积的数据/偏差。<strong class="is hu">该权重例如可以是0和1的128×64矩阵。</strong>该示例中的128将是作为输入的前一层中的节点数目，而64将是下一层中的输出节点数目。</p><p id="ffda" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">在<strong class="is hu"> Adam优化</strong>函数中，我们有具有<strong class="is hu">对数值的等式来归一化比例</strong>。这意味着对“log”或具有“e”的等式(在数学中可以表示为无穷级数)进行积分和微分。</p><p id="85b7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">Adam优化函数有一个<strong class="is hu"> RMS道具</strong>值和一个动量函数，它来自<strong class="is hu"> AdaGrad </strong>。为了便于反向传播，我认为大家对Gradient depression都很熟悉，这一点在其他很多地方会有更好的解释。</p><p id="eca5" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/random _ grade _ depression # Adam</a></p><p id="74e0" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">反向传播步骤将发回维基百科链接中给出的值的“增量”。</p><figure class="ks kt ku kv fq kw fe ff paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="fe ff lk"><img src="../Images/9555e549da3c1ac77b2e718904747865.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ry5EAMjWdQNfFKMqCQ06QQ.png"/></div></div><figcaption class="kz la fg fe ff lb lc bd b be z ek">Wikipedia (ignore the complex symbols if you don’t get them)</figcaption></figure><p id="30b8" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">在哪里</p><p id="03ad" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">m =动量，</p><p id="3fbe" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">v =速度</p><figure class="ks kt ku kv fq kw fe ff paragraph-image"><div class="fe ff ll"><img src="../Images/7ff04c142ba6baf268fb7343400bb651.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*nJaj3HrBDFKZLyYFttJg_Q.png"/></div></figure><p id="cea6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">因此<strong class="is hu">权重(t+ 1) =权重(t) -增量</strong>。</p><p id="7b2b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">实际上，一层神经网络中的每个节点的权重都是通过一种叫做<strong class="is hu">向量化</strong>的技术同时并行计算出来的，这种技术通过在GPU、TPU或某些微处理器上使用矩阵乘法标志进行并行化来提高其性能。</p><p id="e8ce" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">所以更多关于</p><blockquote class="lm ln lo"><p id="100d" class="iq ir lp is b it iu iv iw ix iy iz ja lq jc jd je lr jg jh ji ls jk jl jm jn hm dt translated"><em class="ht">反向传播(对梯度进行积分，以恢复权重变化的方式)</em></p></blockquote><p id="18f7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">假设你在y=mx + c上训练这个模型100次</p><p id="fc24" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">所以在迭代1中:</strong></p><p id="3a94" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">m1 = m0(随机init)+δ(均方根损耗方程)，其中δ是通过对<strong class="is hu"> m </strong>的均方根损耗方程进行部分微分得到的</p><p id="7f6e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">c1 = c0(随机init)+δ(均方根损耗方程)，其中δ是通过对<strong class="is hu"> c </strong>的均方根损耗方程进行部分微分得到的</p><p id="6801" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">迭代2中:</strong></p><p id="6994" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">m2 = m1+δ(均方根损耗公式)</p><p id="f854" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">C2 = C1+δ(均方根损耗公式)</p><p id="041f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">在迭代1至100中:</strong></p><p id="d641" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">m100 = m0 + <strong class="is hu">求和</strong>(δ_从1到100(RMS prop方程))</p><p id="2339" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">这与</p><p id="1774" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">m100 = m0 + <strong class="is hu">积分</strong>(delta _从1到100(RMS prop方程))，这对于我们简单的苹果和橘子来说效果很好，除了如果网络很深，你不可能一次完成，而是一层一层地完成。</p><p id="eaf7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">现在，您已经了解了这个简单的过程，它同样适用于128 x 64权重的单个节点，我们必须讨论如何将其应用于神经网络中的此类节点的网络。要快速复习偏导数和如何计算微积分部分，请查看5分钟后的<a class="ae le" href="https://www.youtube.com/watch?v=q555kfIFUCM" rel="noopener ugc nofollow" target="_blank">反向传播</a>，您现在会更好地理解它。</p><p id="a747" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">如果我们能够将反向传播应用到更复杂的神经网络中，比如那些在LSTM、RNN或GRU的《穿越时间的反向传播》中描述的神经网络，我们就可以进行文本翻译、音乐生成等。</p><figure class="ks kt ku kv fq kw"><div class="bz el l di"><div class="lt lu l"/></div></figure><figure class="ks kt ku kv fq kw fe ff paragraph-image"><div class="fe ff lv"><img src="../Images/41b410275bd5960e3cfa507b6be90539.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/1*l2WkU7lU0R1X3lXFcAUzzA.gif"/></div><figcaption class="kz la fg fe ff lb lc bd b be z ek">Language translation can be done using LSTM’s</figcaption></figure><figure class="ks kt ku kv fq kw fe ff paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="fe ff lw"><img src="../Images/f13afedd482a7ef247370c66d07fe80e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GawZaLyYDZewzbmbqWwhlg.png"/></div></div></figure><p id="b282" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">所以到目前为止，我们讨论的是训练一个有输出函数f的节点的权重。</p><figure class="ks kt ku kv fq kw fe ff paragraph-image"><div class="fe ff lx"><img src="../Images/c48083dc8f67a8d0b4e72f14266094f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*u7YxfX-UZD3lAoSVTnwE-Q.png"/></div></figure><p id="c495" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">在深层神经网络中(deep表示许多层，一层的宽度就是其中的节点数)，必须通过每一层<strong class="is hu">向前传播</strong>得到预测值，计算误差，然后在每一层反向传播误差(用δ更新权重)。</p><p id="27b1" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">因此，将按照相反的顺序计算每一层的增量，网络将如下所示进行训练。如果网络有5层，<strong class="is hu">为了计算第1层</strong>的反向传播误差，我们必须从第1层= &gt;第2层= &gt; … = &gt;第5层= &gt;输出激活进行正向传播，然后从输出激活= &gt;第5层= &gt;第4层… = &gt;第1层进行反向传播，然后使用这个增量固定第1层中每个节点的权重。</p><figure class="ks kt ku kv fq kw fe ff paragraph-image"><div class="fe ff ld"><img src="../Images/338151d6a3ba7187ef9986a967e24fe1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*Atm4E9LZrhecmuGB.gif"/></div><figcaption class="kz la fg fe ff lb lc bd b be z ek">So after backpropagating the errors in all the nodes your model learns to correctly identify how important things are in its guesses or if it should guess/correlate things differently</figcaption></figure><p id="6343" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">或者对于卷积网络，你会有额外的困难，如通过池层反向传播</p><figure class="ks kt ku kv fq kw fe ff paragraph-image"><div class="fe ff ly"><img src="../Images/376915d077be19492b901c660b5adf23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*fiYwRfqCZxFX9qa20Z0ZqQ.png"/></div></figure><p id="784b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">因此，在深度网络中，前向传播后，您必须先反向传播激活函数(在最后一层),如下所示</p><p id="33a1" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">ReLU =&gt;T3】</strong></p><p id="b879" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">如果x&gt;0，y = x</p><p id="a889" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">= 0，如果x &lt;0</p><p id="7268" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">or <strong class="is hu"> Softmax </strong>是堆叠在一起的多个sigmoids</p><p id="cbf2" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">或者<strong class="is hu">Sigmoid</strong>=&gt;y = 1/(1+pow(e，-x))</p><p id="721e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">然后将“delta”作为损失函数传递给倒数第二层，该层将计算delta的delta，依此类推。上面描述的功能似乎很容易区分不是吗？</p><figure class="ks kt ku kv fq kw fe ff paragraph-image"><div class="fe ff lz"><img src="../Images/72a5a231f2c9170cc1dfae8bfd68acf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*lCf56WnZwftzkChAhLNBbA.png"/></div></figure><p id="3af2" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">一个更复杂的情况是，不得不反向传播一个深度网络，这个网络正在被分批训练或通过时间反向传播(就像RNN、LSTM或GRU那样)。</p><p id="22a4" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">这些循环神经网络或RNN神经网络有一种被称为“跳跃连接”的东西，这使它们能够在非常深的网络中轻松传播变化。门控循环单元或GRU氏症也有一个“记忆门”来跟踪数据中的一些“上下文”，还有一个“遗忘门”来标记上下文不相关的地方。LSTM的GRU门还有一个额外的“输出门”,在大型数据集上表现得更好。</p><p id="a62f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">这些LSTM可以用来建立网络，预测种族、性别、语言翻译、创作音乐和混合艺术风格。类似地，LSTM也有一种独特的方法来反向传播他们的误差，你可以通过对不同门的方程进行部分微分来估计。更多细节请看这篇关于<a class="ae le" href="http://axon.cs.byu.edu/~martinez/classes/678/Papers/Werbos_BPTT.pdf" rel="noopener ugc nofollow" target="_blank">通过时间</a>反向传播的论文。</p><p id="c882" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Long_short-term_memory</a></p><figure class="ks kt ku kv fq kw fe ff paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="fe ff lf"><img src="../Images/0758cb8ea140f79503a7f234e5eec4ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*lhP9QkVtAN0_gaKZ8UiUNQ.gif"/></div></div></figure><p id="1b3a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">TL；速度三角形定位法(dead reckoning)</p><p id="6220" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">反向传播的基本原理是，对于每个节点将有一个随机权重矩阵W，反向传播将更新为W = W-δ，其中δ是该节点的输出函数的导数，它表示在该节点的一个步骤中要校正的误差。</p><p id="30d4" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">也看看这个视频:<a class="ae le" href="https://www.youtube.com/watch?v=q555kfIFUCM" rel="noopener ugc nofollow" target="_blank">5分钟后反向传播</a></p><p id="fa23" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">要获得自己实现它的感觉的代码参考，请查看底部的中型文章。</p><h1 id="6824" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl dt translated"><strong class="ak">研究生&amp;博士:</strong></h1><p id="aa23" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hm dt translated">最好从专家的研究论文和视频中更详细地理解这个概念。我会为此提供一些链接。</p><p id="7d1c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">这里<a class="ae le" href="https://arxiv.org/pdf/1412.6980.pdf" rel="noopener ugc nofollow" target="_blank">是给亚当的纸。</a></p><p id="8e4a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><a class="ae le" href="https://www.youtube.com/watch?v=W86H4DpFnLY" rel="noopener ugc nofollow" target="_blank">这里</a>是一段由<strong class="is hu"> Yoshua Bengio </strong>制作的反向传播视频，他在过去几十年里一直大力支持反向传播。</p><p id="f3dd" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><a class="ae le" href="https://www.quora.com/Why-is-Geoffrey-Hinton-suspicious-of-backpropagation-and-wants-AI-to-start-over" rel="noopener ugc nofollow" target="_blank">这里</a>是一篇关于为什么<strong class="is hu"> Geoffrey Hinton </strong>认为我们应该放弃反向传播，寻找更好的方法的文章。</p><blockquote class="lm ln lo"><p id="8dee" class="iq ir lp is b it iu iv iw ix iy iz ja lq jc jd je lr jg jh ji ls jk jl jm jn hm dt translated">伊利诺斯大学厄巴纳-香槟分校(2006年)计算神经科学&amp;神经病学医学博士Tsvi Achler </p><p id="9c91" class="iq ir lp is b it iu iv iw ix iy iz ja lq jc jd je lr jg jh ji ls jk jl jm jn hm dt translated"><a class="ae le" href="https://www.quora.com/Why-is-Geoffrey-Hinton-suspicious-of-backpropagation-and-wants-AI-to-start-over/answer/Tsvi-Achler" rel="noopener ugc nofollow" target="_blank">2017年10月3日更新</a></p><p id="cc5a" class="iq ir lp is b it iu iv iw ix iy iz ja lq jc jd je lr jg jh ji ls jk jl jm jn hm dt translated">我认为辛顿做得还不够。反向传播不是根本问题，是网络结构问题。神经网络不应局限于前馈配置。Backprop只能训练前馈网络，并且只要网络是前馈的，它将仍然是最佳解决方案之一。</p></blockquote><p id="f7ba" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><a class="ae le" href="https://towardsdatascience.com/why-we-need-a-better-learning-algorithm-than-backpropagation-in-deep-learning-2faa0e81f6b" rel="noopener" target="_blank">https://towards data science . com/why-we-need-a-better-learning-in-deep-learning-2 FAA 0 e 81 f 6b</a></p><p id="7500" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">同样，对于如何从随机性和重复猜测中获得智慧的更广泛的直觉，看一看蒙特卡洛方法和马尔可夫链。</p><p id="7382" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">如果你上了<strong class="is hu">吴恩达关于深度学习</strong>的课程，他让你用代码对各种方程进行反向传播，我最喜欢的损失函数是</p><p id="d4cf" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">L(y '，y) = -(y*log(y') + (1-y)*log(1-y '))</p><p id="1bc7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">因为它被用在很多机器学习算法中，比如:</p><ul class=""><li id="e868" class="ma mb ht is b it iu ix iy jb mc jf md jj me jn mf mg mh mi dt translated">如果<code class="eh mj mk ml mm b">y = 1</code> == &gt; <code class="eh mj mk ml mm b">L(y',1) = -log(y')</code> == &gt;我们希望<code class="eh mj mk ml mm b">y'</code>最大== &gt; <code class="eh mj mk ml mm b">y</code>最大值为1</li><li id="1863" class="ma mb ht is b it mn ix mo jb mp jf mq jj mr jn mf mg mh mi dt translated">如果<code class="eh mj mk ml mm b">y = 0</code> == &gt; <code class="eh mj mk ml mm b">L(y',0) = -log(1-y')</code> == &gt;我们希望<code class="eh mj mk ml mm b">1-y'</code>最大== &gt; <code class="eh mj mk ml mm b">y'</code>尽可能小，因为它只能有1个值。</li></ul><p id="33f8" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">然后我们对L(y '，y)求导，得到梯度。</p><p id="0995" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">成为研究生或博士的一部分意味着能够阅读研究论文以获得洞察力。<strong class="is hu"> NIPS </strong>和<strong class="is hu"> ICLR </strong>是这一领域的著名活动，你可以从中了解更多信息，并获得大量资料。</p><p id="080c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">我没有在numpy中包含反向传播的代码示例，因为您最终通常会使用PyTorch或Tensorflow这样的库来实现它。<strong class="is hu">此外，由于反向传播与网络类型密切相关，每个节点类型通常有不同的实现方式</strong>，但事实上它保持不变，即你使用微积分来获得数学函数的误差，所以你必须阅读所有的论文来理解每个反向传播实现方式<strong class="is hu">。对大多数人来说，学习反向传播的目的是为了理解研究背后的数学原理，或者他们的图书馆正在做什么。我希望这篇文章能让你对这个概念有一个直观的认识，然后再去看其他一些高级文章，这些文章都是从直接实现它的困难开始的，比如</strong></p><p id="9e7a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">1.<a class="ae le" href="https://medium.freecodecamp.org/build-a-flexible-neural-network-with-backpropagation-in-python-acffeb7846d0" rel="noopener ugc nofollow" target="_blank">https://medium . freecodecamp . org/build-a-flexible-neural-network-with-back propagation-in-python-acffeb 7846d 0</a></p><p id="d773" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">2.<a class="ae le" href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" rel="noopener ugc nofollow" target="_blank">https://ayearofai . com/rohan-Lenny-1-neural-networks-the-back propagation-algorithm-explained-abf 4609d 4 f 9d</a></p><p id="c231" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">或者一些解释梯度下降直觉的文章比如<a class="ae le" rel="noopener" href="/datathings/neural-networks-and-backpropagation-explained-in-a-simple-way-f540a3611f5e">https://medium . com/data things/neural-networks-and-back propagation-in-a-simple-way-explained-f540a 3611 f5e</a></p><p id="7267" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">那都是乡亲们！</p><blockquote class="ms"><p id="53f6" class="mt mu ht bd mv mw mx my mz na nb jn ek translated">加入Coinmonks <a class="ae le" href="https://t.me/coincodecap" rel="noopener ugc nofollow" target="_blank">电报频道</a>和<a class="ae le" href="https://www.youtube.com/c/coinmonks/videos" rel="noopener ugc nofollow" target="_blank"> Youtube频道</a>获取每日<a class="ae le" href="http://coincodecap.com/" rel="noopener ugc nofollow" target="_blank">加密新闻</a></p></blockquote><h2 id="15d6" class="nc jp ht bd jq nd ne nf ju ng nh ni jy jb nj nk kc jf nl nm kg jj nn no kk np dt translated">另外，阅读</h2><ul class=""><li id="20fb" class="ma mb ht is b it km ix kn jb nq jf nr jj ns jn mf mg mh mi dt translated"><a class="ae le" rel="noopener" href="/coinmonks/top-10-crypto-copy-trading-platforms-for-beginners-d0c37c7d698c">复制交易</a> | <a class="ae le" rel="noopener" href="/coinmonks/crypto-tax-software-ed4b4810e338">加密税务软件</a></li><li id="723e" class="ma mb ht is b it mn ix mo jb mp jf mq jj mr jn mf mg mh mi dt translated"><a class="ae le" href="https://coincodecap.com/grid-trading" rel="noopener ugc nofollow" target="_blank">电网交易</a> | <a class="ae le" rel="noopener" href="/coinmonks/the-best-cryptocurrency-hardware-wallets-of-2020-e28b1c124069">加密硬件钱包</a></li><li id="874f" class="ma mb ht is b it mn ix mo jb mp jf mq jj mr jn mf mg mh mi dt translated"><a class="ae le" href="http://Top 4 Telegram Channels for Crypto Traders" rel="noopener ugc nofollow" target="_blank">密码电报信号</a> | <a class="ae le" rel="noopener" href="/coinmonks/crypto-trading-bot-c2ffce8acb2a">密码交易机器人</a></li><li id="f33b" class="ma mb ht is b it mn ix mo jb mp jf mq jj mr jn mf mg mh mi dt translated"><a class="ae le" rel="noopener" href="/coinmonks/crypto-exchange-dd2f9d6f3769">最佳加密交易所</a> | <a class="ae le" rel="noopener" href="/coinmonks/bitcoin-exchange-in-india-7f1fe79715c9">印度最佳加密交易所</a></li><li id="47a8" class="ma mb ht is b it mn ix mo jb mp jf mq jj mr jn mf mg mh mi dt translated"><a class="ae le" rel="noopener" href="/coinmonks/best-crypto-apis-for-developers-5efe3a597a9f">面向开发人员的最佳加密API</a></li><li id="b359" class="ma mb ht is b it mn ix mo jb mp jf mq jj mr jn mf mg mh mi dt translated">最佳<a class="ae le" rel="noopener" href="/coinmonks/top-5-crypto-lending-platforms-in-2020-that-you-need-to-know-a1b675cec3fa">密码借贷平台</a></li><li id="9487" class="ma mb ht is b it mn ix mo jb mp jf mq jj mr jn mf mg mh mi dt translated">杠杆代币的终极指南</li><li id="f1ee" class="ma mb ht is b it mn ix mo jb mp jf mq jj mr jn mf mg mh mi dt translated"><a class="ae le" href="https://coincodecap.com/zero-fee-crypto-exchanges" rel="noopener ugc nofollow" target="_blank"> 7个最佳零费用加密交易平台</a></li><li id="5aa5" class="ma mb ht is b it mn ix mo jb mp jf mq jj mr jn mf mg mh mi dt translated"><a class="ae le" href="https://coincodecap.com/best-online-casinos" rel="noopener ugc nofollow" target="_blank">最佳网上赌场</a> | <a class="ae le" rel="noopener" href="/coinmonks/futures-trading-bots-5a282ccee3f5">期货交易机器人</a></li><li id="3d0e" class="ma mb ht is b it mn ix mo jb mp jf mq jj mr jn mf mg mh mi dt translated"><a class="ae le" href="https://coincodecap.com/what-are-decentralized-exchanges" rel="noopener ugc nofollow" target="_blank">分散交易所</a> | <a class="ae le" href="https://coincodecap.com/bitbns-fip" rel="noopener ugc nofollow" target="_blank">比特FIP </a></li><li id="5cf9" class="ma mb ht is b it mn ix mo jb mp jf mq jj mr jn mf mg mh mi dt translated"><a class="ae le" href="https://coincodecap.com/buy-crypto-with-credit-card" rel="noopener ugc nofollow" target="_blank">用信用卡购买密码的10个最佳地点</a></li><li id="3d13" class="ma mb ht is b it mn ix mo jb mp jf mq jj mr jn mf mg mh mi dt translated"><a class="ae le" href="https://coincodecap.com/5-best-crypto-trading-bots-in-canada" rel="noopener ugc nofollow" target="_blank">加拿大最佳加密交易机器人</a> | <a class="ae le" href="https://coincodecap.com/bybit-binance-moonxbt" rel="noopener ugc nofollow" target="_blank"> Bybit vs币安</a></li></ul></div></div>    
</body>
</html>