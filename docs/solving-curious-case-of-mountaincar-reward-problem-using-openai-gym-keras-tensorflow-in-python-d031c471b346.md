# ç”¨ Python ä¸­çš„ OpenAI Gymï¼ŒKerasï¼ŒTensorFlow è§£å†³ç™»å±±è½¦å¥–åŠ±é—®é¢˜çš„å¥‡æ¡ˆ

> åŽŸæ–‡ï¼š<https://medium.com/coinmonks/solving-curious-case-of-mountaincar-reward-problem-using-openai-gym-keras-tensorflow-in-python-d031c471b346?source=collection_archive---------3----------------------->

![](img/7ef4cb708af5da6517c0cb0ce32b46fa.png)

è¿™ç¯‡æ–‡ç« å°†å¸®åŠ©ä½ ä½¿ç”¨ OpenAI Gym å’Œ TensorFlow ä¸º MountainCar ç­‰å›žæŠ¥è¾ƒå°‘çš„æ¸¸æˆç¼–å†™æ¸¸æˆæœºå™¨äººã€‚

ä¸€æ—¦æˆ‘å»ºç«‹äº†ä¸€ä¸ªçŽ©é’¢ç®¡èˆžæ¸¸æˆçš„æ¨¡åž‹ï¼Œæˆ‘è§‰å¾—å¾ˆæœ‰ä¿¡å¿ƒï¼Œå¹¶è®¤ä¸ºè®©æˆ‘ä»¬ä¸ºå¦ä¸€ä¸ªæ¸¸æˆç¼–å†™ä»£ç ï¼Œå¹¶å‘çŽ°ç™»å±±è½¦æ¸¸æˆå¾ˆæœ‰è¶£ï¼Œç„¶åŽæˆ‘æƒ³ä¸ºä»€ä¹ˆä¸ä¸ºå®ƒç¼–å†™ä»£ç ã€‚

ä¸€æ—¦æˆ‘å¼€å§‹å†™ä½œï¼Œæˆ‘å°±æ„è¯†åˆ°è¿™ä¸æ˜¯ä¸€ä»¶å®¹æ˜“çš„äº‹æƒ…ã€‚æœ€å¤§çš„é—®é¢˜æ˜¯å®ƒæ€»æ˜¯ç»™å‡ºä¸€ä¸ªè´Ÿçš„å¥–åŠ±ï¼Œæ— è®ºæˆ‘é‡‡å–ä»€ä¹ˆéšæœºè¡ŒåŠ¨éƒ½æ— å…³ç´§è¦ï¼Œæˆ‘æœ€ç»ˆå¾—åˆ°äº†æ€»åˆ† **-200** ï¼Œå¹¶æœ€ç»ˆè¾“æŽ‰äº†æ¯”èµ›ã€‚æˆ‘æ£€æŸ¥äº†ä¸åŒçš„æ–‡ç« ï¼Œå°è¯•äº†ä¸åŒçš„æ–¹æ³•ï¼Œä½†æ²¡æœ‰æ‰¾åˆ°æ­£ç¡®çš„ç­”æ¡ˆã€‚

åœ¨çœ‹äº†è¿™ä¹ˆå¤šåœ°æ–¹ä¹‹åŽï¼Œæˆ‘æ„è¯†åˆ°ä¸Žå…¶ä¾èµ–æ¸¸æˆç»™å‡ºçš„å¥–åŠ±ï¼Œä¸ºä»€ä¹ˆä¸æ ¹æ®å…·ä½“æƒ…å†µè‡ªå·±åˆ›é€ ä¸€ä¸ªå‘¢ï¼Ÿè¿™è§£å†³äº†æˆ‘çš„é—®é¢˜ã€‚æˆ‘æƒ³å’Œæ¯ä¸ªäººåˆ†äº«ï¼Œè¿™æ ·å°±æ²¡æœ‰äººä¼šç»åŽ†æˆ‘æ‰€ç»åŽ†çš„ç—›è‹¦ã€‚

ä¸è¦æµªè´¹å¤ªå¤šæ—¶é—´ï¼Œè®©æˆ‘ä»¬å¼€å§‹ç¼–ç å§ã€‚å¦‚æžœä½ æ˜¯ç¬¬ä¸€æ¬¡å°è¯• OpenAI å¥èº«æˆ¿ï¼Œè¯·åœ¨è¿™é‡Œé˜…è¯»æˆ‘çš„[ä»¥å‰çš„æ–‡ç« ](/@ashok.tankala/build-your-first-ai-game-bot-using-openai-gym-keras-tensorflow-in-python-50a4d4296687)ã€‚

é¦–å…ˆï¼Œè®©æˆ‘ä»¬å¯¼å…¥å®žçŽ°å®ƒæ‰€éœ€çš„åŒ…

```
import gym
import random
import numpy as np
from keras.models     import Sequential
from keras.layers     import Dense
from keras.optimizers import Adam
```

è®©æˆ‘ä»¬åˆ›å»ºçŽ¯å¢ƒå¹¶åˆå§‹åŒ–å˜é‡

```
env = gym.make('MountainCar-v0')
env.reset()
goal_steps = 200
score_requirement = -198
intial_games = 10000
```

åœ¨æˆ‘ä»¬å¼€å§‹å†™ä»£ç ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆäº†è§£ä¸€ä¸‹æˆ‘ä»¬å°†è¦è¿›å…¥çš„å†…å®¹

```
def play_a_random_game_first():
    for step_index in range(goal_steps):
        env.render()
        action = env.action_space.sample()
        observation, reward, done, info = env.step(action)
        print("Step {}:".format(step_index))
        print("action: {}".format(action))
        print("observation: {}".format(observation))
        print("reward: {}".format(reward))
        print("done: {}".format(done))
        print("info: {}".format(info))
        if done:
            break
    env.reset()
```

å¦‚æžœæ‰§è¡Œè¿™æ®µä»£ç ï¼Œæ‚¨å°†å¾—åˆ°å¦‚ä¸‹è¾“å‡º

```
Step 0:
action: 0
observation: [-0.55321127 -0.00078406]
reward: -1.0
done: False
info: {}
Step 1:
action: 1
observation: [-0.55377353 -0.00056225]
reward: -1.0
done: False
info: {}
...
Step 198:
action: 0
observation: [-0.40182971  0.01383677]
reward: -1.0
done: False
info: {}
Step 199:
action: 1
observation: [-0.38888603  0.01294368]
reward: -1.0
done: True
info: {}
```

æ ¹æ®æ–‡æ¡£"-1ï¼Œæ¯ä¸ªæ—¶é—´æ­¥é•¿ï¼Œç›´åˆ°è¾¾åˆ° 0.5 çš„ç›®æ ‡ä½ç½®ã€‚å’Œ MountainCarContinuous v0 ä¸€æ ·ï¼Œçˆ¬å·¦è¾¹çš„å±±æ˜¯æ²¡æœ‰æƒ©ç½šçš„ï¼Œåˆ°è¾¾å·¦è¾¹çš„å±±å°±åƒä¸€å µå¢™ã€‚â€

å½“æ‚¨åˆ°è¾¾ 0.5(é¡¶éƒ¨)ä½ç½®æ—¶ï¼Œæˆ–è€…è¾¾åˆ° 200 æ¬¡è¿­ä»£æ—¶ï¼Œå‰§é›†ç»“æŸã€‚æˆ‘çŽ©äº† 10000 æ¬¡å‡ æ¬¡ï¼Œä½†ä»Žæ¥æ²¡æœ‰è¾¾åˆ°æœ€é«˜ä½ç½®ã€‚æ‰€ä»¥åœ¨æ•°æ®å¡«å……çš„æ—¶å€™ï¼Œæˆ‘æ”¹äº†ä¸€ä¸ªå°é€»è¾‘ç»ˆäºŽç»™äº†æˆ‘è§£å†³æ–¹æ¡ˆã€‚

æ•°æ®å¡«å……çš„ä»£ç æ˜¯

å…³é”®éƒ¨åˆ†åœ¨äºŽä¸Šé¢çš„ä»£ç ï¼Œè®©æˆ‘ä»¬é€è¡Œç†è§£ï¼Œæˆ‘å°†è§£é‡Šå¸®åŠ©æˆ‘è§£å†³è¿™ä¸ªé—®é¢˜çš„è°ƒæ•´ã€‚

1.  æˆ‘ä»¬åˆå§‹åŒ–äº† training_data å’Œ accepted_scores æ•°ç»„ã€‚
2.  æˆ‘ä»¬éœ€è¦çŽ©å¤šæ¬¡ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥æ”¶é›†æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥ä½¿ç”¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æ’­æ”¾ 10000 æ¬¡ï¼Œä»¥ä¾¿èŽ·å¾—å¤§é‡æ•°æ®ã€‚è¿™ä¸€è¡Œè¡¨ç¤ºâ€œæ¸¸æˆç´¢å¼•åœ¨èŒƒå›´å†…(åˆå§‹æ¸¸æˆ):â€
3.  æˆ‘ä»¬åˆå§‹åŒ–äº† scoreã€game_memoryã€previous_observation å˜é‡ï¼Œè¿™äº›å˜é‡å°†å­˜å‚¨å½“å‰æ¸¸æˆçš„æ€»å¾—åˆ†å’Œå‰ä¸€æ­¥è§‚å¯Ÿ(æŒ‡æ±½è½¦çš„ä½ç½®å’Œé€Ÿåº¦)ä»¥åŠæˆ‘ä»¬ä¸ºæ­¤é‡‡å–çš„è¡ŒåŠ¨ã€‚
4.  å¯¹äºŽèŒƒå›´å†…çš„ step _ index(goal _ steps):â€”æ­¤ä»£ç å°†æ¸¸æˆè¿›è¡Œ 200 æ­¥ï¼Œå› ä¸ºå½“æ‚¨åˆ°è¾¾ 0.5(é¡¶éƒ¨)ä½ç½®æ—¶ï¼Œæˆ–è€…å¦‚æžœè¾¾åˆ° 200 æ¬¡è¿­ä»£ï¼Œåˆ™ä¸€é›†ç»“æŸã€‚
5.  æˆ‘ä»¬éœ€è¦é‡‡å–éšæœºè¡ŒåŠ¨ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥çŽ©æ¸¸æˆï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´æˆåŠŸå®Œæˆè¯¥æ­¥éª¤æˆ–è¾“æŽ‰æ¸¸æˆã€‚è¿™é‡Œåªå…è®¸ 3 ä¸ªåŠ¨ä½œ:å‘å·¦æŽ¨(0)ï¼Œä¸æŽ¨(1)å’Œå‘å³æŽ¨(2)ã€‚æ‰€ä»¥è¿™æ®µä»£ç (random.randrange(0ï¼Œ3))ç”¨äºŽé‡‡å–éšæœºæ“ä½œä¹‹ä¸€ã€‚
6.  æˆ‘ä»¬å°†é‡‡å–é‚£ä¸ªè¡ŒåŠ¨/æ­¥éª¤ã€‚ç„¶åŽï¼Œæˆ‘ä»¬å°†æ£€æŸ¥å®ƒæ˜¯å¦æ˜¯ç¬¬ä¸€ä¸ªè¡ŒåŠ¨/æ­¥éª¤ï¼Œç„¶åŽæˆ‘ä»¬å°†å­˜å‚¨ä¹‹å‰çš„è§‚å¯Ÿå’Œæˆ‘ä»¬ä¸ºæ­¤é‡‡å–çš„è¡ŒåŠ¨ã€‚
7.  ç„¶åŽï¼Œæˆ‘ä»¬å°†æ£€æŸ¥è§‚å¯Ÿå€¼ä¸º[0]çš„æ±½è½¦çš„ä½ç½®æ˜¯å¦å¤§äºŽ-0.2ï¼Œå¦‚æžœæ˜¯ï¼Œé‚£ä¹ˆæˆ‘ä¸æŽ¥å—æˆ‘ä»¬çš„æ¸¸æˆçŽ¯å¢ƒç»™å‡ºçš„å¥–åŠ±ï¼Œè€Œæ˜¯æŽ¥å— 1ï¼Œå› ä¸º-0.2 çš„ä½ç½®æ˜¯å±±é¡¶ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬çš„éšæœºè¡ŒåŠ¨äº§ç”Ÿäº†ä¸€äº›ä¸°ç¡•çš„æˆæžœã€‚
8.  å°†å¥–åŠ±åŠ åˆ°åˆ†æ•°ä¸Šï¼Œæ£€æŸ¥æ¸¸æˆæ˜¯å¦å®Œæˆï¼Œå¦‚æžœæ˜¯ï¼Œåˆ™åœæ­¢æ¸¸æˆã€‚
9.  æˆ‘ä»¬å°†æ£€æŸ¥è¿™ä¸ªæ¸¸æˆæ˜¯å¦æ»¡è¶³æˆ‘ä»¬çš„æœ€ä½Žè¦æ±‚ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬æ˜¯å¦èƒ½å¤Ÿå¾—åˆ°å¤§äºŽæˆ–ç­‰äºŽ-198 çš„åˆ†æ•°ã€‚
10.  å¦‚æžœæˆ‘ä»¬èƒ½å¤Ÿå¾—åˆ°å¤§äºŽæˆ–ç­‰äºŽ-198 çš„åˆ†æ•°ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†æŠŠè¿™ä¸ªåˆ†æ•°æ·»åŠ åˆ° accept_scores ä¸­ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ‰“å°è¿™ä¸ªåˆ†æ•°ï¼Œä»¥äº†è§£æˆ‘ä»¬å°†å¤šå°‘æ¸¸æˆæ•°æ®åŠå…¶åˆ†æ•°è¾“å…¥åˆ°æˆ‘ä»¬çš„æ¨¡åž‹ä¸­ã€‚
11.  ç„¶åŽæˆ‘ä»¬å°†å¯¹åŠ¨ä½œè¿›è¡Œçƒ­ç¼–ç ï¼Œå› ä¸ºå®ƒçš„å€¼ 0(å‘å·¦æŽ¨)ã€1(ä¸æŽ¨)ã€2(å‘å³æŽ¨)ä»£è¡¨åˆ†ç±»æ•°æ®ã€‚
12.  ç„¶åŽæˆ‘ä»¬ä¼šå°†å®ƒæ·»åŠ åˆ°æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®ä¸­ã€‚
13.  æˆ‘ä»¬å°†é‡ç½®çŽ¯å¢ƒï¼Œä»¥ç¡®ä¿ä¸€åˆ‡éƒ½æ¸…æ¥šï¼Œå¼€å§‹çŽ©ä¸‹ä¸€ä¸ªæ¸¸æˆã€‚
14.  print(accepted_scores) â€”è¯¥ä»£ç ç”¨äºŽäº†è§£æˆ‘ä»¬å‘æ¨¡åž‹ä¸­è¾“å…¥äº†å¤šå°‘æ¸¸æˆæ•°æ®åŠå…¶åˆ†æ•°ã€‚ç„¶åŽæˆ‘ä»¬ä¼šè¿”å›žè®­ç»ƒæ•°æ®ã€‚

æˆ‘ä»¬å°†å¾—åˆ°ä¸€äº›åˆç†çš„æ¸¸æˆåˆ†æ•°å¦‚ä¸‹

```
[-158.0, -172.0, -188.0, -196.0, -168.0, -182.0, -180.0, -184.0, -184.0, -184.0, -168.0, -184.0, -176.0, -182.0, -182.0, -196.0, -184.0, -194.0, -178.0, -176.0, -170.0, -190.0, -182.0, -184.0, -184.0, -188.0, -184.0, -192.0, -172.0, -186.0, -174.0, -166.0, -188.0, -186.0, -174.0, -190.0, -178.0, -170.0, -164.0, -180.0, -184.0, -172.0, -168.0, -174.0, -172.0, -174.0, -186.0]
```

æ‰€ä»¥æˆ‘ä»¬çš„æ•°æ®å‡†å¤‡å¥½äº†ã€‚æ˜¯æ—¶å€™å»ºç«‹æˆ‘ä»¬çš„ç¥žç»ç½‘ç»œäº†ã€‚

```
def build_model(input_size, output_size):
    model = Sequential()
    model.add(Dense(128, input_dim=input_size, activation='relu'))
    model.add(Dense(52, activation='relu'))
    model.add(Dense(output_size, activation='linear'))
    model.compile(loss='mse', optimizer=Adam())return model
```

è¿™é‡Œæˆ‘ä»¬å°†ä½¿ç”¨åºåˆ—æ¨¡åž‹ã€‚

```
def train_model(training_data):
    X = np.array([i[0] for i in training_data]).reshape(-1, len(training_data[0][0]))
    y = np.array([i[1] for i in training_data]).reshape(-1, len(training_data[0][1]))
    model = build_model(input_size=len(X[0]), output_size=len(y[0]))

    model.fit(X, y, epochs=5)
    return model
```

æˆ‘ä»¬æœ‰è®­ç»ƒæ•°æ®ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬å°†åˆ›å»ºåŠŸèƒ½å’Œæ ‡ç­¾ã€‚

ç„¶åŽæˆ‘ä»¬å°†å¼€å§‹è®­ç»ƒ

```
trained_model = train_model(training_data)
```

æˆ‘ä»¬å°†å¾—åˆ°è¿™æ ·çš„è¾“å‡º

```
Epoch 1/5
9353/9353 [==============================] - 1s 90us/step - loss: 0.2262
Epoch 2/5
9353/9353 [==============================] - 1s 66us/step - loss: 0.2217
Epoch 3/5
9353/9353 [==============================] - 1s 65us/step - loss: 0.2209
Epoch 4/5
9353/9353 [==============================] - 1s 64us/step - loss: 0.2201
Epoch 5/5
9353/9353 [==============================] - 1s 61us/step - loss: 0.2199
```

æ˜¯æ—¶å€™è®©æˆ‘ä»¬çš„æ¸¸æˆæœºå™¨äººä¸ºæˆ‘ä»¬çŽ©æ¸¸æˆäº†ã€‚

```
scores = []
choices = []
for each_game in range(100):
    score = 0
    game_memory = []
    prev_obs = []
    for step_index in range(goal_steps):
        env.render()
        if len(prev_obs)==0:
            action = random.randrange(0,2)
        else:
            action = np.argmax(trained_model.predict(prev_obs.reshape(-1, len(prev_obs)))[0])

        choices.append(action)
        new_observation, reward, done, info = env.step(action)
        prev_obs = new_observation
        game_memory.append([new_observation, action])
        score += reward
        if done:
            breakenv.reset()
    scores.append(score)print(scores)
print('Average Score:',sum(scores)/len(scores))
print('choice 1:{}  choice 0:{} choice 2:{}'.format(choices.count(1)/len(choices),choices.count(0)/len(choices),choices.count(2)/len(choices)))
```

è¿™é‡Œä½ å¯ä»¥çœ‹åˆ°æˆ‘æ ¹æœ¬æ²¡ç¢°å¥–åŠ±éƒ¨åˆ†ã€‚ä½†æ˜¯æˆ‘ä»¬çš„æ¨¡åž‹çŸ¥é“å¦‚æžœå®ƒåšäº†ä»€ä¹ˆåŠ¨ä½œï¼Œå®ƒå°†åˆ°è¾¾å±±é¡¶ï¼Œæ‰€ä»¥å®ƒè‡ªåŠ¨æ‰§è¡Œè‰¯å¥½ã€‚æ‰§è¡Œè¿™æ®µä»£ç åŽï¼Œä½ ä¼šå¾—åˆ°è¿™æ ·çš„åˆ†æ•°

```
[-164.0, -92.0, -162.0, -107.0, -105.0, -93.0, -97.0, -90.0, -96.0, -170.0, -99.0, -200.0, -164.0, -91.0, -200.0, -92.0, -195.0, -166.0, -104.0, -93.0, -164.0, -200.0, -200.0, -164.0, -179.0, -176.0, -122.0, -101.0, -91.0, -162.0, -99.0, -164.0, -190.0, -199.0, -101.0, -200.0, -186.0, -185.0, -170.0, -128.0, -164.0, -164.0, -166.0, -101.0, -167.0, -89.0, -105.0, -168.0, -166.0, -100.0, -100.0, -91.0, -90.0, -163.0, -165.0, -167.0, -165.0, -105.0, -88.0, -134.0, -95.0, -90.0, -166.0, -166.0, -89.0, -167.0, -162.0, -165.0, -164.0, -171.0, -163.0, -127.0, -95.0, -159.0, -89.0, -89.0, -96.0, -168.0, -96.0, -163.0, -89.0, -90.0, -183.0, -166.0, -164.0, -163.0, -171.0, -167.0, -163.0, -97.0, -171.0, -166.0, -89.0, -200.0, -162.0, -175.0, -198.0, -93.0, -200.0, -106.0]
Average Score: -141.12
choice 1:0.007936507936507936  choice 0:0.5136054421768708 choice 2:0.47845804988662133
```

å¹²å¾—å¥½ï¼Œä½ çš„æœºå™¨äººåšå¾—éžå¸¸å¥½ã€‚

![](img/caddff4ff6400196755e85afa68513f5.png)

æ­å–œä½ ã€‚ï¼ï¼ä½ å¾ˆå¥½åœ°ç†è§£äº†å¥–åŠ±æœºåˆ¶ï¼Œä¹Ÿç†è§£äº†å¦‚æžœä½ çš„æ¸¸æˆå¯¹å¥–åŠ±ä¸å‹å¥½ï¼Œè¯¥å¦‚ä½•è®¾è®¡è§£å†³æ–¹æ¡ˆã€‚

ä½ å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ° Jupyter ç¬”è®°æœ¬ã€‚

å¦‚æžœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œè¯·ç»™å®ƒä¸€äº›æŽŒå£°æ¥è¡¨è¾¾ä½ çš„çˆ±ðŸ‘ã€‚
å’Œå¹³ã€‚å¿«ä¹ç¼–ç ã€‚
[è¿™é‡Œçœ‹æˆ‘çš„åŽŸåˆ›æ–‡ç« ã€‚](https://blog.tanka.la/2018/10/19/solving-curious-case-of-mountaincar-reward-problem-using-openai-gym-keras-tensorflow-in-python/)

> [åœ¨æ‚¨çš„æ”¶ä»¶ç®±ä¸­ç›´æŽ¥èŽ·å¾—æœ€ä½³è½¯ä»¶äº¤æ˜“](https://coincodecap.com/?utm_source=coinmonks)

[![](img/7c0b3dfdcbfea594cc0ae7d4f9bf6fcb.png)](https://coincodecap.com/?utm_source=coinmonks)