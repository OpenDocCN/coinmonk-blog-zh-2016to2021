<html>
<head>
<title>Word-level LSTM text generator. Creating automatic song lyrics with Neural Networks.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单词级LSTM文本生成器。用神经网络生成自动歌词。</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/word-level-lstm-text-generator-creating-automatic-song-lyrics-with-neural-networks-b8a1617104fb?source=collection_archive---------0-----------------------#2018-06-04">https://medium.com/coinmonks/word-level-lstm-text-generator-creating-automatic-song-lyrics-with-neural-networks-b8a1617104fb?source=collection_archive---------0-----------------------#2018-06-04</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><figure class="fi fk ir is it iu fe ff paragraph-image"><div class="fe ff iq"><img src="../Images/6cd7a8920f9995ee829bbef63ea352b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*BiFJutnzDsH6migEkun24Q.png"/></div></figure><p id="2d59" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">我开始用非技术性的聊天来谈论这个项目，谈论我对墨西哥班达音乐(西班牙语)的5000首歌词(超过500万字符)进行的分析。</p><div class="jw jx fm fo jy jz"><a rel="noopener follow" target="_blank" href="/@monocasero/analizando-más-5-000-letras-de-canciones-de-música-regional-mexicana-banda-8f6aecceb5b4"><div class="ka ab ej"><div class="kb ab kc cl cj kd"><h2 class="bd hu fv z el ke eo ep kf er et hs dt translated">5，000莱特拉德墨西哥州(班达)</h2><div class="kg l"><h3 class="bd b fv z el ke eo ep kf er et ek translated">作为一个个人项目的一部分，在他工作的时候，他记录了一个文集…</h3></div><div class="kh l"><p class="bd b gc z el ke eo ep kf er et ek translated">medium.com</p></div></div></div></a></div><p id="5b96" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">我决定用英语来写下面的故事/教程，因为这样自然可以接触到更多的读者。</p><p id="c57a" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">完整的代码和语料库可以在<a class="ae jv" href="https://github.com/enriqueav/lstm_lyrics" rel="noopener ugc nofollow" target="_blank">这个github库</a>找到。</p><p id="74be" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated"><strong class="iz hu">更新:</strong>第二部可用<a class="ae jv" rel="noopener" href="/coinmonks/text-classifier-with-keras-tensorflow-using-recurrent-neural-networks-ad63dd5fc316">此处</a>。我用单词嵌入来解释这个训练。"<em class="ki">单词嵌入提供了单词及其相对含义的密集表示."</em></p><h1 id="bcad" class="kj kk ht bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">快速背景</h1><blockquote class="lh li lj"><p id="ba1a" class="ix iy ki iz b ja jb jc jd je jf jg jh lk jj jk jl ll jn jo jp lm jr js jt ju hm dt translated"><strong class="iz hu">长短期记忆</strong> ( <strong class="iz hu"> LSTM </strong>)单位(或块)是一个<a class="ae jv" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank">递归神经网络</a> (RNN)的层的一个构建单位。由LSTM单元组成的RNN通常被称为LSTM网络。一个普通的LSTM单元由一个<strong class="iz hu">单元</strong>、一个<strong class="iz hu">输入门</strong>、一个<strong class="iz hu">输出门</strong>和一个<strong class="iz hu">遗忘门</strong>组成。来源<a class="ae jv" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank">维基百科</a>。</p></blockquote><p id="3c6c" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">rnn可用于进行预测，或从序列数据中学习并生成类似的数据。</p><p id="fc79" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">像许多在线文本生成的例子一样，我从keras-team的<a class="ae jv" href="https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py" rel="noopener ugc nofollow" target="_blank">例子</a>和这个<a class="ae jv" href="https://towardsdatascience.com/yet-another-text-generation-project-5cfb59b26255" rel="noopener" target="_blank">项目中获得灵感，生成类似特朗普的推文</a>。</p><p id="1cea" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">主要的区别是，我创建的文本生成器是在单词级而不是字符级工作的，就像前面的例子一样。此外，由于训练集不适合内存(特别是如果发送到GPU)，我需要为<strong class="iz hu"> fit </strong>和<strong class="iz hu"> evaluate </strong> Keras函数创建一个数据生成器。</p><p id="9d3b" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated"><em class="ki">我不得不承认，在做一个快速的谷歌搜索之前，我一直在做这个项目的版本。我终于做到了，发现</em> <a class="ae jv" href="https://github.com/rdcolema/word-level-rnn-for-text-generation/blob/master/word_gen.py" rel="noopener ugc nofollow" target="_blank"> <em class="ki">这个项目</em> </a> <em class="ki">，也是基于相同的脚本，因此有点类似于我的版本:</em></p><h1 id="6e67" class="kj kk ht bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">算法的解释</h1><p id="4f3d" class="pw-post-body-paragraph ix iy ht iz b ja ln jc jd je lo jg jh ji lp jk jl jm lq jo jp jq lr js jt ju hm dt translated">这个想法是用许多单词序列和目标<em class="ki"> next_word来训练RNN。</em>作为一个简化的例子，如果每个句子是一个五个单词的列表，那么目标是一个只有一个元素的列表，指示哪个是原文中下面的单词:</p><pre class="ls lt lu lv fq lw lx ly lz aw ma dt"><span id="83dc" class="mb kk ht lx b fv mc md l me mf">&gt;&gt;&gt; sentences[0]</span><span id="2018" class="mb kk ht lx b fv mg md l me mf">['put', 'a', 'gun', 'against', 'his']</span><span id="5261" class="mb kk ht lx b fv mg md l me mf">&gt;&gt;&gt; next_words[0]</span><span id="e1ff" class="mb kk ht lx b fv mg md l me mf">'head'</span><span id="7ac1" class="mb kk ht lx b fv mg md l me mf">&gt;&gt;&gt; sentences[1]</span><span id="31f5" class="mb kk ht lx b fv mg md l me mf">['a', 'gun', 'against', 'his', 'head']</span><span id="d965" class="mb kk ht lx b fv mg md l me mf">&gt;&gt;&gt; next_words[1]</span><span id="c1fc" class="mb kk ht lx b fv mg md l me mf">'pulled'</span></pre><p id="67dd" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">我们实际上并不发送字符串，而是发送一个可能单词的字典中的单词的矢量化表示(稍后会详细介绍)。这个想法是，在许多代之后，RNN将学习如何书写语料库的“风格”,试图调整网络的权重，以预测给定N个先前单词序列的下一个单词。</p><h1 id="4705" class="kj kk ht bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">文本语料库</h1><p id="5470" class="pw-post-body-paragraph ix iy ht iz b ja ln jc jd je lo jg jh ji lp jk jl jm lq jo jp jq lr js jt ju hm dt translated">正如我在另一个故事中解释的那样，语料库包含超过100万个单词中的超过500万个字符。获取文本只是问题的开始，因为与任何其他机器学习项目一样，有必要分析、清理和执行这些数据的一些预处理。</p><p id="65cd" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">我现在不会进入细节(可能是另一个故事)，但至少可以说，数据是<strong class="iz hu">脏的。</strong>成千上万的错别字、拼写错误、俚语、不正确的标点符号、<a class="ae jv" href="https://en.wikipedia.org/wiki/Spanglish" rel="noopener ugc nofollow" target="_blank"> spanglish </a>等等。</p><p id="d466" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">现在谈谈算法。</p></div><div class="ab cl mh mi hb mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hm hn ho hp hq"><h1 id="2dde" class="kj kk ht bd kl km mo ko kp kq mp ks kt ku mq kw kx ky mr la lb lc ms le lf lg dt translated">阅读文集，拆分成单词</h1><p id="b24e" class="pw-post-body-paragraph ix iy ht iz b ja ln jc jd je lo jg jh ji lp jk jl jm lq jo jp jq lr js jt ju hm dt translated">第一步，阅读语料库，拆分成单词。</p><pre class="ls lt lu lv fq lw lx ly lz aw ma dt"><span id="8691" class="mb kk ht lx b fv mc md l me mf">corpus = sys.argv[1] # first command line arg<br/>with io.open(corpus, encoding='utf-8') as f:<br/>    text = f.read().lower().replace('\n', ' \n ')<br/>print('Corpus length in characters:', len(text))<br/><br/>text_in_words = [w for w in text.split(' ') if w.strip() != '' or w == '\n']<br/>print('Corpus length in words:', len(text_in_words))</span></pre><p id="12bd" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">注意对<code class="eh mt mu mv lx b">.replace(‘\n’, ‘ \n ‘</code>的调用，这是因为我们希望换行符是一个单词。这背后的想法是，我们也让网络决定何时开始一个新的行(在几个字之后)。在这之后，<em class="ki"> text_in_words </em>是一个包含所有语料库的大数组，逐字逐句。</p><pre class="ls lt lu lv fq lw lx ly lz aw ma dt"><span id="0fdc" class="mb kk ht lx b fv mc md l me mf">&gt;&gt;&gt; text_in_words[3000:3005]</span><span id="0016" class="mb kk ht lx b fv mg md l me mf">[‘ella’, ‘era’, ‘como’, ‘estar’, ‘\n’]</span></pre><h1 id="5014" class="kj kk ht bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">获取词频</h1><p id="f14e" class="pw-post-body-paragraph ix iy ht iz b ja ln jc jd je lo jg jh ji lp jk jl jm lq jo jp jq lr js jt ju hm dt translated">在字符级文本生成器中，你可能有30-50个不同的<em class="ki">尺寸，</em>每个尺寸对应一个不同的字符。在像当前这样的词级生成器中，每个不同的词都有一个维度，这个维度可能有数万个(特别是在像这样肮脏的语料库中)。</p><p id="6031" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">为了避免这种大数量，我们计算每个单词的频率，因此我们可以使用这些信息来过滤不常用的单词，从而降低维数，从而减少训练网络的内存和时间。</p><pre class="ls lt lu lv fq lw lx ly lz aw ma dt"><span id="0c43" class="mb kk ht lx b fv mc md l me mf"><em class="ki"># Calculate word frequency<br/></em>word_freq = {}<br/>for word in text_in_words:<br/>    word_freq[word] = word_freq.get(word, 0) + 1<br/><br/>ignored_words = set()<br/>for k, v in word_freq.items():<br/>    if word_freq[k] &lt; MIN_WORD_FREQUENCY:<br/>        ignored_words.add(k)<br/><br/>words = set(text_in_words)<br/>print('Unique words before ignoring:', len(words))<br/>print('Ignoring words with frequency &lt;', MIN_WORD_FREQUENCY)<br/>words = sorted(set(words) - ignored_words)<br/>print('Unique words after ignoring:', len(words))<br/><br/>word_indices = dict((c, i) for i, c in enumerate(words))<br/>indices_word = dict((i, c) for i, c in enumerate(words))</span></pre><p id="5404" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">变量<em class="ki"> MIN_WORD_FREQUENCY </em>是一个参数，它指示一个单词为了“入选”并出现在最终的单词词典中所需的最小出现次数。</p><p id="0f7d" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">换句话说，如果我们将<em class="ki">的最小词频</em>设置为10，我们将只考虑在语料库中出现10次或更多次的单词。出于调试目的，我们打印原始字典的大小，以及删除不常用单词后的大小。</p><p id="0f34" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">然后我们创建字典来从单词到索引以及从索引到单词进行翻译。这就像<a class="ae jv" href="https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py" rel="noopener ugc nofollow" target="_blank">的keras-team的例子。</a></p><h1 id="554f" class="kj kk ht bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">创建和过滤序列</h1><p id="ead9" class="pw-post-body-paragraph ix iy ht iz b ja ln jc jd je lo jg jh ji lp jk jl jm lq jo jp jq lr js jt ju hm dt translated">如果我们还记得，此时我们有了<em class="ki"> text_in_words </em>，这是一个包含所有逐字语料库的数组。我们需要创建大小为<em class="ki"> SEQUENCE_LEN </em>的序列(另一个可以手动选取的参数)并存储在<em class="ki">句子</em>中，在同一个索引中，存储<em class="ki"> next_words中的下一个单词。</em></p><p id="b7cb" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">但是有一个问题:在<em class="ki"> text_in_words </em>中我们仍然有许多单词被忽略。我们不能直接删除这些词，因为这样会破坏语言，留下不连贯的句子。这就是为什么我们需要验证每个可能的序列+next_word，如果包含至少一个被忽略的单词，它应该被忽略。</p><pre class="ls lt lu lv fq lw lx ly lz aw ma dt"><span id="4e5e" class="mb kk ht lx b fv mc md l me mf"><em class="ki"># cut the text in semi-redundant sequences of SEQUENCE_LEN words<br/></em>STEP = 1<br/>sentences = []<br/>next_words = []<br/>ignored = 0<br/>for i in range(0, len(text_in_words) - SEQUENCE_LEN, STEP):<br/>    <em class="ki"># Only add sequences where no word is in ignored_words<br/>    </em>if len(set(text_in_words[i: i+SEQUENCE_LEN+1]).intersection(ignored_words)) == 0:<br/>        sentences.append(text_in_words[i: i + SEQUENCE_LEN])<br/>        next_words.append(text_in_words[i + SEQUENCE_LEN])<br/>    else:<br/>        ignored = ignored+1<br/>print('Ignored sequences:', ignored)<br/>print('Remaining sequences:', len(sentences))</span></pre><h1 id="718d" class="kj kk ht bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">洗牌和分割训练集</h1><p id="21fa" class="pw-post-body-paragraph ix iy ht iz b ja ln jc jd je lo jg jh ji lp jk jl jm lq jo jp jq lr js jt ju hm dt translated">下一步是标准的，我们改组训练集，并把它分成训练集和测试集(默认为98%-2%)。</p><pre class="ls lt lu lv fq lw lx ly lz aw ma dt"><span id="aa31" class="mb kk ht lx b fv mc md l me mf">sentences, next_words, sentences_test, next_words_test = shuffle_and_split_training_set(sentences, next_words)</span></pre><h1 id="35f4" class="kj kk ht bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">构建模型</h1><p id="5878" class="pw-post-body-paragraph ix iy ht iz b ja ln jc jd je lo jg jh ji lp jk jl jm lq jo jp jq lr js jt ju hm dt translated">现在我们建立RNN模型。在这个例子中，我使用了一组两级堆叠的LSTM双向单元。</p><pre class="ls lt lu lv fq lw lx ly lz aw ma dt"><span id="6c2f" class="mb kk ht lx b fv mc md l me mf">model = Sequential()<br/>model.add(Bidirectional(LSTM(128), input_shape=(SEQUENCE_LEN, len(words))))<br/>if dropout &gt; 0:<br/>    model.add(Dropout(dropout))<br/>model.add(Dense(len(words)))<br/>model.add(Activation('softmax'))</span></pre><p id="f02c" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">在这个架构中有几个任意的决定，我实际上没有时间交叉验证不同大小、不同类型的单元等。</p><p id="39fb" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">这是一些关于架构可能性的讨论:</p><ul class=""><li id="6a3f" class="mw mx ht iz b ja jb je jf ji my jm mz jq na ju nb nc nd ne dt translated">双向与常规LSTM <a class="ae jv" href="https://stackoverflow.com/questions/43035827/whats-the-difference-between-a-bidirectional-lstm-and-an-lstm" rel="noopener ugc nofollow" target="_blank">在这里。</a></li><li id="5424" class="mw mx ht iz b ja nf je ng ji nh jm ni jq nj ju nb nc nd ne dt translated">辍学是一种正则化技术，以防止过度拟合<a class="ae jv" href="https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/" rel="noopener ugc nofollow" target="_blank">在这里</a>。</li><li id="48a5" class="mw mx ht iz b ja nf je ng ji nh jm ni jq nj ju nb nc nd ne dt translated">LSTM单位的数量，我怀疑256可能太多了。<a class="ae jv" href="https://datascience.stackexchange.com/questions/16350/how-many-lstm-cells-should-i-use/18049" rel="noopener ugc nofollow" target="_blank">讨论到这里。</a></li></ul><p id="e5fa" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">根据布尔参数SIMPLE_MODEL，我们创建一层或两层模型。一层普通的LSTM应该足以给出相当好的结果，就像keras团队的例子一样。</p><pre class="ls lt lu lv fq lw lx ly lz aw ma dt"><span id="6c10" class="mb kk ht lx b fv mc md l me mf">model.add(LSTM(128, input_shape=(maxlen, len(chars))))</span></pre><h1 id="371e" class="kj kk ht bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">数据生成程序</h1><p id="376e" class="pw-post-body-paragraph ix iy ht iz b ja ln jc jd je lo jg jh ji lp jk jl jm lq jo jp jq lr js jt ju hm dt translated">没有想太多，在我的第一次尝试中，我想使用与角色级示例相同的<a class="ae jv" href="https://keras.io/models/sequential/" rel="noopener ugc nofollow" target="_blank"> model.fit </a>策略，即一次将整个训练集发送给模型。这很容易也很快让我想到了<strong class="iz hu">的内存不足错误</strong>。</p><p id="9529" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">经过快速分析，我找到了原因。为了将序列矢量化为训练集(x，y ),我们需要这些numpy数组:</p><pre class="ls lt lu lv fq lw lx ly lz aw ma dt"><span id="a24a" class="mb kk ht lx b fv mc md l me mf">x = np.zeros((len(sentences), SEQUENCE_LEN, len(words)), dtype=np.bool)<br/>y = np.zeros((len(sentences), len(words)), dtype=np.bool)</span></pre><p id="40a3" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">如果没有单词过滤，我大概有100万个句子(len(句子)= 1000000)，SEQUENCE_LEN = 10和40000个不同的单词(LEN(单词)=40000)。使用这些数字，<em class="ki"> x </em>的大小为400，000，000，000(！).考虑到numpy中的<a class="ae jv" href="https://stackoverflow.com/questions/5602155/numpy-boolean-array-with-1-bit-entries" rel="noopener ugc nofollow" target="_blank">是1字节</a>，这给了我大约400 GB的内存(！).</p><p id="1ec8" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">因此需要数据生成器。使用数据生成器，你用训练集的<em class="ki">块</em>来填充模型，每一步一个，而不是一次填充所有东西。</p><pre class="ls lt lu lv fq lw lx ly lz aw ma dt"><span id="1288" class="mb kk ht lx b fv mc md l me mf">def generator(sentence_list, next_word_list, batch_size):<br/>    index = 0<br/>    while True:<br/>        x = np.zeros((batch_size, SEQUENCE_LEN, len(words)), dtype=np.bool)<br/>        y = np.zeros((batch_size, len(words)), dtype=np.bool)<br/>        for i in range(batch_size):<br/>            for t, w in enumerate(sentence_list[index]):<br/>                x[i, t, word_indices[w]] = 1<br/>            y[i, word_indices[next_word_list[index]]] = 1<br/><br/>            index = index + 1<br/>            if index == len(sentence_list):<br/>                index = 0<br/>        yield x, y</span></pre><p id="4980" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">生成器函数获取句子和next_words的列表，以及批处理的大小。然后它<a class="ae jv" href="https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do" rel="noopener ugc nofollow" target="_blank">产生</a>两个<em class="ki"> batch_size的numpy数组。</em>我们使用<em class="ki">索引</em>变量来跟踪我们已经返回的例子。当然，当我们到达列表的末尾时，它需要重新初始化为0。这个生成器既可以用于训练，也可以用于评估(只需通过不同的<em class="ki">句子_列表</em>和<em class="ki">下一个_单词_列表</em>)。</p><h1 id="9915" class="kj kk ht bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">完成模型</h1><p id="3af7" class="pw-post-body-paragraph ix iy ht iz b ja ln jc jd je lo jg jh ji lp jk jl jm lq jo jp jq lr js jt ju hm dt translated">函数<strong class="iz hu">样本</strong>和<strong class="iz hu"> on_epoch_end </strong>与keras-team示例中的<a class="ae jv" href="https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py" rel="noopener ugc nofollow" target="_blank">基本相同。然而，在模型编译中，我添加了几个Keras </a><a class="ae jv" href="https://keras.io/callbacks/" rel="noopener ugc nofollow" target="_blank">回调</a>。</p><pre class="ls lt lu lv fq lw lx ly lz aw ma dt"><span id="ec33" class="mb kk ht lx b fv mc md l me mf">file_path = "./checkpoints/LSTM_LYRICS-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-loss{loss:.4f}-acc{acc:.4f}-val_loss{val_loss:.4f}-val_acc{val_acc:.4f}"<strong class="lx hu"> </strong>% (<br/>    len(words),<br/>    SEQUENCE_LEN,<br/>    MIN_WORD_FREQUENCY<br/>)<br/>checkpoint = ModelCheckpoint(file_path, monitor='val_acc', save_best_only=True)<br/>print_callback = LambdaCallback(on_epoch_end=on_epoch_end)<br/>early_stopping = EarlyStopping(monitor='val_acc', patience=5)<br/>callbacks_list = [checkpoint, print_callback, early_stopping]</span></pre><p id="58a4" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">第一个是<strong class="iz hu">模型检查点</strong>以保存每个时期的权重，第二个是<strong class="iz hu">提前停止</strong>以停止训练，如果在5个时期内损失没有增加。</p><h1 id="d272" class="kj kk ht bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">训练模型</h1><p id="7bdc" class="pw-post-body-paragraph ix iy ht iz b ja ln jc jd je lo jg jh ji lp jk jl jm lq jo jp jq lr js jt ju hm dt translated">最后，我们用数据生成器、回调和时期数调用model.fit_generator(而不是model.fit)。我们还发送了另一个带有测试数据的生成器，因此它在每个时期都会得到评估。</p><pre class="ls lt lu lv fq lw lx ly lz aw ma dt"><span id="942e" class="mb kk ht lx b fv mc md l me mf">model.fit_generator(generator(sentences, next_words, BATCH_SIZE),<br/>    steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,<br/>    epochs=100,<br/>    callbacks=callbacks_list,<br/>    validation_data=generator(sentences_test, next_words_test, BATCH_SIZE),              validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)</span></pre><p id="73f4" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">rnn可能很难训练。即使使用相当强大的GPU (GeForce GTX 1070 ti ),使用堆叠的LSTM架构，每个纪元也需要一个多小时。</p><h1 id="125f" class="kj kk ht bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">执行培训</h1><p id="d929" class="pw-post-body-paragraph ix iy ht iz b ja ln jc jd je lo jg jh ji lp jk jl jm lq jo jp jq lr js jt ju hm dt translated">开始训练，需要跑(自然可以用自己的语料库跑)。</p><pre class="ls lt lu lv fq lw lx ly lz aw ma dt"><span id="ae9e" class="mb kk ht lx b fv mc md l me mf">git clone <a class="ae jv" href="https://github.com/enriqueav/lstm_lyrics.git" rel="noopener ugc nofollow" target="_blank">https://github.com/enriqueav/lstm_lyrics.git</a><br/>cd lstm_lyrics<br/>python3 lstm_train.py corpora/corpus_banda.txt examples.txt</span></pre><p id="8bed" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated"><strong class="iz hu">更新:</strong>也可以参考<a class="ae jv" rel="noopener" href="/@monocasero/update-automatic-song-lyrics-creator-with-word-embeddings-e30de94db8d1">这个词嵌入版本</a>。</p><p id="107b" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">之后，脚本将打印关于当前训练集、预处理等的信息。示例语料库(墨西哥“banda”音乐)包含超过一百万个单词中的超过五百万个字符。</p><pre class="ls lt lu lv fq lw lx ly lz aw ma dt"><span id="90cc" class="mb kk ht lx b fv mc md l me mf">Corpus length in characters: 5502159</span><span id="6ddd" class="mb kk ht lx b fv mg md l me mf">Corpus length in words: 1066242</span></pre><p id="ea63" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">最初，语料库中有超过35，000个不同的单词。过滤掉频率小于10 (MIN_WORD_FREQUENCY = 10)的词后，只有6605个。</p><pre class="ls lt lu lv fq lw lx ly lz aw ma dt"><span id="754a" class="mb kk ht lx b fv mc md l me mf">Unique words before ignoring: 36990</span><span id="311f" class="mb kk ht lx b fv mg md l me mf">Ignoring words with frequency &lt; 10</span><span id="27fa" class="mb kk ht lx b fv mg md l me mf">Unique words after ignoring: 6605</span></pre><p id="6c52" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">由于步长为1，最初大约有一百万个不同的序列。然而，由于我们已经忽略了30，000个不常用的单词，我们还需要忽略包含至少一个这些被忽略的单词的序列。经过这一切割，我们得到大约537，000个有效序列。</p><pre class="ls lt lu lv fq lw lx ly lz aw ma dt"><span id="e3d9" class="mb kk ht lx b fv mc md l me mf">Ignored sequences: 529230</span><span id="807d" class="mb kk ht lx b fv mg md l me mf">Remaining sequences: 537002</span><span id="68ee" class="mb kk ht lx b fv mg md l me mf">Shuffling sentences</span><span id="9345" class="mb kk ht lx b fv mg md l me mf">Shuffling finished</span></pre><p id="dec5" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">最后，我们在98%训练2%测试集中拆分这些混洗的537，000。</p><pre class="ls lt lu lv fq lw lx ly lz aw ma dt"><span id="8f84" class="mb kk ht lx b fv mc md l me mf">Size of training set = 526261</span><span id="a983" class="mb kk ht lx b fv mg md l me mf">Size of test set = 10741</span></pre><p id="ac1f" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">然后我们建立模型并开始训练</p><pre class="ls lt lu lv fq lw lx ly lz aw ma dt"><span id="e220" class="mb kk ht lx b fv mc md l me mf">Build model...</span><span id="2875" class="mb kk ht lx b fv mg md l me mf">Epoch 1/100...</span></pre><h1 id="ac6b" class="kj kk ht bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">监控结果</h1><p id="ae7a" class="pw-post-body-paragraph ix iy ht iz b ja ln jc jd je lo jg jh ji lp jk jl jm lq jo jp jq lr js jt ju hm dt translated">就像字符级生成器一样，每个时期几个例句将被写入<em class="ki">示例</em>文本文件，种子是从原始语料库中随机选取的。</p><p id="d338" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated">到第20个纪元时，训练集的准确率将在90%左右，但是在测试集上，我们不会看到这么高的数字，这是正常的。请记住，我们的目标不是获得人类水平的准确性，而是学习“风格”并生成一些连贯的歌词。</p><h1 id="722c" class="kj kk ht bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">例子</h1><p id="c2af" class="pw-post-body-paragraph ix iy ht iz b ja ln jc jd je lo jg jh ji lp jk jl jm lq jo jp jq lr js jt ju hm dt translated">不幸的是，如果你懂一些西班牙语，特别是如果你熟悉墨西哥班达风格，这将更有意义。</p><blockquote class="lh li lj"><p id="cf5c" class="ix iy ki iz b ja jb jc jd je jf jg jh lk jj jk jl ll jn jo jp lm jr js jt ju hm dt translated"><strong class="iz hu">生成种子:“米德porque estoy compometido la que se que da la quiero y la que se”</strong></p><p id="f88a" class="ix iy ki iz b ja jb jc jd je jf jg jh lk jj jk jl ll jn jo jp lm jr js jt ju hm dt translated">米德说，他是一个勇敢的人，也是一个与世界和平共处的人，米德是一个热爱自己的人，因为他是黑手党的头目，他把我带到了世界和平的地方</p><p id="3b52" class="ix iy ki iz b ja jb jc jd je jf jg jh lk jj jk jl ll jn jo jp lm jr js jt ju hm dt translated"><strong class="iz hu">用种子生成:“你是哈特最后一个孩子吗？”</strong></p><p id="c1c8" class="ix iy ki iz b ja jb jc jd je jf jg jh lk jj jk jl ll jn jo jp lm jr js jt ju hm dt translated">上次去哈特的时候，你们一起看了一会儿电影，看了一会儿电影，看了一会儿电影，有一个女人给我打电话，说她不喜欢我，但是她没有给我打电话</p><p id="5ca0" class="ix iy ki iz b ja jb jc jd je jf jg jh lk jj jk jl ll jn jo jp lm jr js jt ju hm dt translated"><strong class="iz hu">用种子生成:" mis brazos me muero de ganas por volvert a besar en mis noches despierto gritando "</strong></p><p id="3f41" class="ix iy ki iz b ja jb jc jd je jf jg jh lk jj jk jl ll jn jo jp lm jr js jt ju hm dt translated">我的母亲是一个很大的孩子，因为我知道你的名字，我的母亲是另一个生活在美丽的地方的男人，因为我的母亲是一个很年轻的女人，所以她没有去过任何一个地方</p></blockquote><h1 id="bc12" class="kj kk ht bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dt translated">后续步骤</h1><p id="0253" class="pw-post-body-paragraph ix iy ht iz b ja ln jc jd je lo jg jh ji lp jk jl jm lq jo jp jq lr js jt ju hm dt translated">目前，我只包括项目的培训部分。我将对此进行扩展(或创建另一个故事),以便实际使用训练模型从种子生成歌词。我还会上传已经训练好的网络的权重。</p><p id="322e" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated"><strong class="iz hu">2018年6月15日更新:</strong>更改为包括使用新行作为单独的单词，以及在fit_generator上发送验证数据。</p><p id="49d5" class="pw-post-body-paragraph ix iy ht iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hm dt translated"><strong class="iz hu">2019 . 1 . 21更新:</strong>添加链接到<a class="ae jv" rel="noopener" href="/@monocasero/update-automatic-song-lyrics-creator-with-word-embeddings-e30de94db8d1">故事的第二部分</a>。</p><blockquote class="nk"><p id="6a87" class="nl nm ht bd nn no np nq nr ns nt ju ek translated">加入Coinmonks <a class="ae jv" href="https://t.me/coincodecap" rel="noopener ugc nofollow" target="_blank">电报频道</a>和<a class="ae jv" href="https://www.youtube.com/c/coinmonks/videos" rel="noopener ugc nofollow" target="_blank"> Youtube频道</a>获取每日<a class="ae jv" href="http://coincodecap.com/" rel="noopener ugc nofollow" target="_blank">加密新闻</a></p></blockquote><h2 id="15d6" class="mb kk ht bd kl nu nv nw kp nx ny nz kt ji oa ob kx jm oc od lb jq oe of lf og dt translated">另外，阅读</h2><ul class=""><li id="20fb" class="mw mx ht iz b ja ln je lo ji oh jm oi jq oj ju nb nc nd ne dt translated"><a class="ae jv" href="http://Top 4 Telegram Channels for Crypto Traders" rel="noopener ugc nofollow" target="_blank">密码电报信号</a> | <a class="ae jv" rel="noopener" href="/coinmonks/crypto-trading-bot-c2ffce8acb2a">密码交易机器人</a></li><li id="14e6" class="mw mx ht iz b ja nf je ng ji nh jm ni jq nj ju nb nc nd ne dt translated"><a class="ae jv" rel="noopener" href="/coinmonks/top-10-crypto-copy-trading-platforms-for-beginners-d0c37c7d698c">复制交易</a> | <a class="ae jv" rel="noopener" href="/coinmonks/crypto-tax-software-ed4b4810e338">加密税务软件</a></li><li id="723e" class="mw mx ht iz b ja nf je ng ji nh jm ni jq nj ju nb nc nd ne dt translated"><a class="ae jv" href="https://coincodecap.com/grid-trading" rel="noopener ugc nofollow" target="_blank">网格交易</a> | <a class="ae jv" rel="noopener" href="/coinmonks/the-best-cryptocurrency-hardware-wallets-of-2020-e28b1c124069">加密硬件钱包</a></li><li id="f33b" class="mw mx ht iz b ja nf je ng ji nh jm ni jq nj ju nb nc nd ne dt translated"><a class="ae jv" rel="noopener" href="/coinmonks/crypto-exchange-dd2f9d6f3769">最佳加密交易所</a> | <a class="ae jv" rel="noopener" href="/coinmonks/bitcoin-exchange-in-india-7f1fe79715c9">印度最佳加密交易所</a></li><li id="47a8" class="mw mx ht iz b ja nf je ng ji nh jm ni jq nj ju nb nc nd ne dt translated"><a class="ae jv" rel="noopener" href="/coinmonks/best-crypto-apis-for-developers-5efe3a597a9f">面向开发人员的最佳加密API</a></li><li id="b359" class="mw mx ht iz b ja nf je ng ji nh jm ni jq nj ju nb nc nd ne dt translated">最佳<a class="ae jv" rel="noopener" href="/coinmonks/top-5-crypto-lending-platforms-in-2020-that-you-need-to-know-a1b675cec3fa">密码借贷平台</a></li><li id="9487" class="mw mx ht iz b ja nf je ng ji nh jm ni jq nj ju nb nc nd ne dt translated"><a class="ae jv" rel="noopener" href="/coinmonks/leveraged-token-3f5257808b22">杠杆代币的终极指南</a></li><li id="95d1" class="mw mx ht iz b ja nf je ng ji nh jm ni jq nj ju nb nc nd ne dt translated"><a class="ae jv" href="https://coincodecap.com/best-vpns-for-crypto-trading" rel="noopener ugc nofollow" target="_blank">加密交易的最佳VPNs】</a></li><li id="918f" class="mw mx ht iz b ja nf je ng ji nh jm ni jq nj ju nb nc nd ne dt translated"><a class="ae jv" href="https://coincodecap.com/blockchain-analytics" rel="noopener ugc nofollow" target="_blank">最佳加密分析或链上数据</a> | <a class="ae jv" href="https://coincodecap.com/bexplus-review" rel="noopener ugc nofollow" target="_blank"> Bexplus评论</a></li><li id="51af" class="mw mx ht iz b ja nf je ng ji nh jm ni jq nj ju nb nc nd ne dt translated"><a class="ae jv" href="https://coincodecap.com/nft-marketplaces" rel="noopener ugc nofollow" target="_blank">NFT十大市场造币集锦</a></li><li id="f72f" class="mw mx ht iz b ja nf je ng ji nh jm ni jq nj ju nb nc nd ne dt translated"><a class="ae jv" href="https://coincodecap.com/ascendex-staking" rel="noopener ugc nofollow" target="_blank">AscendEx Staking</a>|<a class="ae jv" href="https://coincodecap.com/bot-ocean-review" rel="noopener ugc nofollow" target="_blank">Bot Ocean Review</a>|<a class="ae jv" href="https://coincodecap.com/bitcoin-wallets-india" rel="noopener ugc nofollow" target="_blank">最佳比特币钱包</a></li><li id="4ec1" class="mw mx ht iz b ja nf je ng ji nh jm ni jq nj ju nb nc nd ne dt translated"><a class="ae jv" href="https://coincodecap.com/bitget-review" rel="noopener ugc nofollow" target="_blank"> Bitget回顾</a>|<a class="ae jv" href="https://coincodecap.com/gemini-vs-blockfi" rel="noopener ugc nofollow" target="_blank">Gemini vs block fi</a>|<a class="ae jv" href="https://coincodecap.com/okex-futures-trading" rel="noopener ugc nofollow" target="_blank">OKEx期货交易</a></li></ul></div></div>    
</body>
</html>