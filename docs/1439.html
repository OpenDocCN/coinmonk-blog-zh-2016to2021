<html>
<head>
<title>Review: PReLU-Net — The First to Surpass Human-Level Performance in ILSVRC 2015 (Image Classification)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾:PReLU-Net——第一个在ILSVRC 2015(图像分类)中超越人类水平的性能</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/review-prelu-net-the-first-to-surpass-human-level-performance-in-ilsvrc-2015-image-f619dddd5617?source=collection_archive---------0-----------------------#2018-09-03">https://medium.com/coinmonks/review-prelu-net-the-first-to-surpass-human-level-performance-in-ilsvrc-2015-image-f619dddd5617?source=collection_archive---------0-----------------------#2018-09-03</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><p id="b4b6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt jo translated"><span class="l jp jq jr bm js jt ju jv jw di">在</span>这个故事中，<strong class="is hu">PReLU-Net</strong>【1】进行了回顾。<strong class="is hu">参数整流线性单元(PReLU) </strong>是对传统整流单元(ReLU)的推广。这是深度学习方法，在ILSVRC  (ImageNet大规模视觉识别挑战)<strong class="is hu">图像分类中<strong class="is hu">首次超过人类水平的性能。</strong>此外，<strong class="is hu">提出了整流器</strong>的更好的权重初始化，这有助于直接从零开始训练的深度模型(30层)<strong class="is hu"/>的收敛。</strong></p><p id="23c6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">最终，PReLU-Net在测试集上获得了4.94%的top-5错误率，优于5.1%的人类水平性能，也优于6.66%的GoogLeNet！！！</strong></p><p id="43ac" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">这是2015年ICCV的论文，在我写这篇文章的时候，它被引用了大约3000次。(<a class="jx jy gr" href="https://medium.com/u/aff72a0c1243?source=post_page-----f619dddd5617--------------------------------" rel="noopener" target="_blank">曾植和</a> @中)</p></div><div class="ab cl jz ka hb kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hm hn ho hp hq"><h1 id="9763" class="kg kh ht bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated">资料组</h1><p id="d063" class="pw-post-body-paragraph iq ir ht is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hm dt translated"><strong class="is hu">分类</strong>:超过1500万张带标签的高分辨率图像，约22000个类别。ILSVRC使用ImageNet的一个子集，在<strong class="is hu"> 1000个类别</strong>的每个类别中包含大约1000幅图像。总共大约有<strong class="is hu"> 1.3M/50k/100k图像</strong>用于<strong class="is hu">训练/验证/测试集。</strong></p></div><div class="ab cl jz ka hb kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hm hn ho hp hq"><h1 id="67a9" class="kg kh ht bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated">涵盖哪些内容</h1><ol class=""><li id="b21e" class="lj lk ht is b it le ix lf jb ll jf lm jj ln jn lo lp lq lr dt translated"><strong class="is hu">参数整流线性单元(PReLU) </strong></li><li id="44c1" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated"><strong class="is hu">整流器更好的权重初始化</strong></li><li id="a6dc" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated"><strong class="is hu"> 22层深度学习模型</strong></li><li id="04b0" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated"><strong class="is hu">与最先进方法的比较</strong></li><li id="33e8" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated"><strong class="is hu">利用快速R-CNN进行物体检测</strong></li></ol></div><div class="ab cl jz ka hb kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hm hn ho hp hq"><h1 id="4b53" class="kg kh ht bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated"><strong class="ak"> 1。参数校正线性单元(PReLU) </strong></h1><p id="a8c5" class="pw-post-body-paragraph iq ir ht is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hm dt translated">在AlexNet [2]中，ReLU建议如下，其中只有正值会通过ReLU激活函数，而所有负值都设置为零。并且由于在1处不饱和，ReLU以快得多的训练速度胜过Tanh。</p><figure class="ly lz ma mb fq mc fe ff paragraph-image"><div class="fe ff lx"><img src="../Images/b33e0cc8c4311fa69c358935b31217a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*avswi5XZlGlYx4wqdzo_Xg.png"/></div><figcaption class="mf mg fg fe ff mh mi bd b be z ek"><strong class="bd mj">PReLU</strong></figcaption></figure><p id="397c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">PReLU建议对负值应该有惩罚，并且应该是参数化的。</p><p id="b40c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">注意<strong class="is hu">当<em class="mk"> a </em> = 0时，为ReLU </strong>。<br/> <strong class="is hu">当<em class="mk"> a </em> = 0.01时，为泄漏ReLU </strong>。<br/> <strong class="is hu">现在<em class="mk"> a </em>的值可以学习</strong>，因此成为<strong class="is hu">广义ReLU </strong>。</p><p id="62a5" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">在反向传播期间，我们可以估计梯度:</p><figure class="ly lz ma mb fq mc fe ff paragraph-image"><div class="fe ff ml"><img src="../Images/b35535359c2246766d86aa0ac213de94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*A3RcPH83K5ZHpqmhXJtmYA.png"/></div><figcaption class="mf mg fg fe ff mh mi bd b be z ek"><strong class="bd mj">Backpropagation, gradient from deep layer (Left), gradient of the activation (Right)</strong></figcaption></figure><p id="9b7b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">我们可以从深层(左)估算梯度，活化的<strong class="is hu">梯度</strong>(右)。我们可以看到，它是特征图(通道方向)所有位置的<strong class="is hu">之和。如果是<strong class="is hu">通道共享</strong>变体，则是该层所有通道的<strong class="is hu">总和。</strong>没有重量衰减应用于<em class="mk"> a </em>。</strong></p><figure class="ly lz ma mb fq mc fe ff paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="fe ff mm"><img src="../Images/3c1504bd96fdcb6b89687b55fabf48d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ObETCuzwzVAe4WVnbKi5CA.png"/></div></div><figcaption class="mf mg fg fe ff mh mi bd b be z ek"><strong class="bd mj">The average value of <em class="mr">a</em> over all channels for each layer</strong></figcaption></figure><p id="c8d9" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">观察到两个有趣的现象:</p><ol class=""><li id="0d5c" class="lj lk ht is b it iu ix iy jb ms jf mt jj mu jn lo lp lq lr dt translated"><strong class="is hu">首先，第一conv层(conv1)的系数(0.681和0.596)明显大于0。</strong>由于conv1的滤波器大多是类似Gabor的滤波器，如边缘或纹理检测器，<strong class="is hu">学习结果显示滤波器的正负响应都受到重视</strong>。</li><li id="e69e" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated">对于<strong class="is hu">通道方式</strong>版本，较深的conv层通常具有<strong class="is hu">较小的系数</strong>。随着深度的增加，激活逐渐变得“更加非线性”。换句话说，学习的模型倾向于在早期阶段保留更多的信息，并在更深的阶段变得更具辨别能力。</li></ol></div><div class="ab cl jz ka hb kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hm hn ho hp hq"><h1 id="5928" class="kg kh ht bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated"><strong class="ak"> 2。整流器更好的重量初始化</strong></h1><p id="c0e3" class="pw-post-body-paragraph iq ir ht is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hm dt translated">良好的权重初始化对于<strong class="is hu">不要让网络以指数方式减少或放大输入信号</strong>至关重要。因为权重初始化取决于均值为0的高斯分布，而不同的方差取决于Xavier之类的算法。因此，通过考虑输入和输出网络大小，建议了更好的权重初始化。</p><p id="cf9e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">将L层放在一起，方差为(左):</p><figure class="ly lz ma mb fq mc fe ff paragraph-image"><div class="fe ff mv"><img src="../Images/93a4f50648df2a180afc97df966dfeb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*zCJ3V8-x9Q0ZrV5NS7HE6A.png"/></div><figcaption class="mf mg fg fe ff mh mi bd b be z ek"><strong class="bd mj">Variance of L layers (Left), and sufficient condition (right)</strong></figcaption></figure><p id="9050" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">如果满足右边的充分条件，网络可以变得稳定。因此，最后，方差应该是2/n_l，其中n_l是第l层中的连接数。</p><p id="e921" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">(在反向传播情况下也有证明。有趣的是，它也提出了同样的充分条件。但我只是不想在这里表现出来。)</p><figure class="ly lz ma mb fq mc fe ff paragraph-image"><div class="fe ff mw"><img src="../Images/3ec9badd31b34c48f1ed3e275fb6a990.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*nLbDMT48zLCMldZsgL50kg.png"/></div><figcaption class="mf mg fg fe ff mh mi bd b be z ek"><strong class="bd mj">Red (Ours) and Blue (Xavier), 22-layer (Left) and 30-layer (Right)</strong></figcaption></figure><p id="b66b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">如上图所示，<strong class="is hu">建议权重初始化收敛更快。</strong>而泽维尔甚至在从零开始训练的时候都无法收敛到右边更深的一层。</p></div><div class="ab cl jz ka hb kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hm hn ho hp hq"><h1 id="58ca" class="kg kh ht bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated"><strong class="ak"> 3。22层深度学习模型</strong></h1><figure class="ly lz ma mb fq mc fe ff paragraph-image"><div class="fe ff mx"><img src="../Images/64f31d6ea2c658f51a059bd35c0a73f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*SfMy4iwnvLucysxwYTcetQ.png"/></div><figcaption class="mf mg fg fe ff mh mi bd b be z ek"><strong class="bd mj">PReLU-Net: Model A, B, C</strong></figcaption></figure><p id="45be" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu"> SPP层:4级SPP net[3–4]{ 7×7，3×3，2×2，1×1} </strong></p><p id="1af0" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">A型</strong>:比VGG-19 [5]效果更好的一种型号</p><p id="1ba6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">B型</strong>:比A型深的型号</p><p id="5315" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">型号C </strong>:比型号B更宽的型号(更多过滤器)</p><figure class="ly lz ma mb fq mc fe ff paragraph-image"><div class="fe ff my"><img src="../Images/0ac86bb3f981dacfe4185145231349b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*EMCjfsHkvh3IWD_8ucoLvw.png"/></div><figcaption class="mf mg fg fe ff mh mi bd b be z ek"><strong class="bd mj">Model A using PReLU is better than the one using ReLU</strong></figcaption></figure><figure class="ly lz ma mb fq mc fe ff paragraph-image"><div class="fe ff mz"><img src="../Images/025b5565293b3cdc3a4635378069af52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*1r1zy_nimVo9UbslywbEjg.png"/></div><figcaption class="mf mg fg fe ff mh mi bd b be z ek"><strong class="bd mj">Model A: PReLU converges faster</strong></figcaption></figure></div><div class="ab cl jz ka hb kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hm hn ho hp hq"><h1 id="4d9b" class="kg kh ht bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated"><strong class="ak"> 4。与最先进方法的比较</strong></h1><figure class="ly lz ma mb fq mc fe ff paragraph-image"><div class="fe ff na"><img src="../Images/a2388f11aae46720a690e2a5147f9387.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*q2ttwkgZFzrn_Vp2MDcCrg.png"/></div><figcaption class="mf mg fg fe ff mh mi bd b be z ek"><strong class="bd mj">Single model, 10-view</strong></figcaption></figure><p id="865a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">仅使用单个模型和10个视图，模型C具有7.38%的错误率。</p><figure class="ly lz ma mb fq mc fe ff paragraph-image"><div class="fe ff nb"><img src="../Images/c3eaaeb014ba380597eb893177f057d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*hAmg7Nzp1GK_JHEDsTpGlw.png"/></div><figcaption class="mf mg fg fe ff mh mi bd b be z ek"><strong class="bd mj">Single model, Multi-view, Multi-scale</strong></figcaption></figure><p id="66bb" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated"><strong class="is hu">多视角多尺度，模型C误差率5.71%。</strong>这一结果甚至优于SPPNet[3–4]、VGGNet [5]和GoogLeNet [6]的多模型。</p><figure class="ly lz ma mb fq mc fe ff paragraph-image"><div class="fe ff nc"><img src="../Images/37adc8a906cf569229e5c7d0acea5a9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*Ru1wdrNsFpBfYUaxOYHAnw.png"/></div><figcaption class="mf mg fg fe ff mh mi bd b be z ek"><strong class="bd mj">Multi-model, Multi-view, Multi-scale</strong></figcaption></figure><p id="b659" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">用多模型，即<strong class="is hu"> 6个模型预网络，得到4.94%的错误率</strong>。<br/>相对于GoogLeNet，这个数字提高了26%!！！</p></div><div class="ab cl jz ka hb kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hm hn ho hp hq"><h1 id="d802" class="kg kh ht bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated"><strong class="ak"> 5。使用快速R-CNN进行目标检测</strong></h1><p id="ca65" class="pw-post-body-paragraph iq ir ht is b it le iv iw ix lf iz ja jb lg jd je jf lh jh ji jj li jl jm jn hm dt translated"><strong class="is hu"> PReLU-Net使用快速R-CNN [7]实现</strong>在PASCAL VOC 2007数据集中进行对象检测。</p><figure class="ly lz ma mb fq mc fe ff paragraph-image"><div class="fe ff nd"><img src="../Images/94807686c4c6409092e40afffc5296f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*o5zaqX_6oNmb7Y3bzMFo0w.png"/></div><figcaption class="mf mg fg fe ff mh mi bd b be z ek"><strong class="bd mj">Model C + PReLU-Net has the best mAP result</strong></figcaption></figure><p id="a855" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">通过imagenet预训练模型和在VOC 2007数据集上的微调，模型C获得了比VGG-16更好的结果。</p></div><div class="ab cl jz ka hb kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hm hn ho hp hq"><p id="aafa" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hm dt translated">由于深度学习网络的训练需要大量的时间，并且为了公平的比较或消融研究，实际上，有许多知识或技术是从大量的现有技术中构建的。如果感兴趣，请访问我对AlexNet、VGGNet、SPPNet和GoogLeNet等其他网络的评论(底部的链接)。:)</p></div><div class="ab cl jz ka hb kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hm hn ho hp hq"><h1 id="3805" class="kg kh ht bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dt translated">参考</h1><ol class=""><li id="b2eb" class="lj lk ht is b it le ix lf jb ll jf lm jj ln jn lo lp lq lr dt translated">【2015 ICCV】【PReLU-Net】<br/><a class="ae ne" href="https://arxiv.org/pdf/1502.01852" rel="noopener ugc nofollow" target="_blank">深入挖掘整流器:在ImageNet分类上超越人类水平的表现</a></li><li id="0482" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated">【2012 NIPS】【Alex net】<br/><a class="ae ne" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank">使用深度卷积神经网络的ImageNet分类</a></li><li id="1ad6" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated">【2014 ECCV】【sp pnet】<br/><a class="ae ne" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.699.8052&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">用于视觉识别的深度卷积网络中的空间金字塔池</a></li><li id="41e9" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated">【2015 TPAMI】【SPPNet】<br/><a class="ae ne" href="https://arxiv.org/pdf/1406.4729.pdf" rel="noopener ugc nofollow" target="_blank">用于视觉识别的深度卷积网络中的空间金字塔池</a></li><li id="52fb" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated">【2015 ICLR】【VGGNet】<br/><a class="ae ne" href="https://arxiv.org/pdf/1409.1556" rel="noopener ugc nofollow" target="_blank">用于大规模图像识别的极深度卷积网络</a></li><li id="2389" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated">【2015】【CVPR】【谷歌网】<br/> <a class="ae ne" href="https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf" rel="noopener ugc nofollow" target="_blank">用回旋更深入</a></li><li id="821c" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated">【2015 ICCV】【快速R-CNN】<br/><a class="ae ne" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf" rel="noopener ugc nofollow" target="_blank">快速R-CNN </a></li></ol><h1 id="a331" class="kg kh ht bd ki kj nf kl km kn ng kp kq kr nh kt ku kv ni kx ky kz nj lb lc ld dt translated">我的评论</h1><ol class=""><li id="9bca" class="lj lk ht is b it le ix lf jb ll jf lm jj ln jn lo lp lq lr dt translated"><a class="ae ne" rel="noopener" href="/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160">回顾AlexNet，CaffeNet—ils vrc 2012获奖者(图像分类)</a></li><li id="5813" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated"><a class="ae ne" rel="noopener" href="/coinmonks/review-sppnet-1st-runner-up-object-detection-2nd-runner-up-image-classification-in-ilsvrc-906da3753679">回顾:spp net—ils vrc 2014亚军(物体检测)，季军(图像分类)</a></li><li id="3f1d" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated"><a class="ae ne" rel="noopener" href="/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11">点评:VGGNet—ILSVRC 2014亚军(图像分类)，冠军(本地化)</a></li><li id="a82c" class="lj lk ht is b it ls ix lt jb lu jf lv jj lw jn lo lp lq lr dt translated"><a class="ae ne" rel="noopener" href="/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7">点评:谷歌网(Inception v1)——2014年ILSVRC(图像分类)获奖者</a></li></ol></div></div>    
</body>
</html>