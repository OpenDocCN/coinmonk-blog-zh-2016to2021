<html>
<head>
<title>Implementing an Artificial Neural Network in Pure Java (No external dependencies).</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用纯Java实现人工神经网络(无外部依赖性)。</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/implementing-an-artificial-neural-network-in-pure-java-no-external-dependencies-975749a38114?source=collection_archive---------0-----------------------#2018-08-17">https://medium.com/coinmonks/implementing-an-artificial-neural-network-in-pure-java-no-external-dependencies-975749a38114?source=collection_archive---------0-----------------------#2018-08-17</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><figure class="fi fk ir is it iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff iq"><img src="../Images/02518797ff3871b4f38f76612702dcbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OBfrl1JBomgU7nPjk9FVIg.png"/></div></div><figcaption class="jb jc fg fe ff jd je bd b be z ek">Visualization of the training loss with JavaFx</figcaption></figure><p id="cf41" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">深度学习框架过度简化了实现神经网络的过程，有时很容易陷入抽象学习过程的陷阱，认为你可以简单地将任意层堆叠在一起，它会自动处理一切[1]。通过从零开始实现核心概念，如反向传播算法(用于NNs、CNN和r NNs)，在机器学习(ML)方面拥有坚实的基础是很重要的。花点时间去理解它的派生，试着自己从头开始推导它，并从头用代码实现它，看看你是否能让它工作。你获得的知识将会坚持下去，并且独立于你以后决定学习的任何框架。在我的学习过程中，我认为出于求知欲，了解幕后发生的事情是值得的。在本文中，我将向您展示我用纯java实现的两层神经网络。</p><p id="c354" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated"><a class="ae kd" rel="noopener" href="/@jeraldydeus/going-deep-from-scratch-3809f5890c5e">这是我的第二篇帖子</a></p><blockquote class="ke kf kg"><p id="0d00" class="jf jg kh jh b ji jj jk jl jm jn jo jp ki jr js jt kj jv jw jx kk jz ka kb kc hm dt translated">如果<!-- -->你在赶时间<a class="ae kd" href="https://gist.github.com/Jeraldy/1aa6ae6fefa46b7a9cc02b6573cfeefe" rel="noopener ugc nofollow" target="_blank">这里</a>是完整的代码。它的python/numpy版本可以在<a class="ae kd" href="https://gist.github.com/Jeraldy/b5bb83ed10df20834c75bab6b963bebd" rel="noopener ugc nofollow" target="_blank">这里</a>找到</p></blockquote><p id="9f4e" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated"><strong class="jh hu">网络架构</strong></p><p id="4f38" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">下面展示的是一个我们将要用java实现的两层前馈神经网络。我们将使用以下网络架构，但是所有概念都可以扩展到任意数量的层和节点。</p><figure class="km kn ko kp fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff kl"><img src="../Images/7233ec0bb58b52631da75fbdbd306bcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xPTadyO8pRYUZ67VF7oWjg.jpeg"/></div></div><figcaption class="jb jc fg fe ff jd je bd b be z ek">Two-layer Neural Network</figcaption></figure><p id="cd4c" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">我们将教我们的神经网络识别的模式是异或运算。运算<strong class="jh hu"> y= x1 XOR x2 </strong>的<strong class="jh hu"> XOR </strong>运算符真值表如下所示</p><figure class="km kn ko kp fq iu fe ff paragraph-image"><div class="fe ff kq"><img src="../Images/0da27bd92115f51250d9a7ece14cfa3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/format:webp/1*3_urO1TZGlxHJA6zBnScgw.png"/></div><figcaption class="jb jc fg fe ff jd je bd b be z ek">XOR table</figcaption></figure><p id="2077" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated"><strong class="jh hu">一些背景数学</strong></p><p id="1972" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">以下是上述神经网络体系结构的正向推进方程[2]。较高的索引表示层，较低的索引表示节点的索引。</p><p id="0f27" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated"><strong class="jh hu"> <em class="kh">第一部分:正向传播方程</em> </strong></p><figure class="km kn ko kp fq iu fe ff paragraph-image"><div class="fe ff kr"><img src="../Images/9d7bc39b1cb1ff55a38a1a60e79d1cec.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*AJb3dVNFCiaUlKpovq77OA.png"/></div></figure><p id="7b93" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated"><strong class="jh hu">矢量化</strong></p><p id="1f47" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">对于大多数需要在一长串元素上迭代的任务，我们都使用过for循环。我敢肯定，几乎所有正在阅读这篇文章的人，都是在高中或大学时使用for循环编写了他们的第一个矩阵或向量乘法代码。For-loop长期稳定地服务于编程社区。然而，它带来了一些负担，并且在处理大型数据集(在这个大数据时代有数百万条记录)时，执行速度通常很慢[3]。</p><p id="5ef7" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">让我们对方程进行矢量化。结合计算隐藏层的节点(先忽略激活函数，然后再考虑它们)。</p><figure class="km kn ko kp fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff ks"><img src="../Images/f7712921de8d30085aab1be9bd4422c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Am5SoEZoIF1dwqEysyszQ.png"/></div></div></figure><figure class="km kn ko kp fq iu fe ff paragraph-image"><div class="fe ff kt"><img src="../Images/f63e89ea26b9ef03a6cce731d6b2623a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*0EcCcMZZ_FYB121aBYlX8w.png"/></div></figure><p id="f5f0" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated"><strong class="jh hu">需要注意的是</strong></p><p id="8477" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">1.增加节点的数量会增加权重矩阵的行数。</p><p id="1ec9" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">2.增加特征的数量将增加矩阵的列数。</p><p id="fe78" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated"><strong class="jh hu"> <em class="kh">我这里是指</em> </strong></p><p id="623a" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">例如，假设我们在隐藏层中添加了另一个节点。矩阵方程向下生长。</p><figure class="km kn ko kp fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff ku"><img src="../Images/dc9f815cfe9e8bebce98691196d3ce9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dZU2Ilxhf8SyTFFYleU9Pg.png"/></div></div></figure><p id="2337" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated"><strong class="jh hu">激活功能</strong></p><p id="2901" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">我们需要激活函数来学习数据输入和目标输出之间的非线性复杂函数映射。从上一节，我只是忽略了激活函数方程，以便于证明。我们将使用乙状结肠激活功能。</p><figure class="km kn ko kp fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff kv"><img src="../Images/b91527b0c6d788139421efc519120641.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*DABZIo4YwQbdu_xLAGYg7g.png"/></div></div></figure><p id="109a" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated"><strong class="jh hu"> <em class="kh">第二部分:反向传播方程</em> </strong></p><p id="4510" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">为了直观地了解back prop是如何工作的，我将使用下图来说明梯度计算，梯度下降将使用该计算来更新可学习的参数w和b。为了简单起见，我将使用单层神经网络(逻辑回归)。该思想可推广到N层神经网络[2]。</p><figure class="km kn ko kp fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff kw"><img src="../Images/1eb61ed25d37d84c670d3476df2cd345.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jhRhUfpZ_7RlP2QBeLFi8g.png"/></div></div></figure><p id="8313" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">我们将使用交叉熵损失来计算成本</p><figure class="km kn ko kp fq iu fe ff paragraph-image"><div class="fe ff kx"><img src="../Images/1b44dd07a119696b03f0e8873650f1d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OTAuc1c_knb0UJnbTslayQ.png"/></div></figure><p id="b2ca" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">计算损失w.r.t对权重的(<strong class="jh hu"> dw) </strong>导数。这可以通过使用如下所示的链式法则来完成。</p><figure class="km kn ko kp fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff ky"><img src="../Images/d998d9bb023c8155274e0204b087f7a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Y4TjEmD7j3bViNgXGP4NQ.png"/></div></div></figure><p id="27f9" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">计算损失w.r.t对偏差的(<strong class="jh hu"> db) </strong>导数。</p><figure class="km kn ko kp fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff kz"><img src="../Images/09b57a6a86edf4a9dbd1ea7143c10a89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NMFfFg8JsPAaxWEZSJZiuQ.png"/></div></div></figure><p id="29a5" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated"><strong class="jh hu">更新方程式</strong></p><p id="1789" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">我们将使用梯度下降执行每个层的参数更新如下。</p><figure class="km kn ko kp fq iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff la"><img src="../Images/07bf3fb400dd5300c90111b1e0faf4b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m1zkxo767ObARHpcKFRyNA.png"/></div></div></figure><p id="6c30" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated"><strong class="jh hu">完整Java代码</strong></p><p id="e484" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">理解上述概念是理解该代码如何工作的关键部分。</p><figure class="km kn ko kp fq iu"><div class="bz el l di"><div class="lb lc l"/></div></figure><p id="a42f" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated"><a class="ae kd" href="https://gist.github.com/Jeraldy/7d4262db0536d27906b1e397662512bc" rel="noopener ugc nofollow" target="_blank">np.java</a>包含了所有的矩阵运算</p><p id="83a8" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated"><strong class="jh hu">训练结果</strong></p><p id="6c2c" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">以下是对神经网络进行4000次迭代训练后的结果。我们可以清楚地看到(预测= [[0.01212，0.9864，0.986300，0.01569]])，我们的网络在尝试模拟异或运算方面做得很好。正如我们所看到的，内部值被推到1，而外部值被推到0。</p><p id="eafc" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">========= <br/>费用= 0.1257569282040295 <br/>预测= [[0.15935，0.8900528，0.88589，0.0877284694]] <br/> <br/>。</p><p id="5d69" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt">.</p><p id="c18e" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">。<br/>费用= 0.015787269324306925 <br/>预测= [[0.013838，0.984561，0.9844246，0.0177971]]<br/>= = = = = = = = = = = = = = = = = = = = = = = =<br/>费用= 0.01386971354598404<br/>预测= [[0.01212，0</p><p id="86a3" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated"><strong class="jh hu">结论</strong></p><p id="5cb3" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">这不是一个有效的神经网络的实现，但我的意图是传达一个直观的理解机器学习的概念，并有能力沟通他们成为代码。</p><p id="9561" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">你觉得这篇文章有帮助吗？你发现什么错误了吗？(<em class="kh">有可能，因为这是我的第一篇文章，而英语不是我的母语</em>)。有意见/评论？把它们放到下面。</p><p id="e502" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated"><a class="ae kd" rel="noopener" href="/@jeraldydeus/going-deep-from-scratch-3809f5890c5e">我的第二个帖子在这里</a></p><p id="03bd" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated"><strong class="jh hu">参考文献</strong></p><p id="0ce0" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">[1]<a class="ae kd" rel="noopener" href="/@karpathy/yes-you-should-understand-backprop-e2f06eab496b">https://medium . com/@ karpathy/yes-you-should-understand-back prop-e 2f 06 eab 496 b</a></p><p id="cfcd" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated"><a class="ae kd" href="https://www.coursera.org/specializations/deep-learning" rel="noopener ugc nofollow" target="_blank">https://www.coursera.org/specializations/deep-learning</a></p><p id="3c93" class="pw-post-body-paragraph jf jg ht jh b ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc hm dt translated">[3]<a class="ae kd" href="https://towardsdatascience.com/why-you-should-forget-for-loop-for-data-science-code-and-embrace-vectorization-696632622d5f" rel="noopener" target="_blank">https://towards data science . com/why-you-should-forget-loop-for-data-science-code-and-embrace-vectorization-696632622 d5f</a></p><blockquote class="ld"><p id="869b" class="le lf ht bd lg lh li lj lk ll lm kc ek translated">加入Coinmonks <a class="ae kd" href="https://t.me/coincodecap" rel="noopener ugc nofollow" target="_blank">电报频道</a>和<a class="ae kd" href="https://www.youtube.com/c/coinmonks/videos" rel="noopener ugc nofollow" target="_blank"> Youtube频道</a>获取每日<a class="ae kd" href="http://coincodecap.com/" rel="noopener ugc nofollow" target="_blank">加密新闻</a></p></blockquote><h2 id="15d6" class="ln lo ht bd lp lq lr ls lt lu lv lw lx jq ly lz ma ju mb mc md jy me mf mg mh dt translated">另外，阅读</h2><ul class=""><li id="20fb" class="mi mj ht jh b ji mk jm ml jq mm ju mn jy mo kc mp mq mr ms dt translated"><a class="ae kd" href="http://Top 4 Telegram Channels for Crypto Traders" rel="noopener ugc nofollow" target="_blank">密码电报信号</a> | <a class="ae kd" rel="noopener" href="/coinmonks/crypto-trading-bot-c2ffce8acb2a">密码交易机器人</a></li><li id="14e6" class="mi mj ht jh b ji mt jm mu jq mv ju mw jy mx kc mp mq mr ms dt translated"><a class="ae kd" rel="noopener" href="/coinmonks/top-10-crypto-copy-trading-platforms-for-beginners-d0c37c7d698c">复制交易</a> | <a class="ae kd" rel="noopener" href="/coinmonks/crypto-tax-software-ed4b4810e338">加密税务软件</a></li><li id="723e" class="mi mj ht jh b ji mt jm mu jq mv ju mw jy mx kc mp mq mr ms dt translated"><a class="ae kd" href="https://coincodecap.com/grid-trading" rel="noopener ugc nofollow" target="_blank">网格交易</a> | <a class="ae kd" rel="noopener" href="/coinmonks/the-best-cryptocurrency-hardware-wallets-of-2020-e28b1c124069">加密硬件钱包</a></li><li id="f33b" class="mi mj ht jh b ji mt jm mu jq mv ju mw jy mx kc mp mq mr ms dt translated"><a class="ae kd" rel="noopener" href="/coinmonks/crypto-exchange-dd2f9d6f3769">印度的加密交易所</a> | <a class="ae kd" rel="noopener" href="/coinmonks/buy-bitcoin-in-india-feb50ddfef94">加密应用</a></li><li id="47a8" class="mi mj ht jh b ji mt jm mu jq mv ju mw jy mx kc mp mq mr ms dt translated"><a class="ae kd" rel="noopener" href="/coinmonks/best-crypto-apis-for-developers-5efe3a597a9f">面向开发人员的最佳加密API</a></li><li id="c9bf" class="mi mj ht jh b ji mt jm mu jq mv ju mw jy mx kc mp mq mr ms dt translated">最佳<a class="ae kd" rel="noopener" href="/coinmonks/top-5-crypto-lending-platforms-in-2020-that-you-need-to-know-a1b675cec3fa">加密借贷平台</a></li><li id="9487" class="mi mj ht jh b ji mt jm mu jq mv ju mw jy mx kc mp mq mr ms dt translated"><a class="ae kd" rel="noopener" href="/coinmonks/leveraged-token-3f5257808b22">杠杆代币的终极指南</a></li><li id="4a2a" class="mi mj ht jh b ji mt jm mu jq mv ju mw jy mx kc mp mq mr ms dt translated"><a class="ae kd" href="https://coincodecap.com/crypto-affiliate-programs" rel="noopener ugc nofollow" target="_blank">八大加密附属计划</a> | <a class="ae kd" href="https://coincodecap.com/etoro-vs-coinbase" rel="noopener ugc nofollow" target="_blank"> eToro vs比特币基地</a></li><li id="e0bf" class="mi mj ht jh b ji mt jm mu jq mv ju mw jy mx kc mp mq mr ms dt translated"><a class="ae kd" href="https://coincodecap.com/best-ethereum-wallets" rel="noopener ugc nofollow" target="_blank">最佳以太坊钱包</a> | <a class="ae kd" href="https://coincodecap.com/telegram-crypto-bots" rel="noopener ugc nofollow" target="_blank">电报上的加密货币机器人</a></li><li id="fc80" class="mi mj ht jh b ji mt jm mu jq mv ju mw jy mx kc mp mq mr ms dt translated"><a class="ae kd" href="https://coincodecap.com/leveraged-token-exchanges" rel="noopener ugc nofollow" target="_blank">交易杠杆代币的最佳交易所</a> | <a class="ae kd" href="https://coincodecap.com/buy-floki-inu-token" rel="noopener ugc nofollow" target="_blank">购买Floki </a></li><li id="990c" class="mi mj ht jh b ji mt jm mu jq mv ju mw jy mx kc mp mq mr ms dt translated"><a class="ae kd" href="https://coincodecap.com/3commas-vs-pionex-vs-cryptohopper" rel="noopener ugc nofollow" target="_blank">3 commas vs . Pionex vs . crypto hopper</a>|<a class="ae kd" href="https://coincodecap.com/bingbon-review" rel="noopener ugc nofollow" target="_blank">Bingbon Review</a></li></ul></div></div>    
</body>
</html>